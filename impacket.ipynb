{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25a81978-79e4-4c1d-acc8-bd6394a9861c",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ipywidgets'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpickle\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mipywidgets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwidgets\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m display\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbuiltins\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'ipywidgets'"
     ]
    }
   ],
   "source": [
    "# Cell 1: Global Configuration Initialization with Debug Slider\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import logging\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "import builtins\n",
    "\n",
    "# Initialize logging\n",
    "logger = logging.getLogger('smbreplay')\n",
    "logger.handlers = []  # Clear existing handlers\n",
    "\n",
    "# Stream handler for Jupyter output\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - [%(asctime)s] %(message)s', datefmt='%a %b %d %H:%M:%S %Y'))\n",
    "logger.addHandler(stream_handler)\n",
    "\n",
    "# File handler for persistent logs\n",
    "log_file = '/home/jovyan/work/smbreplay/smbreplay.log'\n",
    "os.makedirs(os.path.dirname(log_file), exist_ok=True)\n",
    "file_handler = logging.FileHandler(log_file)\n",
    "file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - [%(asctime)s] %(message)s', datefmt='%a %b %d %H:%M:%S %Y'))\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "# Map verbosity levels (0–3) to logging levels\n",
    "VERBOSITY_TO_LOGGING = {\n",
    "    0: logging.CRITICAL,  # Only critical errors\n",
    "    1: logging.INFO,      # Info and above\n",
    "    2: logging.DEBUG,     # Debug and above\n",
    "    3: logging.DEBUG      # Same as 2, but can extend for finer granularity later\n",
    "}\n",
    "\n",
    "# Export logger to builtins\n",
    "builtins.logger = logger\n",
    "\n",
    "# Initialize configurations\n",
    "config_dir = \"/home/jovyan/work/smbreplay/\"\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "config_file = os.path.join(config_dir, \"config.pkl\")\n",
    "\n",
    "if not hasattr(builtins, 'pcap_config'):\n",
    "    builtins.pcap_config = {\n",
    "        \"capture_path\": None,\n",
    "        \"verbose_level\": 0  # Default to CRITICAL\n",
    "    }\n",
    "if not hasattr(builtins, 'replay_config'):\n",
    "    builtins.replay_config = {\n",
    "        \"server_ip\": \"10.216.29.241\",\n",
    "        \"domain\": \"nas-deep.local\",\n",
    "        \"username\": \"jtownsen\",\n",
    "        \"password\": \"PASSWORD\",\n",
    "        \"tree_name\": \"2pm\",\n",
    "        \"max_wait\": 5.0\n",
    "    }\n",
    "\n",
    "# Load from config.pkl if it exists\n",
    "if os.path.exists(config_file):\n",
    "    try:\n",
    "        with open(config_file, 'rb') as f:\n",
    "            loaded_config = pickle.load(f)\n",
    "            if 'pcap_config' in loaded_config:\n",
    "                # Update all pcap_config keys, including verbose_level\n",
    "                builtins.pcap_config.update({k: v for k, v in loaded_config['pcap_config'].items() if k in builtins.pcap_config})\n",
    "            if 'replay_config' in loaded_config:\n",
    "                builtins.replay_config.update({k: v for k, v in loaded_config['replay_config'].items() if k in builtins.replay_config})\n",
    "        logger.info(f\"Loaded config from {config_file}: pcap_config={builtins.pcap_config}, replay_config={builtins.replay_config}\")\n",
    "    except (pickle.PickleError, IOError) as e:\n",
    "        logger.critical(f\"Failed to load {config_file}: {e}. Using defaults.\")\n",
    "\n",
    "# Set logger level from pcap_config\n",
    "logger.setLevel(VERBOSITY_TO_LOGGING.get(builtins.pcap_config.get(\"verbose_level\", 0), logging.CRITICAL))\n",
    "\n",
    "# Define debug slider\n",
    "builtins.debug_slider = widgets.IntSlider(\n",
    "    value=builtins.pcap_config.get(\"verbose_level\", 0),\n",
    "    min=0,\n",
    "    max=3,\n",
    "    step=1,\n",
    "    description=\"Debug Level:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\"d\"\n",
    ")\n",
    "\n",
    "# Define debug slider handler\n",
    "def on_debug_slider_change(change):\n",
    "    \"\"\"Handle debug slider changes.\"\"\"\n",
    "    verbosity = change[\"new\"]\n",
    "    builtins.pcap_config[\"verbose_level\"] = verbosity\n",
    "    logger.setLevel(VERBOSITY_TO_LOGGING.get(verbosity, logging.CRITICAL))\n",
    "    # Save updated config to config.pkl\n",
    "    try:\n",
    "        with open(config_file, 'wb') as f:\n",
    "            pickle.dump({'pcap_config': builtins.pcap_config, 'replay_config': builtins.replay_config}, f)\n",
    "        logger.info(f\"Updated {config_file} with verbose_level={verbosity} (logging level: {logging.getLevelName(logger.level)})\")\n",
    "    except (pickle.PickleError, IOError) as e:\n",
    "        logger.critical(f\"Failed to save {config_file}: {e}\")\n",
    "    logger.debug(f\"Debug slider changed to verbose_level={verbosity}\")\n",
    "\n",
    "# Attach handler\n",
    "builtins.debug_slider.observe(on_debug_slider_change, names=\"value\")\n",
    "\n",
    "# Initialize other globals\n",
    "if not hasattr(builtins, 'operations'):\n",
    "    builtins.operations = []\n",
    "\n",
    "# Initialize capture and server config globals\n",
    "builtins.capture = builtins.pcap_config.get(\"capture_path\")\n",
    "builtins.server_ip = builtins.replay_config.get(\"server_ip\")\n",
    "builtins.domain = builtins.replay_config.get(\"domain\")\n",
    "builtins.username = builtins.replay_config.get(\"username\")\n",
    "builtins.password = builtins.replay_config.get(\"password\")\n",
    "builtins.tree_name = builtins.replay_config.get(\"tree_name\")\n",
    "builtins.all_cells_run = False\n",
    "\n",
    "# Display debug slider\n",
    "display(builtins.debug_slider)\n",
    "logger.info(\"Cell 1 initialized. Configuration set in /home/jovyan/work/smbreplay/config.pkl.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e6a1ec",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "## Objective\n",
    "Develop a system to capture, store, and replay SMB2 (Server Message Block version 2) network traffic in a controlled lab environment for diagnostic, testing, and protocol interaction analysis, critical for file sharing in Windows-based systems.\n",
    "\n",
    "## Approach\n",
    "- **Capture**: Extract SMB2 packets from PCAP files using `ntap-tshark` via SSH.\n",
    "- **Storage**: Store data in Parquet files by session (`smb2.sesid`) with JSON metadata.\n",
    "- **Replay**: Replicate file operations on a lab server using `impacket` (replacing `smbclient` due to `pysmb` deprecation).\n",
    "- **User Interface**: Provide an interactive dashboard for PCAP selection, ingestion, session visualization, and replay configuration, with settings in `config.pkl`.\n",
    "- **Modular Design**: Use reusable Python functions in `builtins` across notebook cells.\n",
    "\n",
    "## Tools\n",
    "- **Data Storage**: Parquet (compressed), JSON (metadata).\n",
    "- **Packet Capture**: `tshark`/`ntap-tshark`.\n",
    "- **Replay**: `impacket`.\n",
    "- **UI and Data Handling**: `ipywidgets`, `pandas`, `pyarrow`.\n",
    "- **SSH Interactions**: `paramiko`, `subprocess`.\n",
    "- **Memory Monitoring**: `psutil`.\n",
    "- **Logging**: Structured with `JupyterOutputHandler`.\n",
    "\n",
    "## Development Environment\n",
    "- Jupyter notebooks (Cells 1–11) in a container with an 8 GB memory limit.\n",
    "- Remote server access via SSH for packet capture and operations.\n",
    "\n",
    "## Lab Server Details\n",
    "- **IP**: 10.216.29.241\n",
    "- **Domain**: nas-deep.local\n",
    "- **Username**: jtownsen\n",
    "- **Password**: [REDACTED]\n",
    "- **Share**: 2pm\n",
    "\n",
    "## Key Workflow\n",
    "1. Capture SMB2 traffic with `ntap-tshark`.\n",
    "2. Process and store sessions in Parquet files with JSON metadata.\n",
    "3. Replay sessions using `impacket` on the lab server.\n",
    "4. Manage via interactive UI with settings in `config.pkl`.\n",
    "\n",
    "## Pre-Trace Conditions\n",
    "Ensure the lab server’s file system matches the original state by pre-creating directories and files based on `smb2.filename` and `smb2.cmd` before replay.\n",
    "\n",
    "## Phases and Current Status\n",
    "### Phase 1: Comprehensive SMB2 Field Capture\n",
    "- **Status**: Completed\n",
    "- **Details**: Captured 619 SMB2 fields (e.g., `smb2.cmd`, `smb2.filename`) using `ntap-tshark`, stored in Parquet with zstd compression. Fixed `smb2.filename` accuracy.\n",
    "\n",
    "### Phase 2: Session-Based Storage\n",
    "- **Status**: Completed\n",
    "- **Details**: Organized data by `smb2.sesid` into Parquet files (e.g., `smb2_session_0x98fc00000000d580.parquet`) with JSON metadata. Validated with 5,741 frames from a 319,000-packet PCAP.\n",
    "\n",
    "### Phase 3: Replay Mechanism Development\n",
    "- **Status**: In Progress\n",
    "- **Details**: Transitioning to `impacket`. Partial `smb2.cmd` mapping (e.g., 5 → Create). Focus on completing `impacket` integration and `smb2.cmd` mapping (0–18).\n",
    "- **Recent Progress**: UI enhancements (tooltips, `smb2.nt_status` mapping), replay logic refactoring.\n",
    "- **Time Remaining**: 2–3 days\n",
    "\n",
    "### Phase 4: Validation and Iteration\n",
    "- **Status**: Not Started\n",
    "- **Details**: Plan to compare original and replayed PCAPs, handle edge cases, and scale to multiple sessions.\n",
    "- **Time Estimate**: 2–4 days\n",
    "\n",
    "### Phase 5: Automation and Deployment\n",
    "- **Status**: Partially Completed\n",
    "- **Details**: Interactive UI with dynamic dropdowns and `config.pkl` settings. Automation and packaging pending.\n",
    "- **Time Estimate**: 3–5 days\n",
    "\n",
    "## Timeline\n",
    "- **Completed**: Phases 1 and 2\n",
    "- **Phase 3**: 2–3 days\n",
    "- **Phase 4**: 2–4 days\n",
    "- **Phase 5**: 3–5 days\n",
    "- **Total Remaining**: 7–12 days\n",
    "\n",
    "## Key Functions by Cell\n",
    "Below are the key functions from each cell, detailing their roles in the system:\n",
    "\n",
    "### Cell 1: Global Configuration Initialization\n",
    "- **`on_debug_slider_change(change)`**:\n",
    "  - Updates logging verbosity (0–3, mapping to CRITICAL, INFO, DEBUG) via a slider.\n",
    "  - Saves `pcap_config` to `config.pkl`.\n",
    "  - Logs verbosity changes.\n",
    "- **Purpose**: Initializes logging (`smbreplay.log`), `pcap_config` (capture path, verbosity), and `replay_config` (server details). Stores configs in `/home/jovyan/work/smbreplay/config.pkl`. Displays debug slider.\n",
    "\n",
    "### Cell 2: Setup and SMB2 Utility Functions\n",
    "- **`shorten_path(full_path, max_components=3)`**:\n",
    "  - Shortens file paths to the last `max_components` for display.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths (lowercase, backslashes to slashes) for comparison.\n",
    "- **`get_tree_name_mapping(frames)`**:\n",
    "  - Maps `smb2.tid` to share names from Tree Connect frames.\n",
    "- **`check_ssh_connectivity()`**:\n",
    "  - Verifies SSH connection to `backend` server for `ntap-tshark`.\n",
    "- **Purpose**: Sets up `itables` for interactive tables, defines SMB2 command mappings (`SMB2_OP_NAME_DESC`), FSCTL constants, and file/info level mappings. Ensures SSH connectivity.\n",
    "\n",
    "### Cell 3: SMB2 Field Definitions\n",
    "- **`normalize_hex_field(value, field_name)`**:\n",
    "  - Normalizes hex fields (e.g., `smb2.nt_status`, `smb2.sesid`) to uppercase hex format (32-bit or 64-bit).\n",
    "- **`normalize_fid(value)`**:\n",
    "  - Normalizes `smb2.fid` to 128-bit hex, handling UUID or hex formats.\n",
    "- **Purpose**: Defines 619 SMB2 fields from `smb2_fields.txt`, tracking fields (e.g., `frame.number`), and hex fields for normalization. Validates critical fields and defines mappings for `smb2.cmd`, `smb2.nt_status`, etc.\n",
    "\n",
    "### Cell 4: NTAP-Tshark Processing\n",
    "- **`build_tshark_command(capture, fields, reassembly, packet_limit, log_level, temp_dir, verbose)`**:\n",
    "  - Constructs SSH command for `ntap-tshark` to extract SMB2 fields from PCAP.\n",
    "- **`extract_fields(line, fields)`**:\n",
    "  - Parses `tshark` output lines into dictionaries with frame, stream, IP, and SMB2 fields.\n",
    "- **`process_tshark_output(cmd, fields)`**:\n",
    "  - Processes `tshark` output into a DataFrame, optimizing memory and normalizing fields.\n",
    "- **`save_to_parquet(df, parquet_path)`**:\n",
    "  - Saves DataFrame to Parquet with zstd compression, handling multi-value fields.\n",
    "- **`create_remote_directory(case_number, trace_name, force_reingest)`**:\n",
    "  - Creates and verifies remote session storage directory (`/stingray/<case_number>/.tracer/<trace_name>/sessions`).\n",
    "- **`clear_directory(directory)`**:\n",
    "  - Clears files in a remote directory for re-ingestion.\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs status messages for ingestion progress.\n",
    "- **Purpose**: Handles `ntap-tshark` execution, data parsing, and storage in Parquet files. Manages remote directories with SSH.\n",
    "\n",
    "### Cell 5: Ingestion and Session Extraction\n",
    "- **`get_packet_count(capture_path)`**:\n",
    "  - Retrieves packet count from PCAP using `ntap-capinfos`.\n",
    "- **`normalize_sesid(sesid_str)`**:\n",
    "  - Normalizes `smb2.sesid`, handling lists/commas and excluding invalid values.\n",
    "- **`normalize_cmd(cmd_str)`**:\n",
    "  - Normalizes `smb2.cmd`, handling lists/commas.\n",
    "- **`save_session_metadata(case_number, trace_name, sessions, output_dir)`**:\n",
    "  - Saves session metadata (e.g., session count, frame count) to JSON.\n",
    "- **`run_ingestion(capture_path, reassembly_enabled, force_reingest, verbose)`**:\n",
    "  - Orchestrates PCAP ingestion: validates PCAP, extracts fields with `tshark`, splits into sessions by `smb2.sesid`, and saves to Parquet and JSON.\n",
    "- **Purpose**: Manages ingestion of PCAP files, splitting into session-based Parquet files, and storing metadata.\n",
    "\n",
    "### Cell 6: Session Selection and Utilities\n",
    "- **`load_capture()`**:\n",
    "  - Loads capture path from `config.pkl` or `pcap_config`, validating existence.\n",
    "- **`get_output_dir(capture)`**:\n",
    "  - Derives session storage directory from capture path, ensuring write access.\n",
    "- **`list_session_files(output_dir)`**:\n",
    "  - Lists session Parquet files, normalizing to lowercase and removing duplicates.\n",
    "- **`shorten_path(path, max_length=50, min_filename_length=20)`**:\n",
    "  - Shortens paths for display, prioritizing filenames.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths for comparison, preserving leading slashes.\n",
    "- **`get_tree_name_mapping(df)`**:\n",
    "  - Maps `smb2.tid` to tree names from Tree Connect requests.\n",
    "- **Purpose**: Provides utilities for loading captures, managing session directories, and handling path normalization for UI display.\n",
    "\n",
    "### Cell 7: Session Loading and Filtering\n",
    "- **`load_and_summarize_session(capture, session_file)`**:\n",
    "  - Loads session Parquet file, returning frames, field options, file options, and default fields.\n",
    "- **`update_operations(capture, session_file, selected_file, selected_fields)`**:\n",
    "  - Prepares operations data, normalizing fields and filtering by file if specified.\n",
    "- **Purpose**: Loads and filters session data, preparing operations for display with normalized fields and mapped descriptions.\n",
    "\n",
    "### Cell 8: Replay Mechanism\n",
    "- **`setup_pre_trace_state(conn, selected_operations, default_tree_id)`**:\n",
    "  - Sets up lab server file system by creating directories and pre-existing files before replay.\n",
    "- **`replay_session(selected_operations, output_widget)`**:\n",
    "  - Replays SMB2 operations using `impacket`, handling Tree Connect, Create, Close, Read, and Write commands. Manages `tid` and `fid` mappings.\n",
    "- **Purpose**: Implements session replay on the lab server, ensuring pre-trace state and executing SMB2 commands.\n",
    "\n",
    "### Cell 9: Dashboard Setup\n",
    "- **Purpose**: Initializes dashboard widgets (e.g., `case_number_input`, `capture_dropdown`, `session_dropdown`, `ingest_button`, `replay_button`) and output widgets (`log_output`, `output_cell`). Sets up `JupyterOutputHandler` for logging and initializes `replay_config` values.\n",
    "\n",
    "### Cell 10: Event Handlers and Rendering\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs and displays status messages in `log_output`.\n",
    "- **`update_progress(message)`**:\n",
    "  - Updates progress messages in `progress_output`.\n",
    "- **`update_button_states()`**:\n",
    "  - Enables/disables buttons based on session availability.\n",
    "- **`render_page()`**:\n",
    "  - Renders operations DataTable with mandatory (`Frame`, `Command`, `Path`, `smb2.nt_status`) and optional columns, including summaries of commands and create actions.\n",
    "- **`on_case_number_change(change)`**:\n",
    "  - Updates `capture_dropdown` with PCAP files for the entered case number.\n",
    "- **`on_capture_change(change)`**:\n",
    "  - Updates `session_dropdown` with session files, adjusts button states, and saves `capture_path` to `config.pkl`.\n",
    "- **`on_ingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP ingestion with `run_ingestion` (no force re-ingest).\n",
    "- **`on_reingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP re-ingestion with `force_reingest=True`.\n",
    "- **`on_replay_button_clicked(b)`**:\n",
    "  - Initiates session replay via `replay_session`.\n",
    "- **`on_session_change(change)`**:\n",
    "  - Loads session data, updates `file_combobox` and `check_fields_select`, and renders operations.\n",
    "- **`on_file_change(change)`**:\n",
    "  - Filters operations by selected file and re-renders the table.\n",
    "- **`on_fields_change(change)`**:\n",
    "  - Updates table columns based on selected fields and re-renders.\n",
    "- **`on_save_config(b)`**:\n",
    "  - Saves `replay_config` to `config.pkl`, preserving `pcap_config`.\n",
    "- **`initialize_dashboard()`**:\n",
    "  - Auto-loads capture and sessions if pre-set in `config.pkl`.\n",
    "- **Purpose**: Handles UI interactions, rendering operations tables, and managing configuration saves.\n",
    "\n",
    "### Cell 11: Dashboard Display\n",
    "- **`update_dashboard_layout(verbose_level)`**:\n",
    "  - Constructs and displays the dashboard, conditionally showing logs based on verbosity.\n",
    "- **Purpose**: Finalizes dashboard display, integrating all widgets and ensuring dynamic log visibility.\n",
    "\n",
    "## Recent Developments\n",
    "- Standardized storage path: `/stingray/<case_number>/.tracer/<trace_name>/sessions`.\n",
    "- Optimized directory clearing and logging.\n",
    "- Fixed `smb2.sesid` and `smb2.cmd` normalization.\n",
    "- Enhanced UI with tooltips and dynamic table rendering.\n",
    "- Added debug slider for verbosity control (Cell 1).\n",
    "\n",
    "## Next Steps\n",
    "- **Verify Ingestion**: Re-run on `az3-CVO-python.pcapng` and test with a smaller PCAP.\n",
    "- **Finalize `smb2.cmd` Mapping**: Complete mappings (0–18) using `SMB2_OP_NAME_DESC`.\n",
    "- **Develop `impacket` Replay**: Fully implement and test in Cell 8.\n",
    "- **Start Phase 4**: Validate replay accuracy by comparing PCAPs.\n",
    "- **Complete Phase 5**: Automate and package the system.\n",
    "\n",
    "## Getting Started\n",
    "- **Priority**: Finalize `smb2.cmd` mapping and `impacket` replay.\n",
    "- **Key Cells**:\n",
    "  - **Cell 1**: Configuration and logging.\n",
    "  - **Cells 8–10**: Replay, dashboard, and event handling.\n",
    "  - **Cell 11**: Dashboard display.\n",
    "- **Action Plan**:\n",
    "  1. Test replay script in Cell 8 with `impacket`.\n",
    "  2. Re-run ingestion on `az3-CVO-python.pcapng` and validate.\n",
    "  3. Complete `smb2.cmd` mappings (0–18).\n",
    "  4. Begin Phase 4 validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0ce83d",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "## Objective\n",
    "Develop a system to capture, store, and replay SMB2 (Server Message Block version 2) network traffic in a controlled lab environment for diagnostic, testing, and protocol interaction analysis, critical for file sharing in Windows-based systems.\n",
    "\n",
    "## Approach\n",
    "- **Capture**: Extract SMB2 packets from PCAP files using `ntap-tshark` via SSH.\n",
    "- **Storage**: Store data in Parquet files by session (`smb2.sesid`) with JSON metadata.\n",
    "- **Replay**: Replicate file operations on a lab server using `impacket` (replacing `smbclient` due to `pysmb` deprecation).\n",
    "- **User Interface**: Provide an interactive dashboard for PCAP selection, ingestion, session visualization, and replay configuration, with settings in `config.pkl`.\n",
    "- **Modular Design**: Use reusable Python functions in `builtins` across notebook cells.\n",
    "\n",
    "## Tools\n",
    "- **Data Storage**: Parquet (compressed), JSON (metadata).\n",
    "- **Packet Capture**: `tshark`/`ntap-tshark`.\n",
    "- **Replay**: `impacket`.\n",
    "- **UI and Data Handling**: `ipywidgets`, `pandas`, `pyarrow`.\n",
    "- **SSH Interactions**: `paramiko`, `subprocess`.\n",
    "- **Memory Monitoring**: `psutil`.\n",
    "- **Logging**: Structured with `JupyterOutputHandler`.\n",
    "\n",
    "## Development Environment\n",
    "- Jupyter notebooks (Cells 1–11) in a container with an 8 GB memory limit.\n",
    "- Remote server access via SSH for packet capture and operations.\n",
    "\n",
    "## Lab Server Details\n",
    "- **IP**: 10.216.29.241\n",
    "- **Domain**: nas-deep.local\n",
    "- **Username**: jtownsen\n",
    "- **Password**: [REDACTED]\n",
    "- **Share**: 2pm\n",
    "\n",
    "## Key Workflow\n",
    "1. Capture SMB2 traffic with `ntap-tshark`.\n",
    "2. Process and store sessions in Parquet files with JSON metadata.\n",
    "3. Replay sessions using `impacket` on the lab server.\n",
    "4. Manage via interactive UI with settings in `config.pkl`.\n",
    "\n",
    "## Pre-Trace Conditions\n",
    "Ensure the lab server’s file system matches the original state by pre-creating directories and files based on `smb2.filename` and `smb2.cmd` before replay.\n",
    "\n",
    "## Phases and Current Status\n",
    "### Phase 1: Comprehensive SMB2 Field Capture\n",
    "- **Status**: Completed\n",
    "- **Details**: Captured 619 SMB2 fields (e.g., `smb2.cmd`, `smb2.filename`) using `ntap-tshark`, stored in Parquet with zstd compression. Fixed `smb2.filename` accuracy.\n",
    "\n",
    "### Phase 2: Session-Based Storage\n",
    "- **Status**: Completed\n",
    "- **Details**: Organized data by `smb2.sesid` into Parquet files (e.g., `smb2_session_0x98fc00000000d580.parquet`) with JSON metadata. Validated with 5,741 frames from a 319,000-packet PCAP.\n",
    "\n",
    "### Phase 3: Replay Mechanism Development\n",
    "- **Status**: In Progress\n",
    "- **Details**: Transitioning to `impacket`. Partial `smb2.cmd` mapping (e.g., 5 → Create). Focus on completing `impacket` integration and `smb2.cmd` mapping (0–18).\n",
    "- **Recent Progress**: UI enhancements (tooltips, `smb2.nt_status` mapping), replay logic refactoring.\n",
    "- **Time Remaining**: 2–3 days\n",
    "\n",
    "### Phase 4: Validation and Iteration\n",
    "- **Status**: Not Started\n",
    "- **Details**: Plan to compare original and replayed PCAPs, handle edge cases, and scale to multiple sessions.\n",
    "- **Time Estimate**: 2–4 days\n",
    "\n",
    "### Phase 5: Automation and Deployment\n",
    "- **Status**: Partially Completed\n",
    "- **Details**: Interactive UI with dynamic dropdowns and `config.pkl` settings. Automation and packaging pending.\n",
    "- **Time Estimate**: 3–5 days\n",
    "\n",
    "## Timeline\n",
    "- **Completed**: Phases 1 and 2\n",
    "- **Phase 3**: 2–3 days\n",
    "- **Phase 4**: 2–4 days\n",
    "- **Phase 5**: 3–5 days\n",
    "- **Total Remaining**: 7–12 days\n",
    "\n",
    "## Key Functions by Cell\n",
    "Below are the key functions from each cell, detailing their roles in the system:\n",
    "\n",
    "### Cell 1: Global Configuration Initialization\n",
    "- **`on_debug_slider_change(change)`**:\n",
    "  - Updates logging verbosity (0–3, mapping to CRITICAL, INFO, DEBUG) via a slider.\n",
    "  - Saves `pcap_config` to `config.pkl`.\n",
    "  - Logs verbosity changes.\n",
    "- **Purpose**: Initializes logging (`smbreplay.log`), `pcap_config` (capture path, verbosity), and `replay_config` (server details). Stores configs in `/home/jovyan/work/smbreplay/config.pkl`. Displays debug slider.\n",
    "\n",
    "### Cell 2: Setup and SMB2 Utility Functions\n",
    "- **`shorten_path(full_path, max_components=3)`**:\n",
    "  - Shortens file paths to the last `max_components` for display.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths (lowercase, backslashes to slashes) for comparison.\n",
    "- **`get_tree_name_mapping(frames)`**:\n",
    "  - Maps `smb2.tid` to share names from Tree Connect frames.\n",
    "- **`check_ssh_connectivity()`**:\n",
    "  - Verifies SSH connection to `backend` server for `ntap-tshark`.\n",
    "- **Purpose**: Sets up `itables` for interactive tables, defines SMB2 command mappings (`SMB2_OP_NAME_DESC`), FSCTL constants, and file/info level mappings. Ensures SSH connectivity.\n",
    "\n",
    "### Cell 3: SMB2 Field Definitions\n",
    "- **`normalize_hex_field(value, field_name)`**:\n",
    "  - Normalizes hex fields (e.g., `smb2.nt_status`, `smb2.sesid`) to uppercase hex format (32-bit or 64-bit).\n",
    "- **`normalize_fid(value)`**:\n",
    "  - Normalizes `smb2.fid` to 128-bit hex, handling UUID or hex formats.\n",
    "- **Purpose**: Defines 619 SMB2 fields from `smb2_fields.txt`, tracking fields (e.g., `frame.number`), and hex fields for normalization. Validates critical fields and defines mappings for `smb2.cmd`, `smb2.nt_status`, etc.\n",
    "\n",
    "### Cell 4: NTAP-Tshark Processing\n",
    "- **`build_tshark_command(capture, fields, reassembly, packet_limit, log_level, temp_dir, verbose)`**:\n",
    "  - Constructs SSH command for `ntap-tshark` to extract SMB2 fields from PCAP.\n",
    "- **`extract_fields(line, fields)`**:\n",
    "  - Parses `tshark` output lines into dictionaries with frame, stream, IP, and SMB2 fields.\n",
    "- **`process_tshark_output(cmd, fields)`**:\n",
    "  - Processes `tshark` output into a DataFrame, optimizing memory and normalizing fields.\n",
    "- **`save_to_parquet(df, parquet_path)`**:\n",
    "  - Saves DataFrame to Parquet with zstd compression, handling multi-value fields.\n",
    "- **`create_remote_directory(case_number, trace_name, force_reingest)`**:\n",
    "  - Creates and verifies remote session storage directory (`/stingray/<case_number>/.tracer/<trace_name>/sessions`).\n",
    "- **`clear_directory(directory)`**:\n",
    "  - Clears files in a remote directory for re-ingestion.\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs status messages for ingestion progress.\n",
    "- **Purpose**: Handles `ntap-tshark` execution, data parsing, and storage in Parquet files. Manages remote directories with SSH.\n",
    "\n",
    "### Cell 5: Ingestion and Session Extraction\n",
    "- **`get_packet_count(capture_path)`**:\n",
    "  - Retrieves packet count from PCAP using `ntap-capinfos`.\n",
    "- **`normalize_sesid(sesid_str)`**:\n",
    "  - Normalizes `smb2.sesid`, handling lists/commas and excluding invalid values.\n",
    "- **`normalize_cmd(cmd_str)`**:\n",
    "  - Normalizes `smb2.cmd`, handling lists/commas.\n",
    "- **`save_session_metadata(case_number, trace_name, sessions, output_dir)`**:\n",
    "  - Saves session metadata (e.g., session count, frame count) to JSON.\n",
    "- **`run_ingestion(capture_path, reassembly_enabled, force_reingest, verbose)`**:\n",
    "  - Orchestrates PCAP ingestion: validates PCAP, extracts fields with `tshark`, splits into sessions by `smb2.sesid`, and saves to Parquet and JSON.\n",
    "- **Purpose**: Manages ingestion of PCAP files, splitting into session-based Parquet files, and storing metadata.\n",
    "\n",
    "### Cell 6: Session Selection and Utilities\n",
    "- **`load_capture()`**:\n",
    "  - Loads capture path from `config.pkl` or `pcap_config`, validating existence.\n",
    "- **`get_output_dir(capture)`**:\n",
    "  - Derives session storage directory from capture path, ensuring write access.\n",
    "- **`list_session_files(output_dir)`**:\n",
    "  - Lists session Parquet files, normalizing to lowercase and removing duplicates.\n",
    "- **`shorten_path(path, max_length=50, min_filename_length=20)`**:\n",
    "  - Shortens paths for display, prioritizing filenames.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths for comparison, preserving leading slashes.\n",
    "- **`get_tree_name_mapping(df)`**:\n",
    "  - Maps `smb2.tid` to tree names from Tree Connect requests.\n",
    "- **Purpose**: Provides utilities for loading captures, managing session directories, and handling path normalization for UI display.\n",
    "\n",
    "### Cell 7: Session Loading and Filtering\n",
    "- **`load_and_summarize_session(capture, session_file)`**:\n",
    "  - Loads session Parquet file, returning frames, field options, file options, and default fields.\n",
    "- **`update_operations(capture, session_file, selected_file, selected_fields)`**:\n",
    "  - Prepares operations data, normalizing fields and filtering by file if specified.\n",
    "- **Purpose**: Loads and filters session data, preparing operations for display with normalized fields and mapped descriptions.\n",
    "\n",
    "### Cell 8: Replay Mechanism\n",
    "- **`setup_pre_trace_state(conn, selected_operations, default_tree_id)`**:\n",
    "  - Sets up lab server file system by creating directories and pre-existing files before replay.\n",
    "- **`replay_session(selected_operations, output_widget)`**:\n",
    "  - Replays SMB2 operations using `impacket`, handling Tree Connect, Create, Close, Read, and Write commands. Manages `tid` and `fid` mappings.\n",
    "- **Purpose**: Implements session replay on the lab server, ensuring pre-trace state and executing SMB2 commands.\n",
    "\n",
    "### Cell 9: Dashboard Setup\n",
    "- **Purpose**: Initializes dashboard widgets (e.g., `case_number_input`, `capture_dropdown`, `session_dropdown`, `ingest_button`, `replay_button`) and output widgets (`log_output`, `output_cell`). Sets up `JupyterOutputHandler` for logging and initializes `replay_config` values.\n",
    "\n",
    "### Cell 10: Event Handlers and Rendering\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs and displays status messages in `log_output`.\n",
    "- **`update_progress(message)`**:\n",
    "  - Updates progress messages in `progress_output`.\n",
    "- **`update_button_states()`**:\n",
    "  - Enables/disables buttons based on session availability.\n",
    "- **`render_page()`**:\n",
    "  - Renders operations DataTable with mandatory (`Frame`, `Command`, `Path`, `smb2.nt_status`) and optional columns, including summaries of commands and create actions.\n",
    "- **`on_case_number_change(change)`**:\n",
    "  - Updates `capture_dropdown` with PCAP files for the entered case number.\n",
    "- **`on_capture_change(change)`**:\n",
    "  - Updates `session_dropdown` with session files, adjusts button states, and saves `capture_path` to `config.pkl`.\n",
    "- **`on_ingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP ingestion with `run_ingestion` (no force re-ingest).\n",
    "- **`on_reingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP re-ingestion with `force_reingest=True`.\n",
    "- **`on_replay_button_clicked(b)`**:\n",
    "  - Initiates session replay via `replay_session`.\n",
    "- **`on_session_change(change)`**:\n",
    "  - Loads session data, updates `file_combobox` and `check_fields_select`, and renders operations.\n",
    "- **`on_file_change(change)`**:\n",
    "  - Filters operations by selected file and re-renders the table.\n",
    "- **`on_fields_change(change)`**:\n",
    "  - Updates table columns based on selected fields and re-renders.\n",
    "- **`on_save_config(b)`**:\n",
    "  - Saves `replay_config` to `config.pkl`, preserving `pcap_config`.\n",
    "- **`initialize_dashboard()`**:\n",
    "  - Auto-loads capture and sessions if pre-set in `config.pkl`.\n",
    "- **Purpose**: Handles UI interactions, rendering operations tables, and managing configuration saves.\n",
    "\n",
    "### Cell 11: Dashboard Display\n",
    "- **`update_dashboard_layout(verbose_level)`**:\n",
    "  - Constructs and displays the dashboard, conditionally showing logs based on verbosity.\n",
    "- **Purpose**: Finalizes dashboard display, integrating all widgets and ensuring dynamic log visibility.\n",
    "\n",
    "## Recent Developments\n",
    "- Standardized storage path: `/stingray/<case_number>/.tracer/<trace_name>/sessions`.\n",
    "- Optimized directory clearing and logging.\n",
    "- Fixed `smb2.sesid` and `smb2.cmd` normalization.\n",
    "- Enhanced UI with tooltips and dynamic table rendering.\n",
    "- Added debug slider for verbosity control (Cell 1).\n",
    "\n",
    "## Next Steps\n",
    "- **Verify Ingestion**: Re-run on `az3-CVO-python.pcapng` and test with a smaller PCAP.\n",
    "- **Finalize `smb2.cmd` Mapping**: Complete mappings (0–18) using `SMB2_OP_NAME_DESC`.\n",
    "- **Develop `impacket` Replay**: Fully implement and test in Cell 8.\n",
    "- **Start Phase 4**: Validate replay accuracy by comparing PCAPs.\n",
    "- **Complete Phase 5**: Automate and package the system.\n",
    "\n",
    "## Getting Started\n",
    "- **Priority**: Finalize `smb2.cmd` mapping and `impacket` replay.\n",
    "- **Key Cells**:\n",
    "  - **Cell 1**: Configuration and logging.\n",
    "  - **Cells 8–10**: Replay, dashboard, and event handling.\n",
    "  - **Cell 11**: Dashboard display.\n",
    "- **Action Plan**:\n",
    "  1. Test replay script in Cell 8 with `impacket`.\n",
    "  2. Re-run ingestion on `az3-CVO-python.pcapng` and validate.\n",
    "  3. Complete `smb2.cmd` mappings (0–18).\n",
    "  4. Begin Phase 4 validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751f3cf8",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "## Objective\n",
    "Develop a system to capture, store, and replay SMB2 (Server Message Block version 2) network traffic in a controlled lab environment for diagnostic, testing, and protocol interaction analysis, critical for file sharing in Windows-based systems.\n",
    "\n",
    "## Approach\n",
    "- **Capture**: Extract SMB2 packets from PCAP files using `ntap-tshark` via SSH.\n",
    "- **Storage**: Store data in Parquet files by session (`smb2.sesid`) with JSON metadata.\n",
    "- **Replay**: Replicate file operations on a lab server using `impacket` (replacing `smbclient` due to `pysmb` deprecation).\n",
    "- **User Interface**: Provide an interactive dashboard for PCAP selection, ingestion, session visualization, and replay configuration, with settings in `config.pkl`.\n",
    "- **Modular Design**: Use reusable Python functions in `builtins` across notebook cells.\n",
    "\n",
    "## Tools\n",
    "- **Data Storage**: Parquet (compressed), JSON (metadata).\n",
    "- **Packet Capture**: `tshark`/`ntap-tshark`.\n",
    "- **Replay**: `impacket`.\n",
    "- **UI and Data Handling**: `ipywidgets`, `pandas`, `pyarrow`.\n",
    "- **SSH Interactions**: `paramiko`, `subprocess`.\n",
    "- **Memory Monitoring**: `psutil`.\n",
    "- **Logging**: Structured with `JupyterOutputHandler`.\n",
    "\n",
    "## Development Environment\n",
    "- Jupyter notebooks (Cells 1–11) in a container with an 8 GB memory limit.\n",
    "- Remote server access via SSH for packet capture and operations.\n",
    "\n",
    "## Lab Server Details\n",
    "- **IP**: 10.216.29.241\n",
    "- **Domain**: nas-deep.local\n",
    "- **Username**: jtownsen\n",
    "- **Password**: [REDACTED]\n",
    "- **Share**: 2pm\n",
    "\n",
    "## Key Workflow\n",
    "1. Capture SMB2 traffic with `ntap-tshark`.\n",
    "2. Process and store sessions in Parquet files with JSON metadata.\n",
    "3. Replay sessions using `impacket` on the lab server.\n",
    "4. Manage via interactive UI with settings in `config.pkl`.\n",
    "\n",
    "## Pre-Trace Conditions\n",
    "Ensure the lab server’s file system matches the original state by pre-creating directories and files based on `smb2.filename` and `smb2.cmd` before replay.\n",
    "\n",
    "## Phases and Current Status\n",
    "### Phase 1: Comprehensive SMB2 Field Capture\n",
    "- **Status**: Completed\n",
    "- **Details**: Captured 619 SMB2 fields (e.g., `smb2.cmd`, `smb2.filename`) using `ntap-tshark`, stored in Parquet with zstd compression. Fixed `smb2.filename` accuracy.\n",
    "\n",
    "### Phase 2: Session-Based Storage\n",
    "- **Status**: Completed\n",
    "- **Details**: Organized data by `smb2.sesid` into Parquet files (e.g., `smb2_session_0x98fc00000000d580.parquet`) with JSON metadata. Validated with 5,741 frames from a 319,000-packet PCAP.\n",
    "\n",
    "### Phase 3: Replay Mechanism Development\n",
    "- **Status**: In Progress\n",
    "- **Details**: Transitioning to `impacket`. Partial `smb2.cmd` mapping (e.g., 5 → Create). Focus on completing `impacket` integration and `smb2.cmd` mapping (0–18).\n",
    "- **Recent Progress**: UI enhancements (tooltips, `smb2.nt_status` mapping), replay logic refactoring.\n",
    "- **Time Remaining**: 2–3 days\n",
    "\n",
    "### Phase 4: Validation and Iteration\n",
    "- **Status**: Not Started\n",
    "- **Details**: Plan to compare original and replayed PCAPs, handle edge cases, and scale to multiple sessions.\n",
    "- **Time Estimate**: 2–4 days\n",
    "\n",
    "### Phase 5: Automation and Deployment\n",
    "- **Status**: Partially Completed\n",
    "- **Details**: Interactive UI with dynamic dropdowns and `config.pkl` settings. Automation and packaging pending.\n",
    "- **Time Estimate**: 3–5 days\n",
    "\n",
    "## Timeline\n",
    "- **Completed**: Phases 1 and 2\n",
    "- **Phase 3**: 2–3 days\n",
    "- **Phase 4**: 2–4 days\n",
    "- **Phase 5**: 3–5 days\n",
    "- **Total Remaining**: 7–12 days\n",
    "\n",
    "## Key Functions by Cell\n",
    "Below are the key functions from each cell, detailing their roles in the system:\n",
    "\n",
    "### Cell 1: Global Configuration Initialization\n",
    "- **`on_debug_slider_change(change)`**:\n",
    "  - Updates logging verbosity (0–3, mapping to CRITICAL, INFO, DEBUG) via a slider.\n",
    "  - Saves `pcap_config` to `config.pkl`.\n",
    "  - Logs verbosity changes.\n",
    "- **Purpose**: Initializes logging (`smbreplay.log`), `pcap_config` (capture path, verbosity), and `replay_config` (server details). Stores configs in `/home/jovyan/work/smbreplay/config.pkl`. Displays debug slider.\n",
    "\n",
    "### Cell 2: Setup and SMB2 Utility Functions\n",
    "- **`shorten_path(full_path, max_components=3)`**:\n",
    "  - Shortens file paths to the last `max_components` for display.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths (lowercase, backslashes to slashes) for comparison.\n",
    "- **`get_tree_name_mapping(frames)`**:\n",
    "  - Maps `smb2.tid` to share names from Tree Connect frames.\n",
    "- **`check_ssh_connectivity()`**:\n",
    "  - Verifies SSH connection to `backend` server for `ntap-tshark`.\n",
    "- **Purpose**: Sets up `itables` for interactive tables, defines SMB2 command mappings (`SMB2_OP_NAME_DESC`), FSCTL constants, and file/info level mappings. Ensures SSH connectivity.\n",
    "\n",
    "### Cell 3: SMB2 Field Definitions\n",
    "- **`normalize_hex_field(value, field_name)`**:\n",
    "  - Normalizes hex fields (e.g., `smb2.nt_status`, `smb2.sesid`) to uppercase hex format (32-bit or 64-bit).\n",
    "- **`normalize_fid(value)`**:\n",
    "  - Normalizes `smb2.fid` to 128-bit hex, handling UUID or hex formats.\n",
    "- **Purpose**: Defines 619 SMB2 fields from `smb2_fields.txt`, tracking fields (e.g., `frame.number`), and hex fields for normalization. Validates critical fields and defines mappings for `smb2.cmd`, `smb2.nt_status`, etc.\n",
    "\n",
    "### Cell 4: NTAP-Tshark Processing\n",
    "- **`build_tshark_command(capture, fields, reassembly, packet_limit, log_level, temp_dir, verbose)`**:\n",
    "  - Constructs SSH command for `ntap-tshark` to extract SMB2 fields from PCAP.\n",
    "- **`extract_fields(line, fields)`**:\n",
    "  - Parses `tshark` output lines into dictionaries with frame, stream, IP, and SMB2 fields.\n",
    "- **`process_tshark_output(cmd, fields)`**:\n",
    "  - Processes `tshark` output into a DataFrame, optimizing memory and normalizing fields.\n",
    "- **`save_to_parquet(df, parquet_path)`**:\n",
    "  - Saves DataFrame to Parquet with zstd compression, handling multi-value fields.\n",
    "- **`create_remote_directory(case_number, trace_name, force_reingest)`**:\n",
    "  - Creates and verifies remote session storage directory (`/stingray/<case_number>/.tracer/<trace_name>/sessions`).\n",
    "- **`clear_directory(directory)`**:\n",
    "  - Clears files in a remote directory for re-ingestion.\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs status messages for ingestion progress.\n",
    "- **Purpose**: Handles `ntap-tshark` execution, data parsing, and storage in Parquet files. Manages remote directories with SSH.\n",
    "\n",
    "### Cell 5: Ingestion and Session Extraction\n",
    "- **`get_packet_count(capture_path)`**:\n",
    "  - Retrieves packet count from PCAP using `ntap-capinfos`.\n",
    "- **`normalize_sesid(sesid_str)`**:\n",
    "  - Normalizes `smb2.sesid`, handling lists/commas and excluding invalid values.\n",
    "- **`normalize_cmd(cmd_str)`**:\n",
    "  - Normalizes `smb2.cmd`, handling lists/commas.\n",
    "- **`save_session_metadata(case_number, trace_name, sessions, output_dir)`**:\n",
    "  - Saves session metadata (e.g., session count, frame count) to JSON.\n",
    "- **`run_ingestion(capture_path, reassembly_enabled, force_reingest, verbose)`**:\n",
    "  - Orchestrates PCAP ingestion: validates PCAP, extracts fields with `tshark`, splits into sessions by `smb2.sesid`, and saves to Parquet and JSON.\n",
    "- **Purpose**: Manages ingestion of PCAP files, splitting into session-based Parquet files, and storing metadata.\n",
    "\n",
    "### Cell 6: Session Selection and Utilities\n",
    "- **`load_capture()`**:\n",
    "  - Loads capture path from `config.pkl` or `pcap_config`, validating existence.\n",
    "- **`get_output_dir(capture)`**:\n",
    "  - Derives session storage directory from capture path, ensuring write access.\n",
    "- **`list_session_files(output_dir)`**:\n",
    "  - Lists session Parquet files, normalizing to lowercase and removing duplicates.\n",
    "- **`shorten_path(path, max_length=50, min_filename_length=20)`**:\n",
    "  - Shortens paths for display, prioritizing filenames.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths for comparison, preserving leading slashes.\n",
    "- **`get_tree_name_mapping(df)`**:\n",
    "  - Maps `smb2.tid` to tree names from Tree Connect requests.\n",
    "- **Purpose**: Provides utilities for loading captures, managing session directories, and handling path normalization for UI display.\n",
    "\n",
    "### Cell 7: Session Loading and Filtering\n",
    "- **`load_and_summarize_session(capture, session_file)`**:\n",
    "  - Loads session Parquet file, returning frames, field options, file options, and default fields.\n",
    "- **`update_operations(capture, session_file, selected_file, selected_fields)`**:\n",
    "  - Prepares operations data, normalizing fields and filtering by file if specified.\n",
    "- **Purpose**: Loads and filters session data, preparing operations for display with normalized fields and mapped descriptions.\n",
    "\n",
    "### Cell 8: Replay Mechanism\n",
    "- **`setup_pre_trace_state(conn, selected_operations, default_tree_id)`**:\n",
    "  - Sets up lab server file system by creating directories and pre-existing files before replay.\n",
    "- **`replay_session(selected_operations, output_widget)`**:\n",
    "  - Replays SMB2 operations using `impacket`, handling Tree Connect, Create, Close, Read, and Write commands. Manages `tid` and `fid` mappings.\n",
    "- **Purpose**: Implements session replay on the lab server, ensuring pre-trace state and executing SMB2 commands.\n",
    "\n",
    "### Cell 9: Dashboard Setup\n",
    "- **Purpose**: Initializes dashboard widgets (e.g., `case_number_input`, `capture_dropdown`, `session_dropdown`, `ingest_button`, `replay_button`) and output widgets (`log_output`, `output_cell`). Sets up `JupyterOutputHandler` for logging and initializes `replay_config` values.\n",
    "\n",
    "### Cell 10: Event Handlers and Rendering\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs and displays status messages in `log_output`.\n",
    "- **`update_progress(message)`**:\n",
    "  - Updates progress messages in `progress_output`.\n",
    "- **`update_button_states()`**:\n",
    "  - Enables/disables buttons based on session availability.\n",
    "- **`render_page()`**:\n",
    "  - Renders operations DataTable with mandatory (`Frame`, `Command`, `Path`, `smb2.nt_status`) and optional columns, including summaries of commands and create actions.\n",
    "- **`on_case_number_change(change)`**:\n",
    "  - Updates `capture_dropdown` with PCAP files for the entered case number.\n",
    "- **`on_capture_change(change)`**:\n",
    "  - Updates `session_dropdown` with session files, adjusts button states, and saves `capture_path` to `config.pkl`.\n",
    "- **`on_ingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP ingestion with `run_ingestion` (no force re-ingest).\n",
    "- **`on_reingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP re-ingestion with `force_reingest=True`.\n",
    "- **`on_replay_button_clicked(b)`**:\n",
    "  - Initiates session replay via `replay_session`.\n",
    "- **`on_session_change(change)`**:\n",
    "  - Loads session data, updates `file_combobox` and `check_fields_select`, and renders operations.\n",
    "- **`on_file_change(change)`**:\n",
    "  - Filters operations by selected file and re-renders the table.\n",
    "- **`on_fields_change(change)`**:\n",
    "  - Updates table columns based on selected fields and re-renders.\n",
    "- **`on_save_config(b)`**:\n",
    "  - Saves `replay_config` to `config.pkl`, preserving `pcap_config`.\n",
    "- **`initialize_dashboard()`**:\n",
    "  - Auto-loads capture and sessions if pre-set in `config.pkl`.\n",
    "- **Purpose**: Handles UI interactions, rendering operations tables, and managing configuration saves.\n",
    "\n",
    "### Cell 11: Dashboard Display\n",
    "- **`update_dashboard_layout(verbose_level)`**:\n",
    "  - Constructs and displays the dashboard, conditionally showing logs based on verbosity.\n",
    "- **Purpose**: Finalizes dashboard display, integrating all widgets and ensuring dynamic log visibility.\n",
    "\n",
    "## Recent Developments\n",
    "- Standardized storage path: `/stingray/<case_number>/.tracer/<trace_name>/sessions`.\n",
    "- Optimized directory clearing and logging.\n",
    "- Fixed `smb2.sesid` and `smb2.cmd` normalization.\n",
    "- Enhanced UI with tooltips and dynamic table rendering.\n",
    "- Added debug slider for verbosity control (Cell 1).\n",
    "\n",
    "## Next Steps\n",
    "- **Verify Ingestion**: Re-run on `az3-CVO-python.pcapng` and test with a smaller PCAP.\n",
    "- **Finalize `smb2.cmd` Mapping**: Complete mappings (0–18) using `SMB2_OP_NAME_DESC`.\n",
    "- **Develop `impacket` Replay**: Fully implement and test in Cell 8.\n",
    "- **Start Phase 4**: Validate replay accuracy by comparing PCAPs.\n",
    "- **Complete Phase 5**: Automate and package the system.\n",
    "\n",
    "## Getting Started\n",
    "- **Priority**: Finalize `smb2.cmd` mapping and `impacket` replay.\n",
    "- **Key Cells**:\n",
    "  - **Cell 1**: Configuration and logging.\n",
    "  - **Cells 8–10**: Replay, dashboard, and event handling.\n",
    "  - **Cell 11**: Dashboard display.\n",
    "- **Action Plan**:\n",
    "  1. Test replay script in Cell 8 with `impacket`.\n",
    "  2. Re-run ingestion on `az3-CVO-python.pcapng` and validate.\n",
    "  3. Complete `smb2.cmd` mappings (0–18).\n",
    "  4. Begin Phase 4 validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d24fb4",
   "metadata": {},
   "source": [
    "# Project Summary\n",
    "\n",
    "## Objective\n",
    "Develop a system to capture, store, and replay SMB2 (Server Message Block version 2) network traffic in a controlled lab environment for diagnostic, testing, and protocol interaction analysis, critical for file sharing in Windows-based systems.\n",
    "\n",
    "## Approach\n",
    "- **Capture**: Extract SMB2 packets from PCAP files using `ntap-tshark` via SSH.\n",
    "- **Storage**: Store data in Parquet files by session (`smb2.sesid`) with JSON metadata.\n",
    "- **Replay**: Replicate file operations on a lab server using `impacket` (replacing `smbclient` due to `pysmb` deprecation).\n",
    "- **User Interface**: Provide an interactive dashboard for PCAP selection, ingestion, session visualization, and replay configuration, with settings in `config.pkl`.\n",
    "- **Modular Design**: Use reusable Python functions in `builtins` across notebook cells.\n",
    "\n",
    "## Tools\n",
    "- **Data Storage**: Parquet (compressed), JSON (metadata).\n",
    "- **Packet Capture**: `tshark`/`ntap-tshark`.\n",
    "- **Replay**: `impacket`.\n",
    "- **UI and Data Handling**: `ipywidgets`, `pandas`, `pyarrow`.\n",
    "- **SSH Interactions**: `paramiko`, `subprocess`.\n",
    "- **Memory Monitoring**: `psutil`.\n",
    "- **Logging**: Structured with `JupyterOutputHandler`.\n",
    "\n",
    "## Development Environment\n",
    "- Jupyter notebooks (Cells 1–11) in a container with an 8 GB memory limit.\n",
    "- Remote server access via SSH for packet capture and operations.\n",
    "\n",
    "## Lab Server Details\n",
    "- **IP**: 10.216.29.241\n",
    "- **Domain**: nas-deep.local\n",
    "- **Username**: jtownsen\n",
    "- **Password**: [REDACTED]\n",
    "- **Share**: 2pm\n",
    "\n",
    "## Key Workflow\n",
    "1. Capture SMB2 traffic with `ntap-tshark`.\n",
    "2. Process and store sessions in Parquet files with JSON metadata.\n",
    "3. Replay sessions using `impacket` on the lab server.\n",
    "4. Manage via interactive UI with settings in `config.pkl`.\n",
    "\n",
    "## Pre-Trace Conditions\n",
    "Ensure the lab server’s file system matches the original state by pre-creating directories and files based on `smb2.filename` and `smb2.cmd` before replay.\n",
    "\n",
    "## Phases and Current Status\n",
    "### Phase 1: Comprehensive SMB2 Field Capture\n",
    "- **Status**: Completed\n",
    "- **Details**: Captured 619 SMB2 fields (e.g., `smb2.cmd`, `smb2.filename`) using `ntap-tshark`, stored in Parquet with zstd compression. Fixed `smb2.filename` accuracy.\n",
    "\n",
    "### Phase 2: Session-Based Storage\n",
    "- **Status**: Completed\n",
    "- **Details**: Organized data by `smb2.sesid` into Parquet files (e.g., `smb2_session_0x98fc00000000d580.parquet`) with JSON metadata. Validated with 5,741 frames from a 319,000-packet PCAP.\n",
    "\n",
    "### Phase 3: Replay Mechanism Development\n",
    "- **Status**: In Progress\n",
    "- **Details**: Transitioning to `impacket`. Partial `smb2.cmd` mapping (e.g., 5 → Create). Focus on completing `impacket` integration and `smb2.cmd` mapping (0–18).\n",
    "- **Recent Progress**: UI enhancements (tooltips, `smb2.nt_status` mapping), replay logic refactoring.\n",
    "- **Time Remaining**: 2–3 days\n",
    "\n",
    "### Phase 4: Validation and Iteration\n",
    "- **Status**: Not Started\n",
    "- **Details**: Plan to compare original and replayed PCAPs, handle edge cases, and scale to multiple sessions.\n",
    "- **Time Estimate**: 2–4 days\n",
    "\n",
    "### Phase 5: Automation and Deployment\n",
    "- **Status**: Partially Completed\n",
    "- **Details**: Interactive UI with dynamic dropdowns and `config.pkl` settings. Automation and packaging pending.\n",
    "- **Time Estimate**: 3–5 days\n",
    "\n",
    "## Timeline\n",
    "- **Completed**: Phases 1 and 2\n",
    "- **Phase 3**: 2–3 days\n",
    "- **Phase 4**: 2–4 days\n",
    "- **Phase 5**: 3–5 days\n",
    "- **Total Remaining**: 7–12 days\n",
    "\n",
    "## Key Functions by Cell\n",
    "Below are the key functions from each cell, detailing their roles in the system:\n",
    "\n",
    "### Cell 1: Global Configuration Initialization\n",
    "- **`on_debug_slider_change(change)`**:\n",
    "  - Updates logging verbosity (0–3, mapping to CRITICAL, INFO, DEBUG) via a slider.\n",
    "  - Saves `pcap_config` to `config.pkl`.\n",
    "  - Logs verbosity changes.\n",
    "- **Purpose**: Initializes logging (`smbreplay.log`), `pcap_config` (capture path, verbosity), and `replay_config` (server details). Stores configs in `/home/jovyan/work/smbreplay/config.pkl`. Displays debug slider.\n",
    "\n",
    "### Cell 2: Setup and SMB2 Utility Functions\n",
    "- **`shorten_path(full_path, max_components=3)`**:\n",
    "  - Shortens file paths to the last `max_components` for display.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths (lowercase, backslashes to slashes) for comparison.\n",
    "- **`get_tree_name_mapping(frames)`**:\n",
    "  - Maps `smb2.tid` to share names from Tree Connect frames.\n",
    "- **`check_ssh_connectivity()`**:\n",
    "  - Verifies SSH connection to `backend` server for `ntap-tshark`.\n",
    "- **Purpose**: Sets up `itables` for interactive tables, defines SMB2 command mappings (`SMB2_OP_NAME_DESC`), FSCTL constants, and file/info level mappings. Ensures SSH connectivity.\n",
    "\n",
    "### Cell 3: SMB2 Field Definitions\n",
    "- **`normalize_hex_field(value, field_name)`**:\n",
    "  - Normalizes hex fields (e.g., `smb2.nt_status`, `smb2.sesid`) to uppercase hex format (32-bit or 64-bit).\n",
    "- **`normalize_fid(value)`**:\n",
    "  - Normalizes `smb2.fid` to 128-bit hex, handling UUID or hex formats.\n",
    "- **Purpose**: Defines 619 SMB2 fields from `smb2_fields.txt`, tracking fields (e.g., `frame.number`), and hex fields for normalization. Validates critical fields and defines mappings for `smb2.cmd`, `smb2.nt_status`, etc.\n",
    "\n",
    "### Cell 4: NTAP-Tshark Processing\n",
    "- **`build_tshark_command(capture, fields, reassembly, packet_limit, log_level, temp_dir, verbose)`**:\n",
    "  - Constructs SSH command for `ntap-tshark` to extract SMB2 fields from PCAP.\n",
    "- **`extract_fields(line, fields)`**:\n",
    "  - Parses `tshark` output lines into dictionaries with frame, stream, IP, and SMB2 fields.\n",
    "- **`process_tshark_output(cmd, fields)`**:\n",
    "  - Processes `tshark` output into a DataFrame, optimizing memory and normalizing fields.\n",
    "- **`save_to_parquet(df, parquet_path)`**:\n",
    "  - Saves DataFrame to Parquet with zstd compression, handling multi-value fields.\n",
    "- **`create_remote_directory(case_number, trace_name, force_reingest)`**:\n",
    "  - Creates and verifies remote session storage directory (`/stingray/<case_number>/.tracer/<trace_name>/sessions`).\n",
    "- **`clear_directory(directory)`**:\n",
    "  - Clears files in a remote directory for re-ingestion.\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs status messages for ingestion progress.\n",
    "- **Purpose**: Handles `ntap-tshark` execution, data parsing, and storage in Parquet files. Manages remote directories with SSH.\n",
    "\n",
    "### Cell 5: Ingestion and Session Extraction\n",
    "- **`get_packet_count(capture_path)`**:\n",
    "  - Retrieves packet count from PCAP using `ntap-capinfos`.\n",
    "- **`normalize_sesid(sesid_str)`**:\n",
    "  - Normalizes `smb2.sesid`, handling lists/commas and excluding invalid values.\n",
    "- **`normalize_cmd(cmd_str)`**:\n",
    "  - Normalizes `smb2.cmd`, handling lists/commas.\n",
    "- **`save_session_metadata(case_number, trace_name, sessions, output_dir)`**:\n",
    "  - Saves session metadata (e.g., session count, frame count) to JSON.\n",
    "- **`run_ingestion(capture_path, reassembly_enabled, force_reingest, verbose)`**:\n",
    "  - Orchestrates PCAP ingestion: validates PCAP, extracts fields with `tshark`, splits into sessions by `smb2.sesid`, and saves to Parquet and JSON.\n",
    "- **Purpose**: Manages ingestion of PCAP files, splitting into session-based Parquet files, and storing metadata.\n",
    "\n",
    "### Cell 6: Session Selection and Utilities\n",
    "- **`load_capture()`**:\n",
    "  - Loads capture path from `config.pkl` or `pcap_config`, validating existence.\n",
    "- **`get_output_dir(capture)`**:\n",
    "  - Derives session storage directory from capture path, ensuring write access.\n",
    "- **`list_session_files(output_dir)`**:\n",
    "  - Lists session Parquet files, normalizing to lowercase and removing duplicates.\n",
    "- **`shorten_path(path, max_length=50, min_filename_length=20)`**:\n",
    "  - Shortens paths for display, prioritizing filenames.\n",
    "- **`normalize_path(path)`**:\n",
    "  - Normalizes paths for comparison, preserving leading slashes.\n",
    "- **`get_tree_name_mapping(df)`**:\n",
    "  - Maps `smb2.tid` to tree names from Tree Connect requests.\n",
    "- **Purpose**: Provides utilities for loading captures, managing session directories, and handling path normalization for UI display.\n",
    "\n",
    "### Cell 7: Session Loading and Filtering\n",
    "- **`load_and_summarize_session(capture, session_file)`**:\n",
    "  - Loads session Parquet file, returning frames, field options, file options, and default fields.\n",
    "- **`update_operations(capture, session_file, selected_file, selected_fields)`**:\n",
    "  - Prepares operations data, normalizing fields and filtering by file if specified.\n",
    "- **Purpose**: Loads and filters session data, preparing operations for display with normalized fields and mapped descriptions.\n",
    "\n",
    "### Cell 8: Replay Mechanism\n",
    "- **`setup_pre_trace_state(conn, selected_operations, default_tree_id)`**:\n",
    "  - Sets up lab server file system by creating directories and pre-existing files before replay.\n",
    "- **`replay_session(selected_operations, output_widget)`**:\n",
    "  - Replays SMB2 operations using `impacket`, handling Tree Connect, Create, Close, Read, and Write commands. Manages `tid` and `fid` mappings.\n",
    "- **Purpose**: Implements session replay on the lab server, ensuring pre-trace state and executing SMB2 commands.\n",
    "\n",
    "### Cell 9: Dashboard Setup\n",
    "- **Purpose**: Initializes dashboard widgets (e.g., `case_number_input`, `capture_dropdown`, `session_dropdown`, `ingest_button`, `replay_button`) and output widgets (`log_output`, `output_cell`). Sets up `JupyterOutputHandler` for logging and initializes `replay_config` values.\n",
    "\n",
    "### Cell 10: Event Handlers and Rendering\n",
    "- **`status_callback(message)`**:\n",
    "  - Logs and displays status messages in `log_output`.\n",
    "- **`update_progress(message)`**:\n",
    "  - Updates progress messages in `progress_output`.\n",
    "- **`update_button_states()`**:\n",
    "  - Enables/disables buttons based on session availability.\n",
    "- **`render_page()`**:\n",
    "  - Renders operations DataTable with mandatory (`Frame`, `Command`, `Path`, `smb2.nt_status`) and optional columns, including summaries of commands and create actions.\n",
    "- **`on_case_number_change(change)`**:\n",
    "  - Updates `capture_dropdown` with PCAP files for the entered case number.\n",
    "- **`on_capture_change(change)`**:\n",
    "  - Updates `session_dropdown` with session files, adjusts button states, and saves `capture_path` to `config.pkl`.\n",
    "- **`on_ingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP ingestion with `run_ingestion` (no force re-ingest).\n",
    "- **`on_reingest_button_clicked(b)`**:\n",
    "  - Triggers PCAP re-ingestion with `force_reingest=True`.\n",
    "- **`on_replay_button_clicked(b)`**:\n",
    "  - Initiates session replay via `replay_session`.\n",
    "- **`on_session_change(change)`**:\n",
    "  - Loads session data, updates `file_combobox` and `check_fields_select`, and renders operations.\n",
    "- **`on_file_change(change)`**:\n",
    "  - Filters operations by selected file and re-renders the table.\n",
    "- **`on_fields_change(change)`**:\n",
    "  - Updates table columns based on selected fields and re-renders.\n",
    "- **`on_save_config(b)`**:\n",
    "  - Saves `replay_config` to `config.pkl`, preserving `pcap_config`.\n",
    "- **`initialize_dashboard()`**:\n",
    "  - Auto-loads capture and sessions if pre-set in `config.pkl`.\n",
    "- **Purpose**: Handles UI interactions, rendering operations tables, and managing configuration saves.\n",
    "\n",
    "### Cell 11: Dashboard Display\n",
    "- **`update_dashboard_layout(verbose_level)`**:\n",
    "  - Constructs and displays the dashboard, conditionally showing logs based on verbosity.\n",
    "- **Purpose**: Finalizes dashboard display, integrating all widgets and ensuring dynamic log visibility.\n",
    "\n",
    "## Recent Developments\n",
    "- Standardized storage path: `/stingray/<case_number>/.tracer/<trace_name>/sessions`.\n",
    "- Optimized directory clearing and logging.\n",
    "- Fixed `smb2.sesid` and `smb2.cmd` normalization.\n",
    "- Enhanced UI with tooltips and dynamic table rendering.\n",
    "- Added debug slider for verbosity control (Cell 1).\n",
    "\n",
    "## Next Steps\n",
    "- **Verify Ingestion**: Re-run on `az3-CVO-python.pcapng` and test with a smaller PCAP.\n",
    "- **Finalize `smb2.cmd` Mapping**: Complete mappings (0–18) using `SMB2_OP_NAME_DESC`.\n",
    "- **Develop `impacket` Replay**: Fully implement and test in Cell 8.\n",
    "- **Start Phase 4**: Validate replay accuracy by comparing PCAPs.\n",
    "- **Complete Phase 5**: Automate and package the system.\n",
    "\n",
    "## Getting Started\n",
    "- **Priority**: Finalize `smb2.cmd` mapping and `impacket` replay.\n",
    "- **Key Cells**:\n",
    "  - **Cell 1**: Configuration and logging.\n",
    "  - **Cells 8–10**: Replay, dashboard, and event handling.\n",
    "  - **Cell 11**: Dashboard display.\n",
    "- **Action Plan**:\n",
    "  1. Test replay script in Cell 8 with `impacket`.\n",
    "  2. Re-run ingestion on `az3-CVO-python.pcapng` and validate.\n",
    "  3. Complete `smb2.cmd` mappings (0–18).\n",
    "  4. Begin Phase 4 validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10ec42-a685-44d3-b726-40ead8787483",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 2: Setup, Imports, and SMB2 Utility Functions\n",
    "import os\n",
    "import subprocess\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "import pandas as pd\n",
    "from impacket.smb3structs import *\n",
    "from impacket.nt_errors import ERROR_MESSAGES\n",
    "from itables import init_notebook_mode\n",
    "import paramiko\n",
    "import sys\n",
    "import builtins\n",
    "\n",
    "# Import logger from Cell 1\n",
    "if not hasattr(builtins, 'logger'):\n",
    "    raise ImportError(\"Cell 1 must define logger in builtins.\")\n",
    "logger = builtins.logger\n",
    "\n",
    "# Import replay_config from Cell 1\n",
    "if not hasattr(builtins, 'replay_config'):\n",
    "    raise ImportError(\"Cell 1 must define replay_config in builtins.\")\n",
    "replay_config = builtins.replay_config\n",
    "\n",
    "# Initialize notebook mode\n",
    "init_notebook_mode(all_interactive=True)\n",
    "logger.debug(\"Initialized itables notebook mode with all_interactive=True\")\n",
    "\n",
    "# Constants\n",
    "SEP = 4 * \" \"\n",
    "NTAPSHARK_PATH = \"/usr/local/bin/ntap-tshark\"\n",
    "\n",
    "# SMB2 Command Names\n",
    "SMB2_OP_NAME_DESC = {\n",
    "    0: (\"Negotiate Protocol Request\", \"Negotiate Protocol Response\"),\n",
    "    1: (\"Session Setup Request\", \"Session Setup Response\"),\n",
    "    2: (\"Session Logoff Request\", \"Session Logoff Response\"),\n",
    "    3: (\"Tree Connect Request\", \"Tree Connect Response\"),\n",
    "    4: (\"Tree Disconnect Request\", \"Tree Disconnect Response\"),\n",
    "    5: (\"Create Request\", \"Create Response\"),\n",
    "    6: (\"Close Request\", \"Close Response\"),\n",
    "    7: (\"Flush Request\", \"Flush Response\"),\n",
    "    8: (\"Read Request\", \"Read Response\"),\n",
    "    9: (\"Write Request\", \"Write Response\"),\n",
    "    10: (\"Lock Request\", \"Lock Response\"),\n",
    "    11: (\"IOCTL Request\", \"IOCTL Response\"),\n",
    "    12: (\"Cancel Request\", \"Cancel Response\"),\n",
    "    13: (\"Echo Request\", \"Echo Response\"),\n",
    "    14: (\"Query Directory Request\", \"Query Directory Response\"),\n",
    "    15: (\"Change Notify Request\", \"Change Notify Response\"),\n",
    "    16: (\"Query Info Request\", \"Query Info Response\"),\n",
    "    17: (\"Set Info Request\", \"Set Info Response\"),\n",
    "    18: (\"Oplock Break Request\", \"Oplock Break Response\"),\n",
    "}\n",
    "logger.debug(f\"Defined SMB2_OP_NAME_DESC with {len(SMB2_OP_NAME_DESC)} command mappings\")\n",
    "\n",
    "# Utility Functions\n",
    "def shorten_path(full_path: str, max_components: int = 3) -> str:\n",
    "    \"\"\"Shorten file paths to the last max_components.\"\"\"\n",
    "    logger.debug(f\"Shortening path: {full_path}\")\n",
    "    if full_path == \"Entire Stream\":\n",
    "        return full_path\n",
    "    components = full_path.split('\\\\')\n",
    "    if len(components) <= max_components:\n",
    "        return full_path\n",
    "    shortened = '...\\\\' + '\\\\'.join(components[-max_components:])\n",
    "    logger.debug(f\"Shortened path to: {shortened}\")\n",
    "    return shortened\n",
    "\n",
    "def normalize_path(path: str) -> str:\n",
    "    \"\"\"Normalize file paths for comparison.\"\"\"\n",
    "    logger.debug(f\"Normalizing path: {path}\")\n",
    "    if pd.isna(path) or path in [\"N/A\", \"\", \"Entire Stream\"]:\n",
    "        return \"N/A\"\n",
    "    normalized = path.strip().replace('/', '\\\\').lower()\n",
    "    logger.debug(f\"Normalized path to: {normalized}\")\n",
    "    return normalized\n",
    "\n",
    "def get_tree_name_mapping(frames: pd.DataFrame) -> Dict[str, str]:\n",
    "    \"\"\"Map tree IDs to share names based on Tree Connect frames.\"\"\"\n",
    "    logger.info(\"Generating tree name mappings\")\n",
    "    tree_mapping = {}\n",
    "    if isinstance(frames, list):\n",
    "        frames = pd.DataFrame(frames)\n",
    "        logger.debug(\"Converted input list to DataFrame\")\n",
    "    \n",
    "    request_frames = frames[(frames['smb2.cmd'] == '3') & (frames['smb2.flags.response'] != 'True')]\n",
    "    logger.debug(f\"Found {len(request_frames)} Tree Connect request frames\")\n",
    "    \n",
    "    for _, request_frame in request_frames.iterrows():\n",
    "        tree_path = request_frame.get('smb2.tree', None)\n",
    "        if pd.isna(tree_path) or not tree_path:\n",
    "            logger.debug(f\"No tree path in request frame {request_frame.get('frame.number')}\")\n",
    "            continue\n",
    "        share_name = tree_path.split('\\\\')[-1] if '\\\\' in tree_path else tree_path\n",
    "        response_frames = frames[(frames['smb2.cmd'] == '3') & \n",
    "                                (frames['smb2.flags.response'] == 'True') &\n",
    "                                (frames['frame.number'].astype(int) > int(request_frame.get('frame.number', 0)))]\n",
    "        for _, response_frame in response_frames.iterrows():\n",
    "            tid = response_frame.get('smb2.tid', None)\n",
    "            if tid and pd.notna(tid):\n",
    "                tree_mapping[tid] = share_name\n",
    "                logger.debug(f\"Mapped tree ID {tid} to share {share_name} from frame {request_frame.get('frame.number')}\")\n",
    "                break\n",
    "    \n",
    "    logger.info(f\"Found {len(tree_mapping)} tree ID mappings\")\n",
    "    return tree_mapping\n",
    "\n",
    "def check_ssh_connectivity() -> bool:\n",
    "    \"\"\"Verify SSH connection to backend server.\"\"\"\n",
    "    ssh_host = \"backend\"\n",
    "    ssh_user = \"root\"\n",
    "    ssh_key = \"/home/jovyan/.ssh/id_rsa\"\n",
    "    logger.info(f\"Checking SSH connection to {ssh_host}\")\n",
    "    try:\n",
    "        client = paramiko.SSHClient()\n",
    "        client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "        client.connect(ssh_host, username=ssh_user, key_filename=ssh_key, timeout=5)\n",
    "        client.close()\n",
    "        logger.info(f\"SSH connection to {ssh_host} successful\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"SSH connection to {ssh_host} failed: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# FSCTL_* Constants\n",
    "FSCTL_CONSTANTS = {\n",
    "    \"FSCTL_CREATE_OR_GET_OBJECT_ID\": 0x900c0,\n",
    "    \"FSCTL_DELETE_OBJECT_ID\": 0x900a0,\n",
    "    \"FSCTL_DELETE_REPARSE_POINT\": 0x900ac,\n",
    "    \"FSCTL_DUPLICATE_EXTENTS_TO_FILE\": 0x98344,\n",
    "    \"FSCTL_FILESYSTEM_GET_STATISTICS\": 0x90060,\n",
    "    \"FSCTL_FIND_FILES_BY_SID\": 0x9008f,\n",
    "    \"FSCTL_GET_COMPRESSION\": 0x9003c,\n",
    "    \"FSCTL_GET_INTEGRITY_INFORMATION\": 0x9027c,\n",
    "    \"FSCTL_GET_NTFS_VOLUME_DATA\": 0x90064,\n",
    "    \"FSCTL_GET_REFS_VOLUME_DATA\": 0x902d8,\n",
    "    \"FSCTL_GET_OBJECT_ID\": 0x9009c,\n",
    "    \"FSCTL_GET_REPARSE_POINT\": 0x900a8,\n",
    "    \"FSCTL_GET_RETRIEVAL_POINTERS\": 0x90073,\n",
    "    \"FSCTL_IS_PATHNAME_VALID\": 0x9002c,\n",
    "    \"FSCTL_LMR_SET_LINK_TRACKING_INFORMATION\": 0x1400ec,\n",
    "    \"FSCTL_OFFLOAD_READ\": 0x94264,\n",
    "    \"FSCTL_OFFLOAD_WRITE\": 0x98268,\n",
    "    \"FSCTL_QUERY_ALLOCATED_RANGES\": 0x940cf,\n",
    "    \"FSCTL_QUERY_FAT_BPB\": 0x90058,\n",
    "    \"FSCTL_QUERY_FILE_REGIONS\": 0x90284,\n",
    "    \"FSCTL_QUERY_ON_DISK_VOLUME_INFO\": 0x9013c,\n",
    "    \"FSCTL_QUERY_SPARING_INFO\": 0x90138,\n",
    "    \"FSCTL_READ_FILE_USN_DATA\": 0x900eb,\n",
    "    \"FSCTL_RECALL_FILE\": 0x90117,\n",
    "    \"FSCTL_SET_COMPRESSION\": 0x9c040,\n",
    "    \"FSCTL_SET_DEFECT_MANAGEMENT\": 0x98134,\n",
    "    \"FSCTL_SET_ENCRYPTION\": 0x900d7,\n",
    "    \"FSCTL_SET_INTEGRITY_INFORMATION\": 0x9c280,\n",
    "    \"FSCTL_SET_OBJECT_ID\": 0x90098,\n",
    "    \"FSCTL_SET_OBJECT_ID_EXTENDED\": 0x900bc,\n",
    "    \"FSCTL_SET_SPARSE\": 0x900c4,\n",
    "    \"FSCTL_SET_ZERO_DATA\": 0x980c8,\n",
    "    \"FSCTL_SET_ZERO_ON_DEALLOCATION\": 0x90194,\n",
    "    \"FSCTL_SIS_COPYFILE\": 0x90100,\n",
    "    \"FSCTL_WRITE_USN_CLOSE_RECORD\": 0x900ef,\n",
    "    \"FSCTL_DFS_GET_REFERRALS\": 0x60194,\n",
    "    \"FSCTL_PIPE_PEEK\": 0x11400c,\n",
    "    \"FSCTL_PIPE_WAIT\": 0x110018,\n",
    "    \"FSCTL_PIPE_TRANSCEIVE\": 0x11c017,\n",
    "    \"FSCTL_SRV_COPYCHUNK\": 0x1440f0,\n",
    "    \"FSCTL_SRV_ENUMERATE_SNAPSHOTS\": 0x144064,\n",
    "    \"FSCTL_SRV_REQUEST_RESUME_KEY\": 0x1400c4,\n",
    "    \"FSCTL_SRV_READ_HASH\": 0x1440e8,\n",
    "    \"FSCTL_SRV_COPYCHUNK_WRITE\": 0x1440f4,\n",
    "    \"FSCTL_LMR_REQUEST_RESILIENCY\": 0x1400d8,\n",
    "    \"FSCTL_QUERY_NETWORK_INTERFACE_INFO\": 0x1400fc,\n",
    "    \"FSCTL_SET_REPARSE_POINT\": 0x900a4,\n",
    "    \"FSCTL_DFS_GET_REFERRALS_EX\": 0x601a0,\n",
    "    \"FSCTL_FILE_LEVEL_TRIM\": 0x98208,\n",
    "    \"FSCTL_VALIDATE_NEGOTIATE_INFO\": 0x140204,\n",
    "    \"FSCTL_QUERY_SHARED_VIRTUAL_DISK_SUPPORT\": 0x90300,\n",
    "    \"FSCTL_SVHDX_SYNC_TUNNEL_REQUEST\": 0x90304,\n",
    "}\n",
    "logger.debug(f\"Defined FSCTL_CONSTANTS with {len(FSCTL_CONSTANTS)} entries\")\n",
    "\n",
    "# File Information Classes\n",
    "FILE_INFO_CLASSES = {\n",
    "    \"FILE_DIRECTORY_INFORMATION\": 1,\n",
    "    \"FILE_FULL_DIRECTORY_INFORMATION\": 2,\n",
    "    \"FILEID_FULL_DIRECTORY_INFORMATION\": 38,\n",
    "    \"FILE_BOTH_DIRECTORY_INFORMATION\": 3,\n",
    "    \"FILEID_BOTH_DIRECTORY_INFORMATION\": 37,\n",
    "    \"FILENAMES_INFORMATION\": 12,\n",
    "}\n",
    "logger.debug(f\"Defined FILE_INFO_CLASSES with {len(FILE_INFO_CLASSES)} entries\")\n",
    "\n",
    "# SMB2 Info Levels\n",
    "SMB2_INFO_LEVELS = {\n",
    "    \"SMB2_0_INFO_FILE\": 0x01,\n",
    "    \"SMB2_0_INFO_FILESYSTEM\": 0x02,\n",
    "    \"SMB2_0_INFO_SECURITY\": 0x03,\n",
    "}\n",
    "logger.debug(f\"Defined SMB2_INFO_LEVELS with {len(SMB2_INFO_LEVELS)} entries\")\n",
    "\n",
    "# File Info Classes\n",
    "SMB2_FILE_INFO_CLASSES = {\n",
    "    \"SMB2_FILE_ACCESS_INFO\": 8,\n",
    "    \"SMB2_FILE_ALIGNMENT_INFO\": 17,\n",
    "    \"SMB2_FILE_ALL_INFO\": 18,\n",
    "    \"SMB2_FILE_ALTERNATE_NAME_INFO\": 21,\n",
    "    \"SMB2_ATTRIBUTE_TAG_INFO\": 35,\n",
    "    \"SMB2_FILE_BASIC_INFO\": 4,\n",
    "    \"SMB2_FILE_COMPRESSION_INFO\": 28,\n",
    "    \"SMB2_FILE_EA_INFO\": 7,\n",
    "    \"SMB2_FULL_EA_INFO\": 15,\n",
    "    \"SMB2_FILE_INTERNAL_INFO\": 6,\n",
    "    \"SMB2_FILE_MODE_INFO\": 16,\n",
    "    \"SMB2_FILE_NETWORK_OPEN_INFO\": 34,\n",
    "    \"SMB2_FILE_PIPE_INFO\": 23,\n",
    "    \"SMB2_FILE_POSITION_INFO\": 14,\n",
    "    \"SMB2_FILE_STANDARD_INFO\": 5,\n",
    "    \"SMB2_FILE_STREAM_INFO\": 22,\n",
    "    \"SMB2_FILESYSTEM_ATTRIBUTE_INFO\": 5,\n",
    "    \"SMB2_FILESYSTEM_CONTROL_INFO\": 6,\n",
    "    \"SMB2_FILESYSTEM_DEVICE_INFO\": 4,\n",
    "    \"SMB2_FILESYSTEM_FULL_SIZE_INFO\": 7,\n",
    "    \"SMB2_FILESYSTEM_OBJECT_ID_INFO\": 8,\n",
    "    \"SMB2_FILESYSTEM_SECTOR_SIZE_INFO\": 11,\n",
    "    \"SMB2_FILESYSTEM_SIZE_INFO\": 3,\n",
    "    \"SMB2_FILESYSTEM_VOLUME_INFO\": 1,\n",
    "    \"SMB2_FILE_ALLOCATION_INFO\": 19,\n",
    "    \"SMB2_FILE_DISPOSITION_INFO\": 13,\n",
    "    \"SMB2_FILE_END_OF_FILE_INFO\": 20,\n",
    "    \"SMB2_FILE_LINK_INFO\": 11,\n",
    "    \"SMB2_FILE_RENAME_INFO\": 10,\n",
    "    \"SMB2_FILE_SHORT_NAME_INFO\": 45,\n",
    "    \"SMB2_FILE_VALID_DATA_LENGTH_INFO\": 47,\n",
    "}\n",
    "logger.debug(f\"Defined SMB2_FILE_INFO_CLASSES with {len(SMB2_FILE_INFO_CLASSES)} entries\")\n",
    "\n",
    "# Add to builtins\n",
    "builtins.check_ssh_connectivity = check_ssh_connectivity\n",
    "builtins.SMB2_OP_NAME_DESC = SMB2_OP_NAME_DESC\n",
    "builtins.FSCTL_CONSTANTS = FSCTL_CONSTANTS\n",
    "builtins.FILE_INFO_CLASSES = FILE_INFO_CLASSES\n",
    "builtins.SMB2_INFO_LEVELS = SMB2_INFO_LEVELS\n",
    "builtins.SMB2_FILE_INFO_CLASSES = SMB2_FILE_INFO_CLASSES\n",
    "builtins.ERROR_MESSAGES = ERROR_MESSAGES\n",
    "builtins.shorten_path = shorten_path\n",
    "builtins.normalize_path = normalize_path\n",
    "builtins.get_tree_name_mapping = get_tree_name_mapping\n",
    "\n",
    "# Setup Confirmation\n",
    "if not check_ssh_connectivity():\n",
    "    logger.critical(\"Setup aborted due to SSH connection failure\")\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    logger.info(f\"Setup initialized on {pd.Timestamp.now()} for remote ntap-tshark on backend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019b1362-6a32-4b92-85d0-6b7dcee37f2b",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Cell 3: Define comprehensive SMB2 and related fields from smb2_fields.txt\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "import os\n",
    "import uuid\n",
    "from typing import List, Dict\n",
    "import builtins\n",
    "\n",
    "# Import from builtins (set in Cells 1–2)\n",
    "from builtins import logger, check_ssh_connectivity, SMB2_OP_NAME_DESC, FSCTL_CONSTANTS, FILE_INFO_CLASSES, SMB2_INFO_LEVELS, SMB2_FILE_INFO_CLASSES, ERROR_MESSAGES\n",
    "\n",
    "SSH_KEY = \"/home/jovyan/.ssh/id_rsa\"\n",
    "SSH_USER = \"root\"\n",
    "SSH_HOST = \"backend\"\n",
    "NTAPSHARK_PATH = \"/usr/local/bin/ntap-tshark\"\n",
    "\n",
    "# Tracking fields for network context\n",
    "TRACKING_FIELDS = [\n",
    "    \"frame.number\", \"tcp.stream\", \"ip.src\", \"ip.dst\", \"frame.time\", \"frame.time_delta\",\n",
    "    \"frame.len\", \"tcp.srcport\", \"tcp.dstport\", \"tcp.seq\", \"tcp.ack\", \"tcp.len\",\n",
    "    \"ip.ttl\", \"ip.proto\", \"frame.time_epoch\", \"tcp.flags\", \"tcp.window_size\", \"ip.id\"\n",
    "]\n",
    "logger.debug(f\"Defined {len(TRACKING_FIELDS)} TRACKING_FIELDS\")\n",
    "\n",
    "# Hex fields requiring normalization\n",
    "HEX_FIELDS = [\n",
    "    \"smb2.nt_status\", \"smb2.ioctl.function\", \"smb2.tid\", \"smb2.sesid\", \"smb2.msg_id\", \"smb2.fid\",\n",
    "    \"smb2.create.action\"  # Added for create action normalization\n",
    "]\n",
    "logger.debug(f\"Defined HEX_FIELDS: {', '.join(HEX_FIELDS)}\")\n",
    "\n",
    "# Create action mappings (from impacket.smb3structs)\n",
    "CREATE_ACTION_DESC = {\n",
    "    0: \"FILE_SUPERSEDED\",\n",
    "    1: \"FILE_OPENED\",\n",
    "    2: \"FILE_CREATED\",\n",
    "    3: \"FILE_OVERWRITTEN\",\n",
    "    4: \"FILE_EXISTS\",\n",
    "    5: \"FILE_DOES_NOT_EXIST\"\n",
    "}\n",
    "logger.debug(f\"Defined CREATE_ACTION_DESC with {len(CREATE_ACTION_DESC)} mappings\")\n",
    "\n",
    "# Generate smb2_fields.txt if missing\n",
    "smb2_fields_file = \"smb2_fields.txt\"\n",
    "if not os.path.exists(smb2_fields_file):\n",
    "    logger.info(f\"Generating {smb2_fields_file}...\")\n",
    "    if not check_ssh_connectivity():\n",
    "        logger.critical(f\"Cannot connect to {SSH_HOST} for ntap-tshark\")\n",
    "        raise RuntimeError(f\"Cannot connect to {SSH_HOST} for ntap-tshark\")\n",
    "    cmd = f\"ssh -i {SSH_KEY} -p 22 {SSH_USER}@{SSH_HOST} {NTAPSHARK_PATH} -G fields | grep smb2 > {smb2_fields_file}\"\n",
    "    logger.debug(f\"Executing command: {cmd}\")\n",
    "    result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        logger.critical(f\"Error generating {smb2_fields_file}: stderr={result.stderr}, stdout={result.stdout}\")\n",
    "        raise RuntimeError(f\"Failed to generate {smb2_fields_file}\")\n",
    "    logger.info(f\"{smb2_fields_file} generated successfully\")\n",
    "\n",
    "# Extract SMB2 fields from smb2_fields.txt\n",
    "logger.info(f\"Reading {smb2_fields_file}\")\n",
    "with open(smb2_fields_file, \"r\") as f:\n",
    "    smb2_field_lines = f.readlines()\n",
    "logger.info(f\"Read {len(smb2_field_lines)} lines from {smb2_fields_file}\")\n",
    "if not smb2_field_lines:\n",
    "    logger.critical(f\"Error: {smb2_fields_file} is empty. Check ntap-tshark -G fields output.\")\n",
    "    raise RuntimeError(f\"{smb2_fields_file} is empty\")\n",
    "\n",
    "SMB2_FIELDS = []\n",
    "for line in smb2_field_lines:\n",
    "    parts = line.strip().split(\"\\t\")\n",
    "    if len(parts) >= 4 and parts[0] == \"F\" and parts[2].startswith(\"smb2.\"):\n",
    "        SMB2_FIELDS.append(parts[2])\n",
    "logger.info(f\"Extracted {len(SMB2_FIELDS)} SMB2 fields from {smb2_fields_file}\")\n",
    "logger.debug(f\"First 5 SMB2 fields: {', '.join(SMB2_FIELDS[:5])}...\")\n",
    "\n",
    "# Combine and deduplicate fields\n",
    "FIELDS = sorted(set(TRACKING_FIELDS + SMB2_FIELDS))\n",
    "logger.info(f\"Combined {len(FIELDS)} unique fields for ingestion\")\n",
    "\n",
    "# Field corrections\n",
    "if 'smb.file_name' in FIELDS:\n",
    "    logger.warning(f\"Replacing 'smb.file_name' with 'smb2.filename' in FIELDS\")\n",
    "    FIELDS[FIELDS.index('smb.file_name')] = 'smb2.filename'\n",
    "\n",
    "# Validate critical fields\n",
    "CRITICAL_FIELDS = [\n",
    "    \"smb2.cmd\", \"smb2.sesid\", \"smb2.filename\", \"smb2.write_data\", \"smb2.read_data\",\n",
    "    \"smb2.ioctl.function\", \"smb2.tid\", \"smb2.nt_status\", \"smb2.msg_id\", \"smb2.fid\",\n",
    "    \"smb2.tree\", \"smb2.create.disposition\", \"smb2.create.options\", \"smb2.share_flags\",\n",
    "    \"smb2.access_mask\", \"smb2.file_attributes\", \"smb2.infolevel\", \"smb2.buffer_code\",\n",
    "    \"smb2.create.action\"  # Added to ensure capture\n",
    "]\n",
    "missing_fields = [f for f in CRITICAL_FIELDS if f not in FIELDS]\n",
    "if missing_fields:\n",
    "    logger.warning(f\"Critical fields missing: {', '.join(missing_fields)}. Proceeding as fields are confirmed present in smb2_fields.txt.\")\n",
    "else:\n",
    "    logger.info(f\"Critical fields validated: {', '.join(CRITICAL_FIELDS)}\")\n",
    "\n",
    "logger.info(f\"Defined {len(FIELDS)} fields for ingestion\")\n",
    "logger.debug(f\"First 10 fields: {', '.join(FIELDS[:10])}...\")\n",
    "\n",
    "# Generic normalization function for hex fields\n",
    "def normalize_hex_field(value, field_name):\n",
    "    \"\"\"Normalize hex fields to uppercase hex format (0xXXXXXXXX or 0xXXXXXXXXXXXXXXXX).\"\"\"\n",
    "    logger.debug(f\"Normalizing {field_name}: {value}\")\n",
    "    if pd.isna(value) or value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            # Handle multi-valued strings (e.g., '0xC05D0000,0x00000000')\n",
    "            value = value.split(',')[0].strip().lower().replace('0x', '')\n",
    "            value = int(value, 16)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            value = int(value)\n",
    "        else:\n",
    "            logger.warning(f\"Invalid {field_name} type: {type(value)}\")\n",
    "            return None\n",
    "        if field_name in ['smb2.sesid', 'smb2.msg_id']:\n",
    "            normalized = f\"0x{value:016X}\"  # 64-bit fields\n",
    "        elif field_name == 'smb2.create.action':\n",
    "            normalized = str(value)  # Keep as integer string for mapping\n",
    "        else:\n",
    "            normalized = f\"0x{value:08X}\"  # 32-bit fields\n",
    "        logger.debug(f\"Normalized {field_name} to: {normalized}\")\n",
    "        return normalized\n",
    "    except (ValueError, TypeError) as e:\n",
    "        logger.warning(f\"Invalid {field_name} format: {value}, error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Normalization function for smb2.fid\n",
    "def normalize_fid(value):\n",
    "    \"\"\"Normalize smb2.fid, handling UUID or hex formats.\"\"\"\n",
    "    logger.debug(f\"Normalizing smb2.fid: {value}\")\n",
    "    if pd.isna(value) or value is None or value == '':\n",
    "        return None\n",
    "    try:\n",
    "        if isinstance(value, str):\n",
    "            # Handle UUID-like format (e.g., '01d01154-9d3e-80b6-c1e9-ce0000000000')\n",
    "            if '-' in value:\n",
    "                uuid_str = value.replace('-', '')\n",
    "                uuid_obj = uuid.UUID(uuid_str)\n",
    "                normalized = f\"0x{uuid_obj.int:032X}\"\n",
    "                logger.debug(f\"Normalized UUID smb2.fid to: {normalized}\")\n",
    "                return normalized\n",
    "            # Handle multi-valued strings or hex\n",
    "            value = value.split(',')[0].strip().lower().replace('0x', '')\n",
    "            value = int(value, 16)\n",
    "        elif isinstance(value, (int, float)):\n",
    "            value = int(value)\n",
    "        else:\n",
    "            logger.warning(f\"Invalid smb2.fid type: {type(value)}\")\n",
    "            return None\n",
    "        normalized = f\"0x{value:032X}\"  # 128-bit field\n",
    "        logger.debug(f\"Normalized smb2.fid to: {normalized}\")\n",
    "        return normalized\n",
    "    except (ValueError, TypeError) as e:\n",
    "        logger.debug(f\"Invalid smb2.fid format: {value}, error: {e}\")\n",
    "        return None  # Suppress warning to reduce log spam\n",
    "\n",
    "# Combine info level mappings\n",
    "INFO_LEVEL_MAPPING = {\n",
    "    **{str(k): v for k, v in FILE_INFO_CLASSES.items()},\n",
    "    **{str(k): v for k, v in SMB2_INFO_LEVELS.items()},\n",
    "    **{str(k): v for k, v in SMB2_FILE_INFO_CLASSES.items()}\n",
    "}\n",
    "logger.debug(f\"Combined {len(INFO_LEVEL_MAPPING)} info level mappings\")\n",
    "\n",
    "FIELD_MAPPINGS = {\n",
    "    \"smb2.cmd\": {\n",
    "        \"mapping\": {str(k): v[0] if not v[1] else f\"{v[0]} / {v[1]}\" for k, v in SMB2_OP_NAME_DESC.items()},\n",
    "        \"normalize\": lambda x: str(int(float(x.split(',')[0].strip()))) if x and pd.notna(x) and isinstance(x, str) else str(int(x)) if x and pd.notna(x) else None,\n",
    "        \"description\": \"Maps SMB2 command codes to operation names (request/response).\"\n",
    "    },\n",
    "    \"smb2.nt_status\": {\n",
    "        \"mapping\": {k: v[0] for k, v in ERROR_MESSAGES.items()},\n",
    "        \"normalize\": lambda x: normalize_hex_field(x, \"smb2.nt_status\"),\n",
    "        \"description\": \"Maps NT status codes to error names (impacket).\"\n",
    "    },\n",
    "    \"smb2.ioctl.function\": {\n",
    "        \"mapping\": {str(f\"0x{v:08X}\"): k for k, v in FSCTL_CONSTANTS.items()},\n",
    "        \"normalize\": lambda x: normalize_hex_field(x, \"smb2.ioctl.function\"),\n",
    "        \"description\": \"Maps IOCTL function codes to FSCTL names.\"\n",
    "    },\n",
    "    \"smb2.tid\": {\n",
    "        \"mapping\": {},\n",
    "        \"normalize\": lambda x: normalize_hex_field(x, \"smb2.tid\"),\n",
    "        \"description\": \"Normalizes tree ID to hex format.\"\n",
    "    },\n",
    "    \"smb2.sesid\": {\n",
    "        \"mapping\": {},\n",
    "        \"normalize\": lambda x: normalize_hex_field(x, \"smb2.sesid\"),\n",
    "        \"description\": \"Normalizes session ID to hex format.\"\n",
    "    },\n",
    "    \"smb2.msg_id\": {\n",
    "        \"mapping\": {},\n",
    "        \"normalize\": lambda x: normalize_hex_field(x, \"smb2.msg_id\"),\n",
    "        \"description\": \"Normalizes message ID to hex format.\"\n",
    "    },\n",
    "    \"smb2.fid\": {\n",
    "        \"mapping\": {},\n",
    "        \"normalize\": normalize_fid,\n",
    "        \"description\": \"Normalizes file ID to 128-bit hex format, handling UUIDs.\"\n",
    "    },\n",
    "    \"smb2.infolevel\": {\n",
    "        \"mapping\": INFO_LEVEL_MAPPING,\n",
    "        \"normalize\": lambda x: str(int(x)) if x and pd.notna(x) else None,\n",
    "        \"description\": \"Maps info level codes to file, directory, and filesystem info class names.\"\n",
    "    },\n",
    "    \"smb2.create.action\": {\n",
    "        \"mapping\": {str(k): v for k, v in CREATE_ACTION_DESC.items()},\n",
    "        \"normalize\": lambda x: normalize_hex_field(x, \"smb2.create.action\"),\n",
    "        \"description\": \"Maps create action codes to action names (e.g., FILE_OPENED).\"\n",
    "    }\n",
    "}\n",
    "logger.info(f\"Field mappings defined for: {', '.join(FIELD_MAPPINGS.keys())}\")\n",
    "logger.info(f\"Hex fields tracked: {', '.join(HEX_FIELDS)}\")\n",
    "\n",
    "builtins.FIELDS = FIELDS\n",
    "builtins.FIELD_MAPPINGS = FIELD_MAPPINGS\n",
    "builtins.HEX_FIELDS = HEX_FIELDS\n",
    "builtins.CREATE_ACTION_DESC = CREATE_ACTION_DESC  # Export for use in other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f913b-e5e8-4b84-9e2a-b0dbe7d262c4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    }
   },
   "outputs": [],
   "source": [
    "# Cell 4: Define ntap-tshark processing functions\n",
    "from typing import List, Dict, Tuple, Any\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shlex\n",
    "import os\n",
    "import json\n",
    "import psutil\n",
    "import traceback\n",
    "import builtins\n",
    "\n",
    "# Import from builtins (set in Cells 1–3)\n",
    "from builtins import logger, check_ssh_connectivity, FIELDS\n",
    "\n",
    "SSH_KEY = \"/home/jovyan/.ssh/id_rsa\"\n",
    "SSH_USER = \"root\"\n",
    "SSH_HOST = \"backend\"\n",
    "NTAPSHARK_PATH = \"/usr/local/bin/ntap-tshark\"\n",
    "PARQUET_FILE = \"tshark_output_full.parquet\"\n",
    "\n",
    "logger.info(f\"Fields being extracted: {len(FIELDS)} fields\")\n",
    "logger.debug(f\"Fields (first 50): {', '.join(FIELDS[:50])}...\")\n",
    "\n",
    "def build_tshark_command(capture, fields, reassembly=False, packet_limit=None, log_level=\"debug\", temp_dir=\"/tmp\", verbose=False):\n",
    "    \"\"\"Construct SSH command for ntap-tshark to process PCAP.\"\"\"\n",
    "    logger.info(f\"Building tshark command for capture: {capture}\")\n",
    "    tshark_args = [\n",
    "        NTAPSHARK_PATH,\n",
    "        \"--log-level\", log_level,\n",
    "        \"--temp-dir\", temp_dir,\n",
    "        \"-r\", capture,\n",
    "        \"-Y\", \"smb2\",\n",
    "        \"-T\", \"fields\",\n",
    "        \"-E\", \"separator=|\",\n",
    "        \"-E\", \"header=y\",\n",
    "        \"-E\", \"occurrence=a\",\n",
    "        \"-q\"\n",
    "    ]\n",
    "    \n",
    "    if reassembly:\n",
    "        tshark_args.append(\"-2\")\n",
    "        logger.debug(\"Enabled TCP reassembly with -2 flag\")\n",
    "    if packet_limit is not None:\n",
    "        tshark_args.extend([\"-c\", str(packet_limit)])\n",
    "        logger.debug(f\"Set packet limit to {packet_limit}\")\n",
    "    if verbose:\n",
    "        tshark_args.append(\"-V\")\n",
    "        logger.debug(\"Enabled verbose tshark output with -V flag\")\n",
    "    \n",
    "    for field in fields:\n",
    "        tshark_args.extend([\"-e\", field])\n",
    "    \n",
    "    quoted_tshark_cmd = \" \".join(shlex.quote(arg) for arg in tshark_args)\n",
    "    cmd = [\n",
    "        \"ssh\",\n",
    "        \"-i\", SSH_KEY,\n",
    "        \"-p\", \"22\",\n",
    "        f\"{SSH_USER}@{SSH_HOST}\",\n",
    "        quoted_tshark_cmd\n",
    "    ]\n",
    "    \n",
    "    logger.debug(f\"Constructed SSH command: {' '.join(cmd)[:400]}...\")\n",
    "    logger.debug(f\"Quoted ntap-tshark command: {quoted_tshark_cmd[:400]}...\")\n",
    "    \n",
    "    return cmd, fields\n",
    "\n",
    "def extract_fields(line: str, fields: List[str]) -> Tuple[int, int, str, str, str, Dict[str, str]]:\n",
    "    \"\"\"Parse a tshark output line into a field dictionary.\"\"\"\n",
    "    logger.debug(f\"Extracting fields from line: {line.strip()[:100]}...\")\n",
    "    try:\n",
    "        split_line = line.split(\"|\")\n",
    "        if not split_line or not split_line[0].strip() or len(split_line) < 5:\n",
    "            logger.warning(f\"Invalid line format (split on |): {line.strip()[:100]}...\")\n",
    "            return 0, -1, \"\", \"\", \"\", {}\n",
    "        \n",
    "        if len(split_line) < len(fields):\n",
    "            logger.warning(f\"Line has {len(split_line)} fields, expected at least {len(fields)}: {line.strip()[:100]}...\")\n",
    "            split_line.extend([\"\"] * (len(fields) - len(split_line)))\n",
    "        \n",
    "        field_dict = {}\n",
    "        for i, value in enumerate(split_line[:len(fields)]):\n",
    "            cleaned_value = value.split(\"\\x02\")[0] if value else \"\"\n",
    "            field_dict[fields[i]] = cleaned_value\n",
    "        \n",
    "        frame_number = field_dict.get(\"frame.number\", \"\")\n",
    "        if frame_number.isdigit() and int(frame_number) <= 10:\n",
    "            logger.debug(f\"Extracted fields (first 5): {dict(list(field_dict.items())[:5])}\")\n",
    "        \n",
    "        frame = 0\n",
    "        stream_str = field_dict.get(\"tcp.stream\", \"\")\n",
    "        stream = int(stream_str) if stream_str and stream_str.isdigit() else -1\n",
    "        if not stream_str or not stream_str.isdigit():\n",
    "            logger.warning(f\"Invalid tcp.stream '{stream_str}' in line: {line.strip()[:100]}...\")\n",
    "        \n",
    "        ip_src = field_dict.pop(\"ip.src\", field_dict.pop(\"ipv6.src\", \"\"))\n",
    "        ip_dst = field_dict.pop(\"ip.dst\", field_dict.pop(\"ipv6.dst\", \"\"))\n",
    "        sesid = field_dict.pop(\"smb2.sesid\", \"\")\n",
    "        field_dict.pop(\"frame.number\", None)\n",
    "        field_dict.pop(\"tcp.stream\", None)\n",
    "        \n",
    "        key_fields = ['smb2.cmd', 'smb2.filename', 'smb2.tid']\n",
    "        for field in key_fields:\n",
    "            if field not in field_dict:\n",
    "                field_dict[field] = \"\"\n",
    "        \n",
    "        multi_value_fields = ['smb2.sesid', 'smb2.cmd', 'smb2.filename', 'smb2.tid', 'smb2.nt_status', 'smb2.msg_id']\n",
    "        for field in multi_value_fields:\n",
    "            if field in field_dict and field_dict[field]:\n",
    "                if ',' in field_dict[field]:\n",
    "                    field_dict[field] = [v.strip() for v in field_dict[field].split(',') if v.strip()]\n",
    "                else:\n",
    "                    field_dict[field] = [field_dict[field]] if field_dict[field] else []\n",
    "        \n",
    "        return frame, stream, ip_src, ip_dst, sesid, field_dict\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in extract_fields: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        return 0, -1, \"\", \"\", \"\", {}\n",
    "\n",
    "def process_tshark_output(cmd: List[str], fields: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"Process tshark output into a DataFrame.\"\"\"\n",
    "    logger.info(f\"Processing tshark output with command: {' '.join(cmd)[:200]}...\")\n",
    "    try:\n",
    "        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        data = []\n",
    "        line_count = 0\n",
    "        skip_count = 0\n",
    "        header_skipped = False\n",
    "        header_fields = None\n",
    "        \n",
    "        for line in proc.stdout:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "                \n",
    "            if not header_skipped:\n",
    "                header_fields = line.split(\"|\")\n",
    "                if header_fields != fields[:len(header_fields)]:\n",
    "                    logger.warning(f\"Header fields mismatch! Expected: {fields[:10]}, Got: {header_fields[:10]}\")\n",
    "                logger.debug(f\"Header row: {line[:200]}...\")\n",
    "                header_skipped = True\n",
    "                continue\n",
    "            \n",
    "            line_count += 1\n",
    "            if line_count % 1000 == 0:\n",
    "                logger.info(f\"Processed {line_count} lines, memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "            if line_count <= 10:\n",
    "                logger.debug(f\"Raw line {line_count}: {line[:200]}...\")\n",
    "            try:\n",
    "                frame, stream, ip_src, ip_dst, sesid, field_dict = extract_fields(line, fields)\n",
    "                corrected_frame = line_count\n",
    "                field_dict['frame.number'] = str(corrected_frame)\n",
    "                record = {\n",
    "                    \"frame.number\": corrected_frame,\n",
    "                    \"tcp.stream\": stream,\n",
    "                    \"ip.src\": ip_src,\n",
    "                    \"ip.dst\": ip_dst,\n",
    "                    \"smb2.sesid\": sesid,\n",
    "                    **field_dict\n",
    "                }\n",
    "                data.append(record)\n",
    "            except (KeyError, ValueError) as e:\n",
    "                skip_count += 1\n",
    "                if skip_count <= 5:\n",
    "                    logger.warning(f\"Skipping line {line_count} due to error: {e} - Raw: {line[:100]}...\")\n",
    "                continue\n",
    "        \n",
    "        proc.stdout.close()\n",
    "        proc.wait()\n",
    "        \n",
    "        if proc.returncode != 0:\n",
    "            stderr_output = proc.stderr.read()\n",
    "            logger.critical(f\"tshark failed with exit code {proc.returncode}, stderr: {stderr_output}\")\n",
    "            raise subprocess.CalledProcessError(proc.returncode, cmd, output=json.dumps(data, indent=2) if data else \"\", stderr=stderr_output)\n",
    "        \n",
    "        if not data:\n",
    "            logger.critical(\"No data extracted from tshark output\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        logger.info(f\"Creating DataFrame with {len(data)} records, memory usage: {psutil.Process().memory_info().rss / 1024**2:.2f} MB\")\n",
    "        defined_columns = [\"frame.number\", \"tcp.stream\", \"ip.src\", \"ip.dst\", \"smb2.sesid\"]\n",
    "        unique_fields = [f for f in fields if f not in defined_columns]\n",
    "        df = pd.DataFrame(data, columns=defined_columns + unique_fields)\n",
    "        logger.info(f\"Processed {len(df)} frames from tshark output, skipped {skip_count} lines\")\n",
    "        logger.info(f\"Total lines processed: {line_count}\")\n",
    "        \n",
    "        # Log DataFrame memory usage before optimization\n",
    "        df_memory_mb = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        logger.info(f\"DataFrame memory usage before optimization: {df_memory_mb:.2f} MB\")\n",
    "        \n",
    "        # Normalize smb2.sesid early\n",
    "        if 'smb2.sesid' in df.columns:\n",
    "            multi_value_count = df['smb2.sesid'].str.contains(',').sum()\n",
    "            logger.debug(f\"Found {multi_value_count} rows with multi-valued smb2.sesid\")\n",
    "            df['smb2.sesid'] = df['smb2.sesid'].apply(lambda x: ','.join(x) if isinstance(x, list) else x if x else '')\n",
    "            logger.debug(f\"Normalized smb2.sesid values (first 10): {list(df['smb2.sesid'].head(10))}\")\n",
    "        \n",
    "        if 'tcp.stream' in df.columns:\n",
    "            df['tcp.stream'] = pd.to_numeric(df['tcp.stream'], errors='coerce', downcast='integer')\n",
    "            logger.info(f\"Converted tcp.stream to dtype: {df['tcp.stream'].dtype}\")\n",
    "        \n",
    "        # Downcast numeric columns\n",
    "        for col in df.columns:\n",
    "            if df[col].dtype == 'float64':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce', downcast='float')\n",
    "            elif df[col].dtype == 'int64':\n",
    "                df[col] = pd.to_numeric(df[col], errors='coerce', downcast='integer')\n",
    "        \n",
    "        hex_fields = ['smb2.nt_status', 'smb2.tid', 'smb2.sesid', 'smb2.fid', 'smb2.flags']\n",
    "        for col in hex_fields:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].map(lambda x: x[0] if isinstance(x, list) and x else x if not isinstance(x, list) else '')\n",
    "                if col == 'smb2.tid':\n",
    "                    logger.debug(f\"Normalized smb2.tid values (first 10): {list(df['smb2.tid'].head(10))}\")\n",
    "        \n",
    "        if 'frame.number' in df.columns:\n",
    "            df['frame.number'] = pd.to_numeric(df['frame.number'], errors='coerce', downcast='integer')\n",
    "        if 'ip.src' in df.columns:\n",
    "            df['ip.src'] = df['ip.src'].astype('string')\n",
    "        if 'ip.dst' in df.columns:\n",
    "            df['ip.dst'] = df['ip.dst'].astype('string')\n",
    "        if 'smb2.msg_id' in df.columns:\n",
    "            df['smb2.msg_id'] = df['smb2.msg_id'].map(lambda x: x[0] if isinstance(x, list) and x else x)\n",
    "            df['smb2.msg_id'] = pd.to_numeric(df['smb2.msg_id'], errors='coerce').astype('UInt64')\n",
    "        \n",
    "        # Log DataFrame memory usage after optimization\n",
    "        df_memory_mb_opt = df.memory_usage(deep=True).sum() / 1024**2\n",
    "        logger.info(f\"DataFrame memory usage after optimization: {df_memory_mb_opt:.2f} MB\")\n",
    "        \n",
    "        logger.debug(\"Hex fields after processing:\")\n",
    "        logger.debug(df[['smb2.nt_status', 'smb2.tid', 'smb2.sesid']].head().to_string())\n",
    "        logger.debug(\"Multi-value fields before processing:\")\n",
    "        logger.debug(df[['smb2.filename', 'smb2.cmd']].head().to_string())\n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in process_tshark_output: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "def save_to_parquet(df, parquet_path):\n",
    "    \"\"\"Save DataFrame to Parquet file.\"\"\"\n",
    "    logger.info(f\"Saving DataFrame to {parquet_path}\")\n",
    "    try:\n",
    "        multi_value_fields = ['smb2.sesid', 'smb2.cmd', 'smb2.filename', 'smb2.tid', 'smb2.nt_status', 'smb2.msg_id']\n",
    "        df_copy = df.copy()\n",
    "        for col in multi_value_fields:\n",
    "            if col in df_copy.columns:\n",
    "                df_copy[col] = df_copy[col].apply(lambda x: ','.join(map(str, x)) if isinstance(x, list) else str(x) if x else '')\n",
    "        table = pa.Table.from_pandas(df_copy, preserve_index=False)\n",
    "        pq.write_table(table, parquet_path, compression='zstd')\n",
    "        logger.info(f\"Saved DataFrame to {parquet_path}\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error saving Parquet file {parquet_path}: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "def create_remote_directory(case_number: str, trace_name: str, force_reingest: bool = False) -> str:\n",
    "    \"\"\"Create remote directory for session storage.\"\"\"\n",
    "    logger.info(f\"Creating remote directory for case {case_number}, trace {trace_name}\")\n",
    "    try:\n",
    "        if not check_ssh_connectivity():\n",
    "            logger.critical(f\"Cannot connect to {SSH_HOST} for directory creation\")\n",
    "            raise RuntimeError(f\"SSH connection to {SSH_HOST} failed\")\n",
    "        \n",
    "        base_dir = os.path.join(\"/stingray\", case_number)\n",
    "        tracer_dir = os.path.join(base_dir, \".tracer\")\n",
    "        pcap_dir = os.path.join(tracer_dir, trace_name.split('.')[0])  # Remove all extensions consistently\n",
    "        output_dir = os.path.join(pcap_dir, \"sessions\")\n",
    "        \n",
    "        cmd = [\n",
    "            \"ssh\", \"-i\", SSH_KEY, \"-p\", \"22\", f\"{SSH_USER}@{SSH_HOST}\",\n",
    "            f\"mkdir -p {shlex.quote(output_dir)} && chmod -R 777 {shlex.quote(base_dir)}\"\n",
    "        ]\n",
    "        logger.debug(f\"Executing directory creation command: {' '.join(cmd)}\")\n",
    "        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        logger.info(f\"Created remote directory: {output_dir} with 777 permissions (stdout: {result.stdout})\")\n",
    "        status_callback(f\"Created remote directory: {output_dir}\")\n",
    "        \n",
    "        if force_reingest:\n",
    "            clear_cmd = [\n",
    "                \"ssh\", \"-i\", SSH_KEY, \"-p\", \"22\", f\"{SSH_USER}@{SSH_HOST}\",\n",
    "                f\"rm -rf {shlex.quote(os.path.join(output_dir, '*'))}\"\n",
    "            ]\n",
    "            logger.debug(f\"Executing clear command for force_reingest: {' '.join(clear_cmd)}\")\n",
    "            result = subprocess.run(clear_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            logger.info(f\"Cleared {output_dir} due to force_reingest (stdout: {result.stdout})\")\n",
    "        \n",
    "        check_cmd = [\n",
    "            \"ssh\", \"-i\", SSH_KEY, \"-p\", \"22\", f\"{SSH_USER}@{SSH_HOST}\",\n",
    "            f\"test -d {shlex.quote(output_dir)} && test -w {shlex.quote(output_dir)}\"\n",
    "        ]\n",
    "        try:\n",
    "            logger.debug(f\"Verifying directory: {' '.join(check_cmd)}\")\n",
    "            result = subprocess.run(check_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            logger.info(f\"Verified {output_dir} exists and is writable\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.critical(f\"Error: {output_dir} not writable after creation: {e.stderr}\")\n",
    "            status_callback(f\"Error: {output_dir} not writable\")\n",
    "            raise\n",
    "        \n",
    "        return output_dir\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.critical(f\"Error creating remote directory {output_dir}: {e.stderr} (stdout: {e.stdout})\")\n",
    "        status_callback(f\"Error creating remote directory: {e.stderr}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in create_remote_directory: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        status_callback(f\"Error in create_remote_directory: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def clear_directory(directory: str):\n",
    "    \"\"\"Clear all files in a remote directory.\"\"\"\n",
    "    logger.info(f\"Clearing directory: {directory}\")\n",
    "    try:\n",
    "        if not check_ssh_connectivity():\n",
    "            logger.critical(f\"Cannot connect to {SSH_HOST} for directory clearing\")\n",
    "            raise RuntimeError(f\"SSH connection to {SSH_HOST} failed\")\n",
    "        \n",
    "        cmd = [\"ssh\", \"-i\", SSH_KEY, \"-p\", \"22\", f\"{SSH_USER}@{SSH_HOST}\",\n",
    "               f\"rm -rf {shlex.quote(directory)}/*\"]\n",
    "        logger.debug(f\"Executing clear command: {' '.join(cmd)}\")\n",
    "        result = subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        logger.info(f\"Cleared all files in {directory} on {SSH_HOST} (stdout: {result.stdout})\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.critical(f\"Failed to clear directory {directory}: stderr={e.stderr}, stdout={e.stdout}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in clear_directory: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "def status_callback(message):\n",
    "    \"\"\"Log status messages.\"\"\"\n",
    "    logger.info(f\"Status: {message}\")\n",
    "\n",
    "builtins.status_callback = status_callback\n",
    "builtins.build_tshark_command = build_tshark_command\n",
    "builtins.process_tshark_output = process_tshark_output\n",
    "builtins.save_to_parquet = save_to_parquet\n",
    "builtins.create_remote_directory = create_remote_directory\n",
    "builtins.clear_directory = clear_directory\n",
    "\n",
    "logger.info(\"NTAPshark processing functions and utilities defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8cd6bf-5586-48a4-a83e-af74ece46e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Define ingestion and session extraction logic\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import traceback\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import subprocess\n",
    "import shlex\n",
    "import psutil\n",
    "import builtins\n",
    "from typing import Dict\n",
    "\n",
    "# Import required functions and variables from builtins with error handling\n",
    "try:\n",
    "    from builtins import logger, status_callback, build_tshark_command, process_tshark_output, create_remote_directory, clear_directory, save_to_parquet, check_ssh_connectivity, FIELDS\n",
    "    print(\"Successfully imported all required functions from builtins\")\n",
    "except ImportError as e:\n",
    "    print(f\"ImportError in Cell 5: {e}\")\n",
    "    print(\"Make sure all previous cells (1-4) have been executed successfully\")\n",
    "    raise\n",
    "\n",
    "_unique_sesids = set()\n",
    "\n",
    "def get_packet_count(capture_path):\n",
    "    \"\"\"Retrieve the number of packets in a PCAP file using ntap-capinfos.\"\"\"\n",
    "    logger.info(f\"Retrieving packet count for {capture_path}\")\n",
    "    try:\n",
    "        cmd = [\n",
    "            \"ssh\", \"-i\", \"/home/jovyan/.ssh/id_rsa\", \"-p\", \"22\", \"root@backend\",\n",
    "            f\"/usr/local/bin/ntap-capinfos -c {shlex.quote(capture_path)}\"\n",
    "        ]\n",
    "        logger.debug(f\"Executing capinfos command: {' '.join(cmd)}\")\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n",
    "        output = result.stdout\n",
    "        for line in output.splitlines():\n",
    "            if \"Number of packets:\" in line:\n",
    "                count_str = line.split(\":\")[1].strip()\n",
    "                try:\n",
    "                    count_str_lower = count_str.lower()\n",
    "                    if 'k' in count_str_lower:\n",
    "                        count = int(float(count_str_lower.replace('k', '')) * 1000)\n",
    "                    elif 'm' in count_str_lower:\n",
    "                        count = int(float(count_str_lower.replace('m', '')) * 1000000)\n",
    "                    else:\n",
    "                        count = int(count_str)\n",
    "                    logger.info(f\"Packet count for {capture_path}: {count}\")\n",
    "                    return count\n",
    "                except ValueError:\n",
    "                    logger.critical(f\"Invalid packet count format: {count_str}\")\n",
    "                    status_callback(f\"Error - Invalid packet count format: {count_str}\")\n",
    "                    return None\n",
    "        logger.critical(f\"Could not parse packet count from capinfos output: {output}\")\n",
    "        status_callback(f\"Error - Could not parse packet count for {capture_path}\")\n",
    "        return None\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.critical(f\"Error running capinfos: {e.stderr}\")\n",
    "        status_callback(f\"Error running capinfos for {capture_path}: {e.stderr}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in get_packet_count: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        status_callback(f\"Error in get_packet_count: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def normalize_sesid(sesid_str):\n",
    "    \"\"\"Normalize smb2.sesid values, handling lists and commas.\"\"\"\n",
    "    logger.debug(f\"Normalizing sesid: {str(sesid_str)[:200]}\")\n",
    "    try:\n",
    "        if pd.isna(sesid_str) or not sesid_str:\n",
    "            return []\n",
    "        if isinstance(sesid_str, list):\n",
    "            sesids = list(dict.fromkeys(item.strip() for item in sesid_str if item and item != \"0x0000000000000000\"))\n",
    "        else:\n",
    "            sesids = list(dict.fromkeys(item.strip() for item in sesid_str.split(',') if item and item != \"0x0000000000000000\"))\n",
    "        \n",
    "        sesid_key = str(sesids)[:200]\n",
    "        if sesid_key not in _unique_sesids:\n",
    "            logger.debug(f\"Normalized sesid to: {sesid_key}\")\n",
    "            _unique_sesids.add(sesid_key)\n",
    "        \n",
    "        return sesids\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in normalize_sesid: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        return []\n",
    "\n",
    "def normalize_cmd(cmd_str):\n",
    "    \"\"\"Normalize smb2.cmd values, handling lists and commas.\"\"\"\n",
    "    logger.debug(f\"Normalizing cmd: {str(cmd_str)[:200]}\")\n",
    "    try:\n",
    "        if pd.isna(cmd_str).any() if isinstance(cmd_str, (list, pd.Series)) else pd.isna(cmd_str):\n",
    "            return []\n",
    "        if not cmd_str:\n",
    "            return []\n",
    "        if isinstance(cmd_str, list):\n",
    "            return [item.strip() for item in cmd_str if item]\n",
    "        return [item.strip() for item in cmd_str.split(',') if item]\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in normalize_cmd: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        return []\n",
    "\n",
    "def save_session_metadata(case_number: str, trace_name: str, sessions: Dict[str, pd.DataFrame], output_dir: str):\n",
    "    \"\"\"Save session metadata to JSON.\"\"\"\n",
    "    logger.info(f\"Saving session metadata to {output_dir}\")\n",
    "    try:\n",
    "        metadata = {\n",
    "            \"case_number\": case_number,\n",
    "            \"trace_name\": trace_name,\n",
    "            \"session_count\": len(sessions),\n",
    "            \"session_details\": {sesid: {\"frame_count\": len(df), \"columns\": len(df.columns)} for sesid, df in sessions.items()}\n",
    "        }\n",
    "        metadata_path = os.path.join(output_dir, \"session_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=2)\n",
    "        logger.info(f\"Saved session metadata to {metadata_path}\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in save_session_metadata: {str(e)}\\n{traceback.format_exc()}\")\n",
    "        raise\n",
    "\n",
    "def run_ingestion(capture_path=None, reassembly_enabled=False, force_reingest=False, verbose=False):\n",
    "    \"\"\"Orchestrate PCAP ingestion and session extraction.\"\"\"\n",
    "    global _unique_sesids\n",
    "    logger.info(f\"Starting ingestion with capture_path: {capture_path}, reassembly_enabled: {reassembly_enabled}, force_reingest: {force_reingest}, verbose: {verbose}\")\n",
    "    \n",
    "    # Get status_callback from builtins first, before any validation\n",
    "    if hasattr(builtins, 'status_callback') and callable(builtins.status_callback):\n",
    "        status_callback = builtins.status_callback\n",
    "        logger.debug(\"Using status_callback from builtins\")\n",
    "    else:\n",
    "        # Fallback status_callback\n",
    "        def status_callback(message):\n",
    "            logger.info(f\"Status: {message}\")\n",
    "        logger.warning(\"Using fallback status_callback - builtins.status_callback not available\")\n",
    "    \n",
    "    try:\n",
    "        # Validate that all required functions are available and callable\n",
    "        required_functions = {\n",
    "            'status_callback': status_callback,\n",
    "            'build_tshark_command': build_tshark_command,\n",
    "            'process_tshark_output': process_tshark_output,\n",
    "            'create_remote_directory': create_remote_directory,\n",
    "            'save_to_parquet': save_to_parquet,\n",
    "            'check_ssh_connectivity': check_ssh_connectivity\n",
    "        }\n",
    "        \n",
    "        for func_name, func in required_functions.items():\n",
    "            if not callable(func):\n",
    "                raise ValueError(f\"Required function '{func_name}' is not callable: {type(func)}\")\n",
    "        \n",
    "        logger.debug(\"All required functions validated as callable\")\n",
    "\n",
    "        # Initialize _unique_sesids\n",
    "        _unique_sesids = set()\n",
    "        logger.debug(\"Initialized _unique_sesids set\")\n",
    "\n",
    "        # Load capture_path from config if None\n",
    "        config_file = \"/home/jovyan/work/smbreplay/config.pkl\"\n",
    "        if capture_path is None and os.path.exists(config_file):\n",
    "            try:\n",
    "                with open(config_file, 'rb') as f:\n",
    "                    settings = pickle.load(f)\n",
    "                    capture_path = settings.get(\"capture_path\")\n",
    "                logger.info(f\"Loaded capture_path from config.pkl: {capture_path}\")\n",
    "            except (pickle.PickleError, IOError) as e:\n",
    "                logger.critical(f\"Error loading {config_file}: {e}\")\n",
    "                status_callback(f\"Error - Failed to load {config_file}: {e}\")\n",
    "                return None\n",
    "\n",
    "        if not capture_path:\n",
    "            logger.critical(\"No valid capture path available for re-ingestion\")\n",
    "            status_callback(\"Critical: No valid capture path available for re-ingestion\")\n",
    "            return None\n",
    "\n",
    "        capture_path = os.path.abspath(capture_path)\n",
    "        trace_name = os.path.basename(capture_path).split('.')[0]\n",
    "        logger.info(f\"Validating PCAP: {capture_path}\")\n",
    "        status_callback(f\"Starting ingestion for {trace_name}\")\n",
    "\n",
    "        if not os.path.exists(capture_path):\n",
    "            logger.critical(f\"PCAP file not found: {capture_path}\")\n",
    "            status_callback(f\"Error - PCAP file not found: {capture_path}\")\n",
    "            return None\n",
    "\n",
    "        # Validate PCAP with tshark\n",
    "        try:\n",
    "            validate_cmd = [\n",
    "                \"ssh\", \"-i\", \"/home/jovyan/.ssh/id_rsa\", \"-p\", \"22\", \"root@backend\",\n",
    "                f\"/usr/local/bin/ntap-tshark -r {shlex.quote(capture_path)} -c 1\"\n",
    "            ]\n",
    "            logger.debug(f\"Executing PCAP validation command: {' '.join(validate_cmd)}\")\n",
    "            result = subprocess.run(validate_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "            logger.debug(f\"PCAP validation stdout: {result.stdout}\")\n",
    "            logger.info(f\"PCAP file {trace_name} validated\")\n",
    "            status_callback(f\"PCAP file {trace_name} validated\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.critical(f\"Error validating PCAP: {e.stderr}\")\n",
    "            status_callback(f\"Error - PCAP file invalid or corrupt: {e.stderr}\")\n",
    "            return None\n",
    "        \n",
    "        if not check_ssh_connectivity():\n",
    "            logger.critical(\"Cannot connect to SSH_HOST for ntap-tshark\")\n",
    "            status_callback(\"Error - Cannot connect to SSH_HOST for ntap-tshark\")\n",
    "            return None\n",
    "        \n",
    "        packet_count = get_packet_count(capture_path)\n",
    "        packet_limit = 10000 if packet_count is not None and packet_count > 10000 else None\n",
    "        logger.info(f\"Packet limit set to: {packet_limit if packet_limit is not None else 'None (full capture)'}\")\n",
    "        status_callback(f\"Packet limit set to: {packet_limit if packet_limit is not None else 'None (full capture)'}\")\n",
    "        \n",
    "        parts = capture_path.split(os.sep)\n",
    "        case_number = parts[2] if len(parts) >= 3 and parts[1] == 'stingray' else \"unknown\"\n",
    "        logger.info(f\"Ingesting {trace_name} for case {case_number}\")\n",
    "        status_callback(f\"Ingesting {trace_name} for case {case_number}\")\n",
    "        \n",
    "        base_fields = [\n",
    "            \"frame.number\", \"frame.time_epoch\", \"ip.src\", \"ip.dst\", \"smb2.sesid\", \n",
    "            \"smb2.cmd\", \"smb2.filename\", \"smb2.tid\", \"smb2.nt_status\", \"smb2.msg_id\"\n",
    "        ]\n",
    "        additional_fields = FIELDS\n",
    "        if \"smb2.ioctl.function\" not in additional_fields:\n",
    "            additional_fields.append(\"smb2.ioctl.function\")\n",
    "        fields = list(OrderedDict.fromkeys(base_fields + additional_fields))\n",
    "        logger.info(f\"Using {len(fields)} fields for tshark extraction\")\n",
    "        status_callback(f\"Using {len(fields)} fields for tshark extraction\")\n",
    "        \n",
    "        cmd, used_fields = build_tshark_command(capture_path, fields, reassembly=reassembly_enabled, packet_limit=packet_limit, verbose=verbose)\n",
    "        logger.debug(f\"Full tshark command: {' '.join(cmd)[:400]}...\")\n",
    "        status_callback(f\"Full tshark command: {' '.join(cmd)[:400]}...\")\n",
    "        \n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            df = process_tshark_output(cmd, used_fields)\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.critical(f\"Error during ingestion: {e.stderr}\")\n",
    "            status_callback(f\"Error during ingestion: {e.stderr}\")\n",
    "            return None\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.critical(\"No data extracted from tshark output\")\n",
    "            status_callback(\"Error - No data extracted from tshark output\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Processed {len(df)} frames\")\n",
    "        status_callback(f\"Processed {len(df)} frames\")\n",
    "        \n",
    "        logger.info(f\"Extracting unique session IDs from {len(df)} rows\")\n",
    "        status_callback(f\"Extracting unique session IDs from {len(df)} rows\")\n",
    "        try:\n",
    "            unique_sesids = df['smb2.sesid'].apply(normalize_sesid).explode().unique()\n",
    "            unique_sesids = [s for s in unique_sesids if s and str(s).lower() != 'nan']\n",
    "            logger.info(f\"Found {len(unique_sesids)} unique session IDs\")\n",
    "            status_callback(f\"Found {len(unique_sesids)} unique session IDs\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Error extracting session IDs: {e}\")\n",
    "            status_callback(f\"Error extracting session IDs: {e}\")\n",
    "            return None\n",
    "        \n",
    "        logger.info(f\"Extracting {len(unique_sesids)} sessions\")\n",
    "        status_callback(f\"Extracting {len(unique_sesids)} sessions\")\n",
    "        \n",
    "        sessions = {}\n",
    "        for i, sesid in enumerate(unique_sesids, 1):\n",
    "            logger.debug(f\"Processing session {i}/{len(unique_sesids)} - sesid: {sesid}\")\n",
    "            status_callback(f\"Processing session {i}/{len(unique_sesids)} - sesid: {sesid}\")\n",
    "            \n",
    "            sesid_filter = df['smb2.sesid'].apply(lambda x: sesid in normalize_sesid(x))\n",
    "            session_df = df[sesid_filter].copy()\n",
    "            \n",
    "            session_df['smb2.cmd'] = session_df['smb2.cmd'].apply(normalize_cmd)\n",
    "            session_df['smb2.filename'] = session_df['smb2.filename'].apply(lambda x: ','.join(x) if isinstance(x, list) else x)\n",
    "            session_df['smb2.sesid'] = session_df['smb2.sesid'].apply(normalize_sesid)\n",
    "            \n",
    "            if not session_df.empty:\n",
    "                sessions[sesid] = session_df\n",
    "                logger.debug(f\"Processed session {sesid} with {len(session_df)} frames\")\n",
    "                status_callback(f\"Processed session {sesid} with {len(session_df)} frames\")\n",
    "            else:\n",
    "                logger.warning(f\"No frames found for session {sesid}\")\n",
    "                status_callback(f\"Warning: No frames found for session {sesid}\")\n",
    "        \n",
    "        logger.info(f\"Extracted {len(sessions)} sessions\")\n",
    "        status_callback(f\"Extracted {len(sessions)} sessions\")\n",
    "        \n",
    "        output_dir = create_remote_directory(case_number, trace_name, force_reingest)\n",
    "        if output_dir is None:\n",
    "            logger.critical(f\"Failed to create output directory for {trace_name}\")\n",
    "            status_callback(f\"Error - Failed to create output directory for {trace_name}\")\n",
    "            return None\n",
    "        logger.info(f\"Output directory: {output_dir}\")\n",
    "        status_callback(f\"Output directory: {output_dir}\")\n",
    "        \n",
    "        parquet_path = os.path.join(output_dir, \"tshark_output_full.parquet\")\n",
    "        try:\n",
    "            if psutil.virtual_memory().available < 512 * 1024**2:\n",
    "                logger.warning(\"Low available memory before saving Parquet files\")\n",
    "                status_callback(\"Warning: Low available memory before saving Parquet files\")\n",
    "            save_to_parquet(df, parquet_path)\n",
    "            logger.info(f\"Saved full data to {parquet_path}\")\n",
    "            status_callback(f\"Saved full data to {parquet_path}\")\n",
    "            \n",
    "            for sesid, session_df in sessions.items():\n",
    "                session_parquet = os.path.join(output_dir, f\"smb2_session_{sesid}.parquet\")\n",
    "                save_to_parquet(session_df, session_parquet)\n",
    "                logger.info(f\"Saved session {sesid} to {session_parquet}\")\n",
    "                status_callback(f\"Saved session {sesid} to {session_parquet}\")\n",
    "            \n",
    "            save_session_metadata(case_number, trace_name, sessions, output_dir)\n",
    "            logger.info(\"Session metadata saved\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Error saving sessions or metadata: {str(e)}\")\n",
    "            status_callback(f\"Error - Failed to save sessions or metadata: {str(e)}\")\n",
    "            return None\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Ingestion completed in {elapsed_time:.2f}s\")\n",
    "        status_callback(f\"Ingestion completed in {elapsed_time:.2f}s\")\n",
    "        \n",
    "        result = {\"full_df\": df, \"sessions\": sessions}\n",
    "        logger.info(f\"Ingestion result: {list(result.get('sessions', {}).keys())}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error in run_ingestion: {e}\\n{traceback.format_exc()}\")\n",
    "        status_callback(f\"Error in run_ingestion: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "builtins.run_ingestion = run_ingestion\n",
    "logger.info(\"Ingestion and session extraction functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82128686-66cd-4545-ad7b-0db663537213",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 6: Session Selection and Utility Functions\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import subprocess\n",
    "import shlex\n",
    "import builtins\n",
    "\n",
    "# Import from builtins (set in Cells 1–5)\n",
    "from builtins import logger, check_ssh_connectivity, FIELDS, create_remote_directory, pcap_config\n",
    "\n",
    "def load_capture():\n",
    "    \"\"\"Load capture from config.pkl, falling back to pcap_config if unavailable.\"\"\"\n",
    "    logger.info(\"Loading capture path from config.pkl\")\n",
    "    config_file = \"/home/jovyan/work/smbreplay/config.pkl\"\n",
    "    if not os.path.exists(config_file):\n",
    "        logger.warning(f\"No {config_file} found. Checking pcap_config.\")\n",
    "        capture = pcap_config.get(\"capture_path\")\n",
    "        if capture and os.path.exists(capture):\n",
    "            logger.info(f\"Fallback to pcap_config capture path: {capture}\")\n",
    "            return capture\n",
    "        logger.warning(\"No valid capture_path in pcap_config. Configure capture in Cell 8 dashboard.\")\n",
    "        return None\n",
    "    try:\n",
    "        with open(config_file, 'rb') as f:\n",
    "            settings = pickle.load(f)\n",
    "            logger.debug(f\"Loaded config.pkl contents: {settings}\")\n",
    "            capture = settings.get(\"pcap_config\", {}).get(\"capture_path\")\n",
    "            if capture and os.path.exists(capture):\n",
    "                logger.info(f\"Loaded capture path: {capture}\")\n",
    "                return capture\n",
    "            logger.warning(f\"Capture path {capture or 'missing'} in config.pkl does not exist. Checking pcap_config.\")\n",
    "            capture = pcap_config.get(\"capture_path\")\n",
    "            if capture and os.path.exists(capture):\n",
    "                logger.info(f\"Fallback to pcap_config capture path: {capture}\")\n",
    "                return capture\n",
    "            logger.warning(\"No valid capture_path in config.pkl or pcap_config. Configure capture in Cell 8 dashboard.\")\n",
    "            return None\n",
    "    except (pickle.PickleError, IOError) as e:\n",
    "        logger.warning(f\"Error loading {config_file}: {e}. Checking pcap_config.\")\n",
    "        capture = pcap_config.get(\"capture_path\")\n",
    "        if capture and os.path.exists(capture):\n",
    "            logger.info(f\"Fallback to pcap_config capture path: {capture}\")\n",
    "            return capture\n",
    "        logger.warning(\"No valid capture_path in pcap_config. Configure capture in Cell 8.\")\n",
    "        return None\n",
    "\n",
    "def get_output_dir(capture):\n",
    "    \"\"\"Derive output directory from capture path, validate write access, return None if invalid.\"\"\"\n",
    "    logger.info(f\"Deriving output directory for capture: {capture}\")\n",
    "    if not capture:\n",
    "        logger.warning(\"No capture file provided. Configure in Cell 8 dashboard.\")\n",
    "        return None\n",
    "    try:\n",
    "        capture = os.path.normpath(capture)\n",
    "        parts = capture.split(os.sep)\n",
    "        if len(parts) < 3 or parts[1] != 'stingray':\n",
    "            raise ValueError(\"Invalid capture path format\")\n",
    "        case_number = parts[2]\n",
    "        trace_name = os.path.basename(capture).split('.')[0]  # Remove all extensions consistently\n",
    "        \n",
    "        # Use create_remote_directory from Cell 4\n",
    "        output_dir = create_remote_directory(case_number, trace_name)\n",
    "        \n",
    "        check_cmd = [\n",
    "            \"ssh\", \"-i\", \"/home/jovyan/.ssh/id_rsa\", \"-p\", \"22\", \"root@backend\",\n",
    "            f\"test -d {shlex.quote(output_dir)} && test -w {shlex.quote(output_dir)}\"\n",
    "        ]\n",
    "        logger.debug(f\"Executing directory check command: {' '.join(check_cmd)}\")\n",
    "        subprocess.run(check_cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "        logger.info(f\"Output directory set to: {output_dir} with write access confirmed\")\n",
    "        return output_dir\n",
    "    except (subprocess.CalledProcessError, ValueError, Exception) as e:\n",
    "        logger.warning(f\"Output directory does not exist or is not writable: {output_dir}. Run Cell 5 to ingest trace with 777 permissions. Error: {e}\")\n",
    "        return None\n",
    "\n",
    "def list_session_files(output_dir):\n",
    "    \"\"\"List session Parquet files in the output directory, normalizing to lowercase and removing duplicates.\"\"\"\n",
    "    logger.info(f\"Listing session files in {output_dir}\")\n",
    "    if not output_dir:\n",
    "        logger.warning(f\"Invalid or missing output directory: {output_dir}\")\n",
    "        return []\n",
    "    try:\n",
    "        ls_cmd = [\n",
    "            \"ssh\", \"-i\", \"/home/jovyan/.ssh/id_rsa\", \"-p\", \"22\", \"root@backend\",\n",
    "            f\"ls {shlex.quote(output_dir)}\"\n",
    "        ]\n",
    "        logger.debug(f\"Executing ls command: {' '.join(ls_cmd)}\")\n",
    "        result = subprocess.run(ls_cmd, capture_output=True, text=True, check=True)\n",
    "        session_files = [f for f in result.stdout.splitlines() if f.startswith('smb2_session_') and f.endswith('.parquet')]\n",
    "        # Normalize to lowercase and remove duplicates\n",
    "        normalized_files = list(dict.fromkeys(f.lower() for f in session_files))\n",
    "        if not normalized_files:\n",
    "            logger.warning(f\"No session files found in {output_dir}. Ensure ingestion completed successfully.\")\n",
    "        else:\n",
    "            logger.info(f\"Found {len(normalized_files)} unique session files in {output_dir}\")\n",
    "            logger.debug(f\"Session files: {', '.join(normalized_files[:5])}...\")\n",
    "        return normalized_files\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        logger.warning(f\"Error listing session files in {output_dir}: {e.stderr}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error listing session files in {output_dir}: {e}\")\n",
    "        return []\n",
    "\n",
    "def shorten_path(path: str, max_length: int = 50, min_filename_length: int = 20) -> str:\n",
    "    \"\"\"Dynamically shorten a file path for display, prioritizing the filename.\"\"\"\n",
    "    logger.debug(f\"Shortening path: {path}\")\n",
    "    if not isinstance(path, str) or not path.strip() or path == \"N/A\":\n",
    "        return \"N/A\"\n",
    "    if len(path) <= max_length:\n",
    "        return path\n",
    "    \n",
    "    components = path.replace('\\\\', '/').split('/')\n",
    "    filename = components[-1]\n",
    "    if len(filename) > min_filename_length:\n",
    "        filename = f\"{filename[:min_filename_length-3]}...\"\n",
    "    \n",
    "    remaining_length = max_length - len(filename) - 3\n",
    "    if remaining_length <= 0:\n",
    "        shortened = f\".../{filename}\"\n",
    "    else:\n",
    "        path_part = '/'.join(components[:-1])\n",
    "        if len(path_part) > remaining_length:\n",
    "            path_part = f\"{path_part[:remaining_length-3]}...\"\n",
    "        shortened = f\"{path_part}/{filename}\"\n",
    "    \n",
    "    logger.debug(f\"Shortened path to: {shortened}\")\n",
    "    return shortened\n",
    "\n",
    "def normalize_path(path: str) -> str:\n",
    "    \"\"\"Normalize a file path for comparison (lowercase, replace backslashes), preserving leading slash for absolute paths.\"\"\"\n",
    "    logger.debug(f\"Normalizing path: {path}\")\n",
    "    if not isinstance(path, str) or not path.strip() or path == \"N/A\":\n",
    "        return \"N/A\"\n",
    "    is_absolute = path.startswith('/')\n",
    "    normalized = path.lower().replace('\\\\', '/').strip('/')\n",
    "    if is_absolute:\n",
    "        normalized = f\"/{normalized}\"\n",
    "    logger.debug(f\"Normalized path to: {normalized}\")\n",
    "    return normalized\n",
    "\n",
    "def get_tree_name_mapping(df: pd.DataFrame) -> dict:\n",
    "    \"\"\"Map SMB2 tree IDs to tree names based on Tree Connect requests.\"\"\"\n",
    "    logger.info(\"Generating tree name mapping\")\n",
    "    tree_mapping = {}\n",
    "    required_columns = ['smb2.cmd', 'smb2.flags.response', 'smb2.tid', 'smb2.tree']\n",
    "    if not all(col in df.columns for col in required_columns):\n",
    "        missing = [col for col in required_columns if col not in df.columns]\n",
    "        logger.warning(f\"Missing columns for tree mapping: {missing}\")\n",
    "        return tree_mapping\n",
    "    \n",
    "    tree_connects = df[(df['smb2.cmd'] == '3') & (df['smb2.flags.response'] == 'False')]\n",
    "    logger.debug(f\"Found {len(tree_connects)} Tree Connect request frames\")\n",
    "    for _, row in tree_connects.iterrows():\n",
    "        tid = row.get('smb2.tid', '')\n",
    "        tree_name = row.get('smb2.tree', '')\n",
    "        if tid and tid.strip() and tid != '0' and isinstance(tid, str) and tree_name.strip():\n",
    "            tree_mapping[tid] = tree_name\n",
    "            logger.debug(f\"Mapped tree ID {tid} to {tree_name}\")\n",
    "    logger.info(f\"Tree mapping created with {len(tree_mapping)} records\")\n",
    "    return tree_mapping\n",
    "\n",
    "# Add core utilities to builtins\n",
    "builtins.load_capture = load_capture\n",
    "builtins.get_output_dir = get_output_dir\n",
    "builtins.shorten_path = shorten_path\n",
    "builtins.normalize_path = normalize_path\n",
    "builtins.get_tree_name_mapping = get_tree_name_mapping\n",
    "builtins.list_session_files = list_session_files\n",
    "\n",
    "# Initialize Cell 6\n",
    "logger.info(\"Session utility functions initialized. Capture loading and session selection deferred to dashboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938efca2-4bcd-4c7b-bec4-0c7c56f23dd5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 7: Session Loading and Filtering - Loads and filters SMB2 sessions\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import pyarrow.parquet as pq\n",
    "import uuid\n",
    "import builtins\n",
    "\n",
    "# Import from builtins (set in Cells 1–6)\n",
    "from builtins import logger, check_ssh_connectivity, FIELDS, FIELD_MAPPINGS, HEX_FIELDS, SMB2_OP_NAME_DESC, load_capture, get_output_dir, shorten_path, normalize_path, get_tree_name_mapping\n",
    "\n",
    "# Global variables\n",
    "operations = []\n",
    "session_frames = None\n",
    "_execution_id = str(uuid.uuid4())  # Unique ID for this run\n",
    "\n",
    "def load_and_summarize_session(capture, session_file):\n",
    "    \"\"\"Load a session file and return field options and file options.\"\"\"\n",
    "    global session_frames\n",
    "    logger.info(f\"Loading session file: {session_file}\")\n",
    "    \n",
    "    output_dir = get_output_dir(capture)\n",
    "    if not output_dir:\n",
    "        logger.warning(\"Invalid output directory. Configure capture in Cell 8 dashboard.\")\n",
    "        return None, [], [], []\n",
    "    \n",
    "    session_path = os.path.join(output_dir, session_file)\n",
    "    if not os.path.exists(session_path):\n",
    "        logger.warning(f\"Session file not found: {session_path}. Note: Path may be remote; consider SSH validation.\")\n",
    "        return None, [], [], []\n",
    "    \n",
    "    try:\n",
    "        session_frames = pq.read_table(session_path).to_pandas()\n",
    "        logger.info(f\"Loaded session {session_file} with {len(session_frames)} frames\")\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error loading session file: {e}. Try re-ingesting the PCAP in Cell 5.\")\n",
    "        return None, [], [], []\n",
    "    \n",
    "    if session_frames.empty:\n",
    "        logger.warning(f\"No frames found in session {session_file}\")\n",
    "        return None, [], [], []\n",
    "    \n",
    "    # Get field options\n",
    "    all_fields = sorted([col for col in session_frames.columns if col.startswith('smb2.')])\n",
    "    volatile_fields = [\"smb2.time\", \"smb2.frame.time\"]\n",
    "    field_options = [f for f in all_fields if f not in volatile_fields]\n",
    "    default_fields = ['smb2.nt_status', 'smb2.create.action']\n",
    "    selected_fields = [f for f in default_fields if f in field_options]\n",
    "    \n",
    "    # Get file options, excluding \"Entire Stream\"\n",
    "    unique_files = sorted(set(session_frames.get('smb2.filename', pd.Series([])).dropna()) - {'N/A', ''})\n",
    "    file_options = unique_files  # No \"Entire Stream\" option\n",
    "    \n",
    "    logger.debug(f\"Field options: {field_options[:5]}... (total: {len(field_options)})\")\n",
    "    logger.debug(f\"File options: {file_options[:5]}... (total: {len(file_options)})\")\n",
    "    \n",
    "    return session_frames, field_options, file_options, selected_fields\n",
    "\n",
    "def update_operations(capture, session_file, selected_file=None, selected_fields=None):\n",
    "    \"\"\"Prepare operations data based on selected file and fields.\"\"\"\n",
    "    global operations, session_frames\n",
    "    operations.clear()\n",
    "    logger.info(f\"Preparing operations for session: {session_file}, file: {selected_file}, fields: {selected_fields}\")\n",
    "    \n",
    "    if session_frames is None or session_frames.empty:\n",
    "        session_frames, _, _, _ = load_and_summarize_session(capture, session_file)\n",
    "        if session_frames is None or session_frames.empty:\n",
    "            logger.warning(f\"Failed to load session data for {session_file}\")\n",
    "            return []\n",
    "    \n",
    "    if selected_fields is None:\n",
    "        selected_fields = ['smb2.nt_status', 'smb2.create.action']\n",
    "    selected_fields = [f for f in selected_fields if f in session_frames.columns]  # Filter invalid fields\n",
    "    \n",
    "    # Apply FIELD_MAPPINGS for normalization and mapping\n",
    "    filtered_frames = session_frames.copy()\n",
    "    for field in FIELD_MAPPINGS:\n",
    "        if field in filtered_frames.columns:\n",
    "            mapping = FIELD_MAPPINGS[field][\"mapping\"]\n",
    "            normalize = FIELD_MAPPINGS[field][\"normalize\"]\n",
    "            logger.debug(f\"Normalizing field: {field}\")\n",
    "            filtered_frames[field] = filtered_frames[field].apply(normalize)\n",
    "            \n",
    "            # Special handling for fields that should only show when meaningful\n",
    "            if field in [\"smb2.create.action\", \"smb2.ioctl.function\"]:\n",
    "                # Only apply mapping to non-null, non-empty values\n",
    "                filtered_frames[f\"{field}_desc\"] = filtered_frames[field].apply(\n",
    "                    lambda x: mapping.get(str(x), \"\") if pd.notna(x) and str(x).strip() != \"\" and str(x) != \"None\" else \"\"\n",
    "                )\n",
    "            else:\n",
    "                filtered_frames[f\"{field}_desc\"] = filtered_frames[field].map(mapping).fillna(f\"Unknown ({filtered_frames[field]})\")\n",
    "    \n",
    "    # Filter frames based on selected file\n",
    "    if selected_file is None or not selected_file:  # No filter means all frames\n",
    "        logger.info(\"No file selected, processing all frames\")\n",
    "    else:\n",
    "        filtered_frames = filtered_frames[filtered_frames['smb2.filename'].apply(normalize_path) == normalize_path(selected_file)]\n",
    "    \n",
    "    logger.debug(f\"Filtered {len(filtered_frames)} frames\")\n",
    "    if filtered_frames.empty and selected_file is not None:\n",
    "        logger.warning(f\"No operations found for file: {selected_file}\")\n",
    "        return []\n",
    "    \n",
    "    # Helper function for normalization\n",
    "    def normalize_field(field_str):\n",
    "        if pd.isna(field_str) or not field_str or field_str.strip() == '':\n",
    "            return \"N/A\"\n",
    "        return field_str.split(',')[0].strip()\n",
    "\n",
    "    # Process frames\n",
    "    start_time = time.time()\n",
    "    total_frames = len(filtered_frames)\n",
    "    mandatory_fields = ['frame.number', 'smb2.cmd', 'smb2.filename', 'smb2.nt_status', 'smb2.flags.response']\n",
    "    for idx, row in filtered_frames.iterrows():\n",
    "        if idx % 10000 == 0 and idx > 0:\n",
    "            logger.debug(f\"Processing frame {idx}/{total_frames}\")\n",
    "        \n",
    "        # Normalize fields\n",
    "        filename = normalize_field(row.get('smb2.filename', 'N/A'))\n",
    "        tid = normalize_field(row.get('smb2.tid', 'N/A'))\n",
    "        path = shorten_path(filename) if filename != \"N/A\" else \"N/A\"\n",
    "        \n",
    "        # Use mapped and normalized fields\n",
    "        status_display = row.get('smb2.nt_status_desc', 'N/A')\n",
    "        cmd = row.get('smb2.cmd', '-1')\n",
    "        is_response = row.get('smb2.flags.response', 'False') == 'True'\n",
    "        op_name = row.get('smb2.cmd_desc', 'Unknown Request / Response')\n",
    "        \n",
    "        # Handle status description\n",
    "        status_desc = status_display.split('(')[0].strip() if status_display != 'N/A' else 'Not applicable'\n",
    "        \n",
    "        op = {\n",
    "            'Frame': row.get('frame.number', 'N/A'),\n",
    "            'Command': op_name,\n",
    "            'Path': path,\n",
    "            'Status': status_display,\n",
    "            'StatusDesc': status_desc,\n",
    "            'TID': tid,\n",
    "            'orig_idx': idx\n",
    "        }\n",
    "        \n",
    "        # Add selected fields\n",
    "        for field in selected_fields:\n",
    "            if field not in mandatory_fields:\n",
    "                # Special handling for fields that should only show when meaningful\n",
    "                if field in [\"smb2.create.action\", \"smb2.ioctl.function\"]:\n",
    "                    value = row.get(f\"{field}_desc\", row.get(field, \"\"))\n",
    "                    # Only include if there's actual meaningful data\n",
    "                    if value and str(value).strip() != \"\" and str(value) != \"N/A\" and not str(value).startswith(\"Unknown\"):\n",
    "                        op[field] = str(value)\n",
    "                    # Don't add the field if it's empty or meaningless\n",
    "                else:\n",
    "                    value = row.get(f\"{field}_desc\", row.get(field, 'N/A'))\n",
    "                    op[field] = str(value) if value is not None else 'N/A'\n",
    "        if 'smb2.ioctl.function' in filtered_frames.columns:\n",
    "            ioctl_value = row.get('smb2.ioctl.function_desc', row.get('smb2.ioctl.function', ''))\n",
    "            # Only include ioctl function if there's meaningful data\n",
    "            if ioctl_value and str(ioctl_value).strip() != \"\" and str(ioctl_value) != \"N/A\" and not str(ioctl_value).startswith(\"Unknown\"):\n",
    "                op['smb2.ioctl.function'] = str(ioctl_value)\n",
    "        \n",
    "        operations.append(op)\n",
    "        logger.debug(f\"Processed row {idx}: filename={filename}, status_code={row.get('smb2.nt_status', 'N/A')}, tid={tid}, path={path}\")\n",
    "    \n",
    "    logger.info(f\"Processed {len(operations)} operations in {time.time() - start_time:.2f}s\")\n",
    "    return operations\n",
    "\n",
    "# Add to builtins for Cell 8\n",
    "builtins.load_and_summarize_session = load_and_summarize_session\n",
    "builtins.update_operations = update_operations\n",
    "builtins.operations = operations\n",
    "builtins.session_frames = session_frames\n",
    "\n",
    "# Initialize Cell 7\n",
    "logger.info(\"Session loading and filtering utilities initialized. Session processing deferred to dashboard.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b18926-422a-44ad-8df8-fda15e2b48ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 8: Replay Mechanism Definitions for SMB2 Sessions\n",
    "import time\n",
    "from impacket.smbconnection import SMBConnection, SessionError\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import os\n",
    "import builtins\n",
    "\n",
    "# Import from builtins (set in Cells 1–7)\n",
    "from builtins import logger, replay_config, SMB2_OP_NAME_DESC\n",
    "\n",
    "def setup_pre_trace_state(conn, selected_operations, default_tree_id):\n",
    "    \"\"\"\n",
    "    Set up the file system state on the lab server before replaying the selected operations.\n",
    "\n",
    "    Args:\n",
    "        conn: SMBConnection object to the lab server.\n",
    "        selected_operations: List of selected operation dictionaries.\n",
    "        default_tree_id: Tree ID to use for creating directories and files.\n",
    "    \"\"\"\n",
    "    logger.info(\"Setting up pre-trace state for selected operations\")\n",
    "\n",
    "    # Collect all unique file paths and identify files created in the selected operations\n",
    "    all_paths = set()\n",
    "    created_files = set()\n",
    "    for op in selected_operations:\n",
    "        filename = op.get('smb2.filename', '')\n",
    "        if filename and filename not in ['.', '..']:\n",
    "            all_paths.add(filename)\n",
    "        # Identify files created in the selected operations\n",
    "        if (op.get('smb2.cmd') == '5' and \n",
    "            op.get('smb2.flags.response') == 'True' and \n",
    "            op.get('smb2.create.action') == 'FILE_CREATED'):\n",
    "            created_files.add(filename)\n",
    "\n",
    "    # Infer directories from paths\n",
    "    directories = set()\n",
    "    for path in all_paths:\n",
    "        parts = path.split('\\\\')\n",
    "        for i in range(1, len(parts)):\n",
    "            dir_path = '\\\\'.join(parts[:i])\n",
    "            if dir_path:\n",
    "                directories.add(dir_path)\n",
    "\n",
    "    # Create directories\n",
    "    for dir_path in sorted(directories, key=lambda x: x.count('\\\\')):\n",
    "        try:\n",
    "            conn.createDirectory(default_tree_id, dir_path)\n",
    "            logger.debug(f\"Created directory: {dir_path}\")\n",
    "        except SessionError as e:\n",
    "            if \"STATUS_OBJECT_NAME_COLLISION\" not in str(e):\n",
    "                logger.error(f\"Failed to create directory {dir_path}: {e}\")\n",
    "\n",
    "    # Create files that existed before the selected operations\n",
    "    for path in all_paths:\n",
    "        if path not in directories and path not in created_files:\n",
    "            try:\n",
    "                conn.createFile(default_tree_id, path, disposition=3)  # FILE_OPEN_IF\n",
    "                logger.debug(f\"Created pre-existing file: {path}\")\n",
    "            except SessionError as e:\n",
    "                logger.error(f\"Failed to create file {path}: {e}\")\n",
    "\n",
    "def replay_session(selected_operations, output_widget):\n",
    "    \"\"\"\n",
    "    Replay selected SMB2 operations using impacket.\n",
    "\n",
    "    Args:\n",
    "        selected_operations: List of selected operation dictionaries.\n",
    "        output_widget: IPython widget for displaying replay status.\n",
    "    \"\"\"\n",
    "    logger.info(\"Starting replay_session for selected operations\")\n",
    "    if not selected_operations:\n",
    "        logger.info(\"No operations selected for replay\")\n",
    "        with output_widget:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: red;'>No operations selected for replay.</p>\"))\n",
    "        return\n",
    "\n",
    "    # Extract server configuration from replay_config\n",
    "    server_ip = replay_config.get(\"server_ip\", \"10.216.29.241\")\n",
    "    domain = replay_config.get(\"domain\", \"nas-deep.local\")\n",
    "    username = replay_config.get(\"username\", \"jtownsen\")\n",
    "    password = replay_config.get(\"password\", \"\")\n",
    "    default_tree_name = replay_config.get(\"tree_name\", \"2pm\")\n",
    "    max_wait = replay_config.get(\"max_wait\", 5.0)\n",
    "\n",
    "    logger.debug(f\"Using replay_config: server_ip={server_ip}, domain={domain}, username={username}, tree_name={default_tree_name}, max_wait={max_wait}\")\n",
    "\n",
    "    try:\n",
    "        # Establish SMB connection\n",
    "        logger.debug(f\"Connecting to SMB server: {server_ip}\")\n",
    "        conn = SMBConnection(server_ip, server_ip, timeout=max_wait)\n",
    "        conn.login(username, password, domain)\n",
    "        logger.info(\"Successfully connected to SMB server\")\n",
    "\n",
    "        # Connect to the default tree\n",
    "        default_tree_id = conn.connectTree(default_tree_name)\n",
    "        logger.debug(f\"Connected to default tree {default_tree_name}, tree_id={default_tree_id}\")\n",
    "\n",
    "        # Setup pre-trace state for selected operations\n",
    "        setup_pre_trace_state(conn, selected_operations, default_tree_id)\n",
    "\n",
    "        # Initialize mappings for tree IDs and file IDs\n",
    "        tid_mapping = {}\n",
    "        fid_mapping = {}\n",
    "        state = {'last_new_tid': None, 'last_new_fid': None}\n",
    "\n",
    "        # Command handlers\n",
    "        def handle_tree_connect(conn, op, tid_mapping, state):\n",
    "            share_path = op.get('smb2.tree', '')\n",
    "            share_name = share_path.split('\\\\')[-1] if '\\\\' in share_path else share_path\n",
    "            state['last_new_tid'] = conn.connectTree(share_name)\n",
    "            logger.debug(f\"Tree Connect: {share_name}, new_tid={state['last_new_tid']}\")\n",
    "\n",
    "        def handle_create(conn, op, tid_mapping, fid_mapping, state):\n",
    "            original_tid = op.get('smb2.tid', '')\n",
    "            new_tid = tid_mapping.get(original_tid, default_tree_id)\n",
    "            filename = op.get('smb2.filename', '')\n",
    "            disposition = int(op.get('smb2.create_disposition', 1))  # Default FILE_OPEN\n",
    "            state['last_new_fid'] = conn.createFile(new_tid, filename, disposition=disposition)\n",
    "            logger.debug(f\"Create: {filename}, new_fid={state['last_new_fid']}\")\n",
    "\n",
    "        def handle_close(conn, op, tid_mapping, fid_mapping):\n",
    "            original_tid = op.get('smb2.tid', '')\n",
    "            original_fid = op.get('smb2.fid', '')\n",
    "            new_tid = tid_mapping.get(original_tid, default_tree_id)\n",
    "            new_fid = fid_mapping.get(original_fid)\n",
    "            if new_fid:\n",
    "                conn.closeFile(new_tid, new_fid)\n",
    "                logger.debug(f\"Close: fid={original_fid}\")\n",
    "\n",
    "        def handle_read(conn, op, tid_mapping, fid_mapping):\n",
    "            original_tid = op.get('smb2.tid', '')\n",
    "            original_fid = op.get('smb2.fid', '')\n",
    "            new_tid = tid_mapping.get(original_tid, default_tree_id)\n",
    "            new_fid = fid_mapping.get(original_fid)\n",
    "            if new_fid:\n",
    "                offset = int(op.get('smb2.read.offset', 0))\n",
    "                length = int(op.get('smb2.read.length', 1024))\n",
    "                conn.readFile(new_tid, new_fid, offset, length)\n",
    "                logger.debug(f\"Read: fid={original_fid}, offset={offset}, length={length}\")\n",
    "\n",
    "        def handle_write(conn, op, tid_mapping, fid_mapping):\n",
    "            original_tid = op.get('smb2.tid', '')\n",
    "            original_fid = op.get('smb2.fid', '')\n",
    "            new_tid = tid_mapping.get(original_tid, default_tree_id)\n",
    "            new_fid = fid_mapping.get(original_fid)\n",
    "            if new_fid:\n",
    "                offset = int(op.get('smb2.write.offset', 0))\n",
    "                data = bytes.fromhex(op.get('smb2.write_data', '')) if op.get('smb2.write_data') else b''\n",
    "                conn.writeFile(new_tid, new_fid, data, offset)\n",
    "                logger.debug(f\"Write: fid={original_fid}, offset={offset}, data_length={len(data)}\")\n",
    "\n",
    "        command_handlers = {\n",
    "            3: handle_tree_connect,  # Tree Connect\n",
    "            5: handle_create,       # Create\n",
    "            6: handle_close,        # Close\n",
    "            8: handle_read,         # Read\n",
    "            9: handle_write         # Write\n",
    "        }\n",
    "\n",
    "        # Process selected operations in order\n",
    "        for op in selected_operations:\n",
    "            is_response = op.get('smb2.flags.response') == 'True'\n",
    "            cmd = int(op.get('smb2.cmd', -1))\n",
    "\n",
    "            if not is_response:  # Request\n",
    "                if cmd in command_handlers:\n",
    "                    try:\n",
    "                        command_handlers[cmd](conn, op, tid_mapping, fid_mapping, state)\n",
    "                    except SessionError as e:\n",
    "                        logger.error(f\"Error executing command {cmd}: {e}\")\n",
    "                elif 0 <= cmd <= 18:\n",
    "                    logger.warning(f\"Command {cmd} ({SMB2_OP_NAME_DESC.get(cmd, ('Unknown', 'Unknown'))[0]}) not yet implemented\")\n",
    "                else:\n",
    "                    logger.warning(f\"Invalid command code: {cmd}\")\n",
    "            else:  # Response\n",
    "                if cmd == 3:  # Tree Connect response\n",
    "                    original_tid = op.get('smb2.tid', '')\n",
    "                    if state['last_new_tid'] is not None:\n",
    "                        tid_mapping[original_tid] = state['last_new_tid']\n",
    "                        logger.debug(f\"Mapped tid {original_tid} to {state['last_new_tid']}\")\n",
    "                        state['last_new_tid'] = None\n",
    "                elif cmd == 5:  # Create response\n",
    "                    original_fid = op.get('smb2.fid', '')\n",
    "                    if state['last_new_fid'] is not None:\n",
    "                        fid_mapping[original_fid] = state['last_new_fid']\n",
    "                        logger.debug(f\"Mapped fid {original_fid} to {state['last_new_fid']}\")\n",
    "                        state['last_new_fid'] = None\n",
    "\n",
    "        # Clean up\n",
    "        logger.debug(\"Disconnecting from SMB server\")\n",
    "        conn.close()\n",
    "        logger.info(\"Disconnected from SMB server\")\n",
    "\n",
    "        with output_widget:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: green;'>Replay completed.</p>\"))\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error during replay: {e}\")\n",
    "        with output_widget:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>Replay failed: {e}</p>\"))\n",
    "\n",
    "# Export to builtins for Cell 10\n",
    "builtins.replay_session = replay_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81215ae1-3e7b-4564-a785-fa3140e49768",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Dashboard Setup for Session Visualization and Configuration\n",
    "import pandas as pd\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import os\n",
    "import json\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import builtins\n",
    "import logging\n",
    "\n",
    "# Import from builtins (set in Cells 1–8)\n",
    "from builtins import logger, pcap_config, replay_config, capture\n",
    "from builtins import load_capture, get_output_dir, shorten_path, normalize_path, get_tree_name_mapping, list_session_files\n",
    "from builtins import load_and_summarize_session, update_operations, operations, session_frames, run_ingestion\n",
    "from builtins import FIELD_MAPPINGS, HEX_FIELDS\n",
    "\n",
    "# Custom JupyterOutputHandler for log_output\n",
    "class JupyterOutputHandler(logging.Handler):\n",
    "    def __init__(self, output_widget):\n",
    "        super().__init__()\n",
    "        self.output_widget = output_widget\n",
    "        self.setFormatter(logging.Formatter(\"%(asctime)s - %(levelname)s - [%(asctime)s] %(message)s\", datefmt=\"%a %b %d %H:%M:%S %Y\"))\n",
    "\n",
    "    def emit(self, record):\n",
    "        msg = self.format(record)\n",
    "        with self.output_widget:\n",
    "            display(HTML(f\"<pre>{msg}</pre>\"))\n",
    "\n",
    "# Initialize output widgets\n",
    "builtins.log_output = widgets.Output()\n",
    "builtins.output_cell = widgets.Output()\n",
    "builtins.progress_output = widgets.Output()\n",
    "builtins.progress_label = widgets.HTML(value=\"Ready\")\n",
    "\n",
    "# Add JupyterOutputHandler to logger\n",
    "jupyter_handler = JupyterOutputHandler(builtins.log_output)\n",
    "logger.addHandler(jupyter_handler)\n",
    "\n",
    "# Log confirmation\n",
    "logger.info(\"Initialized output widgets: log_output, output_cell, progress_output, progress_label\")\n",
    "\n",
    "# Define dashboard widgets\n",
    "builtins.debug_slider = widgets.IntSlider(\n",
    "    value=logger.getEffectiveLevel() // 10,  # Map logging level to slider (e.g., 20=INFO=1)\n",
    "    min=0,\n",
    "    max=3,\n",
    "    description=\"Debug Level:\",\n",
    "    layout={'width': '700px'}\n",
    ")\n",
    "\n",
    "builtins.case_number_input = widgets.Text(\n",
    "    value=\"\",\n",
    "    placeholder=\"2010373016\",\n",
    "    description=\"Case Number:\",\n",
    "    disabled=False,\n",
    "    layout={'width': '700px'}\n",
    ")\n",
    "\n",
    "builtins.capture_dropdown = widgets.Dropdown(\n",
    "    options=[\"Select a capture\"],\n",
    "    description=\"Capture:\",\n",
    "    disabled=True,\n",
    "    layout={'width': '700px'}\n",
    ")\n",
    "\n",
    "builtins.session_dropdown = widgets.Dropdown(\n",
    "    options=[\"Select a session\"],\n",
    "    description=\"Session:\",\n",
    "    disabled=True,\n",
    "    layout={'width': '700px'}\n",
    ")\n",
    "\n",
    "builtins.ingest_button = widgets.Button(\n",
    "    description=\"Ingest Trace\",\n",
    "    button_style=\"warning\",\n",
    "    tooltip=\"Ingest the selected capture into Parquet\",\n",
    "    disabled=True,\n",
    "    layout={'visibility': 'visible'}\n",
    ")\n",
    "\n",
    "builtins.reingest_button = widgets.Button(\n",
    "    description=\"Re-ingest\",\n",
    "    button_style=\"warning\",\n",
    "    tooltip=\"Re-ingest the selected capture, overwriting existing data\",\n",
    "    disabled=True,\n",
    "    layout={'visibility': 'visible'}\n",
    ")\n",
    "\n",
    "builtins.replay_button = widgets.Button(\n",
    "    description=\"Replay Session\",\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Replay the selected session on the lab server\",\n",
    "    disabled=True,\n",
    "    layout={'visibility': 'visible'}\n",
    ")\n",
    "\n",
    "builtins.button_box = widgets.HBox([builtins.ingest_button, builtins.reingest_button, builtins.replay_button])\n",
    "\n",
    "builtins.file_combobox = widgets.Combobox(\n",
    "    options=[\"Entire Stream\"],\n",
    "    placeholder=\"Select a file (or use Entire Stream for all)\",\n",
    "    description=\"File Filter:\",\n",
    "    ensure_option=True,\n",
    "    disabled=True,\n",
    "    layout={'width': '900px'},\n",
    "    style={'description_width': 'initial'},\n",
    "    rows=10\n",
    ")\n",
    "\n",
    "# Apply custom CSS for combobox width\n",
    "display(HTML(\"\"\"\n",
    "<style>\n",
    "    .widget-combobox > select {\n",
    "        width: 900px !important;\n",
    "        max-width: 900px !important;\n",
    "        min-width: 900px !important;\n",
    "    }\n",
    "</style>\n",
    "\"\"\"))\n",
    "\n",
    "builtins.check_fields_select = widgets.SelectMultiple(\n",
    "    options=[],\n",
    "    description=\"Fields:\",\n",
    "    disabled=True,\n",
    "    layout={'width': '900px'},\n",
    "    tooltip=\"Select fields to display as columns in the table for Entire Stream\"\n",
    ")\n",
    "\n",
    "builtins.server_ip_input = widgets.Text(\n",
    "    value=replay_config.get(\"server_ip\", \"10.216.29.241\"),\n",
    "    placeholder=\"Enter server IP\",\n",
    "    description=\"Server IP:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "builtins.domain_input = widgets.Text(\n",
    "    value=replay_config.get(\"domain\", \"nas-deep.local\"),\n",
    "    placeholder=\"Enter domain\",\n",
    "    description=\"Domain:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "builtins.username_input = widgets.Text(\n",
    "    value=replay_config.get(\"username\", \"jtownsen\"),\n",
    "    placeholder=\"Enter username\",\n",
    "    description=\"Username:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "builtins.password_input = widgets.Password(\n",
    "    value=replay_config.get(\"password\", \"!Elephant1\"),\n",
    "    placeholder=\"Enter password\",\n",
    "    description=\"Password:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "builtins.tree_name_input = widgets.Text(\n",
    "    value=replay_config.get(\"tree_name\", \"2pm\"),\n",
    "    placeholder=\"Enter tree name\",\n",
    "    description=\"Tree Name:\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "builtins.max_time_input = widgets.FloatText(\n",
    "    value=replay_config.get(\"max_wait\", 5.0),\n",
    "    description=\"Max Wait (s):\",\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "builtins.save_button = widgets.Button(\n",
    "    description=\"Save Config\",\n",
    "    button_style=\"info\",\n",
    "    tooltip=\"Save replay server configuration\",\n",
    "    disabled=True\n",
    ")\n",
    "\n",
    "builtins.dashboard = widgets.VBox([\n",
    "    builtins.debug_slider, builtins.case_number_input, builtins.capture_dropdown, builtins.session_dropdown,\n",
    "    builtins.button_box, builtins.file_combobox, builtins.check_fields_select, builtins.server_ip_input,\n",
    "    builtins.domain_input, builtins.username_input, builtins.password_input, builtins.tree_name_input,\n",
    "    builtins.max_time_input, builtins.save_button, builtins.log_output, builtins.progress_output, builtins.output_cell\n",
    "])\n",
    "\n",
    "# Initialize case number if capture_path is valid\n",
    "capture = load_capture()\n",
    "if capture and os.path.exists(capture):\n",
    "    case_num = capture.split(os.sep)[2] if len(capture.split(os.sep)) > 2 else \"\"\n",
    "    if case_num:\n",
    "        builtins.case_number_input.value = case_num\n",
    "        logger.info(f\"Initialized case number: {case_num} from capture: {capture}\")\n",
    "else:\n",
    "    logger.info(\"No valid initial capture path in config. Enter a case number and select a capture.\")\n",
    "    with builtins.output_cell:\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(\"<p>Enter a case number and select a capture to view data.</p>\"))\n",
    "\n",
    "# Export to builtins\n",
    "builtins.debug_slider = debug_slider\n",
    "builtins.pcap_config = pcap_config\n",
    "builtins.replay_config = replay_config\n",
    "logger.info(\"Dashboard widgets initialized and exported to builtins.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd523b8-0480-43da-b583-56bc7954ea68",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 10: Event Handlers and Rendering for Session Visualization Dashboard\n",
    "import time\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import os\n",
    "import pickle\n",
    "import builtins\n",
    "import mimetypes\n",
    "import struct\n",
    "\n",
    "# Import from builtins (set in Cells 1–9) with fallback\n",
    "try:\n",
    "    from builtins import (\n",
    "        logger, pcap_config, replay_config, replay_session,\n",
    "        load_capture, get_output_dir, list_session_files, load_and_summarize_session,\n",
    "        update_operations, operations, session_frames, run_ingestion,\n",
    "        log_output, output_cell, progress_output, progress_label,\n",
    "        case_number_input, capture_dropdown, session_dropdown,\n",
    "        ingest_button, reingest_button, replay_button, button_box, file_combobox,\n",
    "        check_fields_select, server_ip_input, domain_input, username_input,\n",
    "        password_input, tree_name_input, max_time_input, save_button, dashboard,\n",
    "        FIELD_MAPPINGS, HEX_FIELDS, CREATE_ACTION_DESC\n",
    "    )\n",
    "except ImportError as e:\n",
    "    logger.critical(f\"Failed to import from builtins: {e}. Check Cell 9 execution.\")\n",
    "    raise\n",
    "\n",
    "config_file = \"/home/jovyan/work/smbreplay/config.pkl\"\n",
    "\n",
    "def status_callback(message):\n",
    "    \"\"\"Log status messages to logger and display in log_output.\"\"\"\n",
    "    logger.info(f\"Status: {message}\")\n",
    "    with log_output:\n",
    "        display(HTML(f\"<pre>{message}</pre>\"))\n",
    "\n",
    "builtins.status_callback = status_callback\n",
    "\n",
    "def is_capture_file(file_path):\n",
    "    \"\"\"\n",
    "    Detect if a file is a network capture file using file signatures and heuristics.\n",
    "    Handles files with non-standard names like 'pcap1', 'pcap2', 'trace_data', etc.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the file is identified as a capture file, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get filename and extension for analysis\n",
    "        filename = os.path.basename(file_path).lower()\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        # Comprehensive list of capture file extensions\n",
    "        common_capture_extensions = [\n",
    "            '.pcap', '.pcapng', '.cap', '.dmp', '.5vw', '.acp', '.apc', '.atc', \n",
    "            '.bfr', '.enc', '.erf', '.fdc', '.pkt', '.trc', '.trace', '.wpz', \n",
    "            '.snoop', '.rf5', '.ntar', '.wpc', '.logpkt', '.out', '.raw'\n",
    "        ]\n",
    "        \n",
    "        # Check for capture-related keywords in filename (even without proper extensions)\n",
    "        capture_filename_patterns = [\n",
    "            'pcap', 'capture', 'trace', 'packet', 'sniff', 'dump', 'traffic',\n",
    "            'network', 'wireshark', 'tcpdump', 'tshark', 'netmon', 'sniffer',\n",
    "            'ethereal', 'cap', 'pkt', 'trc'\n",
    "        ]\n",
    "        \n",
    "        # Read file header for signature detection\n",
    "        with open(file_path, 'rb') as f:\n",
    "            header = f.read(32)  # Read more bytes for better detection\n",
    "            \n",
    "        if len(header) < 4:\n",
    "            logger.debug(f\"File {file_path} too small to be a capture file\")\n",
    "            return False\n",
    "            \n",
    "        # === PRIMARY: File signature detection ===\n",
    "        \n",
    "        # PCAP file signatures (most common)\n",
    "        # Classic pcap: 0xA1B2C3D4 (big endian) or 0xD4C3B2A1 (little endian)\n",
    "        # Modified pcap: 0xA1B23C4D (big endian) or 0x4D3CB2A1 (little endian)\n",
    "        # Nanosecond pcap: 0xA1B23C4D or 0x4D3CB2A1\n",
    "        pcap_signatures = [\n",
    "            b'\\xa1\\xb2\\xc3\\xd4',  # microsecond big endian\n",
    "            b'\\xd4\\xc3\\xb2\\xa1',  # microsecond little endian\n",
    "            b'\\xa1\\xb2\\x3c\\x4d',  # nanosecond big endian\n",
    "            b'\\x4d\\x3c\\xb2\\xa1'   # nanosecond little endian\n",
    "        ]\n",
    "        \n",
    "        # PCAP-NG file signature\n",
    "        # Section Header Block: 0x0A0D0D0A followed by block length\n",
    "        pcapng_signature = b'\\x0a\\x0d\\x0d\\x0a'\n",
    "        \n",
    "        # Check PCAP signatures\n",
    "        if header[:4] in pcap_signatures:\n",
    "            logger.debug(f\"File {file_path} detected as PCAP file by signature\")\n",
    "            return True\n",
    "            \n",
    "        # Check PCAP-NG signature\n",
    "        if header[:4] == pcapng_signature:\n",
    "            logger.debug(f\"File {file_path} detected as PCAP-NG file by signature\")\n",
    "            return True\n",
    "            \n",
    "        # Snoop capture file (Sun/Solaris)\n",
    "        # Signature: \"snoop\\0\\0\\0\" followed by version\n",
    "        if header[:8] == b'snoop\\x00\\x00\\x00':\n",
    "            logger.debug(f\"File {file_path} detected as Snoop capture file\")\n",
    "            return True\n",
    "            \n",
    "        # Visual Networks capture (.5vw files)\n",
    "        # Signature: 0x0556 at start\n",
    "        if header[:2] == b'\\x05\\x56':\n",
    "            logger.debug(f\"File {file_path} detected as Visual Networks capture file\")\n",
    "            return True\n",
    "            \n",
    "        # Microsoft Network Monitor capture (.cap files)\n",
    "        # Signature: \"TRSNIFF data    \" followed by 0x1a\n",
    "        if header[:11] == b'TRSNIFF data':\n",
    "            logger.debug(f\"File {file_path} detected as Network Monitor capture file\")\n",
    "            return True\n",
    "            \n",
    "        # NetXray/Sniffer capture files\n",
    "        # Signature: \"XCP\\0\" at start\n",
    "        if header[:4] == b'XCP\\x00':\n",
    "            logger.debug(f\"File {file_path} detected as NetXray capture file\")\n",
    "            return True\n",
    "            \n",
    "        # Novell LANalyzer capture files\n",
    "        # Signature: 0x01100000 or similar patterns\n",
    "        if header[:4] == b'\\x01\\x10\\x00\\x00':\n",
    "            logger.debug(f\"File {file_path} detected as LANalyzer capture file\")\n",
    "            return True\n",
    "            \n",
    "        # EtherPeek/AiroPeek capture files\n",
    "        # Signature: 0x7F followed by \"EtherPeek\" or similar\n",
    "        if header[0:1] == b'\\x7f' and b'Peek' in header[:16]:\n",
    "            logger.debug(f\"File {file_path} detected as EtherPeek capture file\")\n",
    "            return True\n",
    "            \n",
    "        # Tektronix K12xx capture files\n",
    "        # Look for specific patterns in K12 files\n",
    "        if b'K12' in header[:16] or header[:3] == b'\\x00\\x00\\x80':\n",
    "            logger.debug(f\"File {file_path} detected as Tektronix K12 capture file\")\n",
    "            return True\n",
    "            \n",
    "        # === SECONDARY: Extension-based detection ===\n",
    "        if file_extension in common_capture_extensions:\n",
    "            logger.debug(f\"File {file_path} detected by known extension: {file_extension}\")\n",
    "            return True\n",
    "            \n",
    "        # === EXCLUSION CHECK: Known non-capture binary formats ===\n",
    "        # Check for common binary file signatures that are NOT capture files\n",
    "        \n",
    "        # Parquet files (Apache Parquet)\n",
    "        if header[:4] == b'PAR1' or header[-4:] == b'PAR1':\n",
    "            logger.debug(f\"File {file_path} detected as Parquet file - excluding\")\n",
    "            return False\n",
    "            \n",
    "        # SQLite database files\n",
    "        if header[:16] == b'SQLite format 3\\x00':\n",
    "            logger.debug(f\"File {file_path} detected as SQLite database - excluding\")\n",
    "            return False\n",
    "            \n",
    "        # Common image formats\n",
    "        image_signatures = [\n",
    "            (b'\\xff\\xd8\\xff', 'JPEG'),\n",
    "            (b'\\x89PNG\\r\\n\\x1a\\n', 'PNG'),\n",
    "            (b'GIF8', 'GIF'),\n",
    "            (b'BM', 'BMP'),\n",
    "            (b'RIFF', 'RIFF/WEBP'),\n",
    "            (b'\\x00\\x00\\x01\\x00', 'ICO')\n",
    "        ]\n",
    "        \n",
    "        for sig, format_name in image_signatures:\n",
    "            if header.startswith(sig):\n",
    "                logger.debug(f\"File {file_path} detected as {format_name} image - excluding\")\n",
    "                return False\n",
    "                \n",
    "        # Archive formats (with special handling for gzipped capture files)\n",
    "        archive_signatures = [\n",
    "            (b'PK\\x03\\x04', 'ZIP'),\n",
    "            (b'PK\\x05\\x06', 'ZIP'),\n",
    "            (b'PK\\x07\\x08', 'ZIP'),\n",
    "            (b'BZh', 'BZIP2'),\n",
    "            (b'\\x7fELF', 'ELF'),\n",
    "            (b'\\xfd7zXZ\\x00', 'XZ'),\n",
    "            (b'Rar!', 'RAR')\n",
    "        ]\n",
    "        \n",
    "        for sig, format_name in archive_signatures:\n",
    "            if header.startswith(sig):\n",
    "                logger.debug(f\"File {file_path} detected as {format_name} archive - excluding\")\n",
    "                return False\n",
    "                \n",
    "        # Special handling for GZIP files - check if they might be gzipped capture files\n",
    "        if header.startswith(b'\\x1f\\x8b\\x08'):  # GZIP signature\n",
    "            # Check filename for capture-related patterns\n",
    "            gzip_capture_patterns = ['.pcap.gz', '.pcapng.gz', '.cap.gz', '.trace.gz', '.dmp.gz']\n",
    "            if any(filename.endswith(pattern) for pattern in gzip_capture_patterns):\n",
    "                logger.debug(f\"File {file_path} detected as gzipped capture file - including\")\n",
    "                return True\n",
    "            # Check if filename without .gz has capture patterns\n",
    "            if filename.endswith('.gz'):\n",
    "                base_filename = filename[:-3]  # Remove .gz\n",
    "                for pattern in capture_filename_patterns:\n",
    "                    if pattern in base_filename:\n",
    "                        logger.debug(f\"File {file_path} detected as gzipped capture file by pattern - including\")\n",
    "                        return True\n",
    "            # Otherwise exclude as regular gzip\n",
    "            logger.debug(f\"File {file_path} detected as regular GZIP file - excluding\")\n",
    "            return False\n",
    "                \n",
    "        # Office documents and other formats\n",
    "        office_signatures = [\n",
    "            (b'\\xd0\\xcf\\x11\\xe0\\xa1\\xb1\\x1a\\xe1', 'MS Office'),\n",
    "            (b'%PDF', 'PDF'),\n",
    "            (b'\\x7b\\x5c\\x72\\x74\\x66', 'RTF'),\n",
    "            (b'ftyp', 'MP4/MOV', 4),  # Check at offset 4\n",
    "        ]\n",
    "        \n",
    "        for sig_data in office_signatures:\n",
    "            if len(sig_data) == 3:\n",
    "                sig, format_name, offset = sig_data\n",
    "                if len(header) > offset + len(sig) and header[offset:offset+len(sig)] == sig:\n",
    "                    logger.debug(f\"File {file_path} detected as {format_name} document - excluding\")\n",
    "                    return False\n",
    "            else:\n",
    "                sig, format_name = sig_data\n",
    "                if header.startswith(sig):\n",
    "                    logger.debug(f\"File {file_path} detected as {format_name} document - excluding\")\n",
    "                    return False\n",
    "                    \n",
    "        # Database files\n",
    "        db_signatures = [\n",
    "            (b'\\x00\\x01\\x00\\x00Standard Jet DB', 'MS Access'),\n",
    "            (b'Microsoft C/C++ MSF 7.00', 'MS Debug'),\n",
    "        ]\n",
    "        \n",
    "        for sig, format_name in db_signatures:\n",
    "            if header.startswith(sig):\n",
    "                logger.debug(f\"File {file_path} detected as {format_name} database - excluding\")\n",
    "                return False\n",
    "\n",
    "        # === TERTIARY: Filename pattern matching ===\n",
    "        # Check for capture-related keywords in filename (handles pcap1, pcap2, trace_data, etc.)\n",
    "        for pattern in capture_filename_patterns:\n",
    "            if pattern in filename:\n",
    "                logger.debug(f\"File {file_path} detected by filename pattern: '{pattern}' in '{filename}'\")\n",
    "                # Do additional validation to reduce false positives\n",
    "                if len(header) >= 8:\n",
    "                    # Check if it's likely binary data (not a text file)\n",
    "                    try:\n",
    "                        # If we can decode as text and it's mostly printable, probably not a capture\n",
    "                        text_content = header.decode('utf-8', errors='strict')\n",
    "                        if all(c.isprintable() or c.isspace() for c in text_content):\n",
    "                            logger.debug(f\"File {file_path} appears to be text despite pattern match - skipping\")\n",
    "                            continue\n",
    "                    except UnicodeDecodeError:\n",
    "                        # Can't decode as text, likely binary - good sign for capture file\n",
    "                        pass\n",
    "                        \n",
    "                    # Additional check: exclude files with known non-capture extensions\n",
    "                    # But allow gzipped capture files\n",
    "                    non_capture_extensions = [\n",
    "                        '.parquet', '.db', '.sqlite', '.sqlite3', '.jpg', '.jpeg', '.png', '.gif', \n",
    "                        '.bmp', '.zip', '.tar', '.bz2', '.xz', '.rar', '.7z', '.pdf', \n",
    "                        '.doc', '.docx', '.xls', '.xlsx', '.ppt', '.pptx', '.mp4', '.avi', \n",
    "                        '.mov', '.mkv', '.mp3', '.wav', '.exe', '.dll', '.so', '.dylib'\n",
    "                    ]\n",
    "                    \n",
    "                    # Special handling for .gz files - allow if they appear to be gzipped captures\n",
    "                    if file_extension == '.gz':\n",
    "                        gzip_capture_patterns = ['.pcap.gz', '.pcapng.gz', '.cap.gz', '.trace.gz', '.dmp.gz']\n",
    "                        if any(filename.endswith(pattern) for pattern in gzip_capture_patterns):\n",
    "                            logger.debug(f\"File {file_path} is gzipped capture file - allowing\")\n",
    "                            return True\n",
    "                        # Check if base filename (without .gz) has capture patterns\n",
    "                        base_filename = filename[:-3]  # Remove .gz\n",
    "                        for capture_pattern in capture_filename_patterns:\n",
    "                            if capture_pattern in base_filename:\n",
    "                                logger.debug(f\"File {file_path} is gzipped capture file by pattern - allowing\")\n",
    "                                return True\n",
    "                        # Otherwise treat as regular gzip (exclude)\n",
    "                        logger.debug(f\"File {file_path} is regular gzip file - excluding\")\n",
    "                        continue\n",
    "                    \n",
    "                    if file_extension in non_capture_extensions:\n",
    "                        logger.debug(f\"File {file_path} has non-capture extension {file_extension} despite pattern match - excluding\")\n",
    "                        continue\n",
    "                        \n",
    "                    return True\n",
    "                        \n",
    "        # === QUATERNARY: Heuristic checks for unknown formats ===\n",
    "        # Look for patterns that suggest network capture data\n",
    "        if len(header) >= 16:\n",
    "            # Check for repeating structures that might indicate packet headers\n",
    "            # Look for common ethernet frame patterns\n",
    "            if header[12:14] in [b'\\x08\\x00', b'\\x08\\x06', b'\\x86\\xdd']:  # IP, ARP, IPv6\n",
    "                logger.debug(f\"File {file_path} detected by ethernet frame pattern\")\n",
    "                return True\n",
    "                \n",
    "            # Check for common IP header patterns\n",
    "            if header[0:1] in [b'\\x45', b'\\x46'] and len(header) > 20:  # IPv4 headers\n",
    "                logger.debug(f\"File {file_path} detected by IP header pattern\")\n",
    "                return True\n",
    "        \n",
    "        # === FINAL: MIME type check ===\n",
    "        mime_type, _ = mimetypes.guess_type(file_path)\n",
    "        if mime_type and any(capture_type in mime_type.lower() for capture_type in ['pcap', 'capture', 'tcpdump']):\n",
    "            logger.debug(f\"File {file_path} detected as capture file by MIME type: {mime_type}\")\n",
    "            return True\n",
    "            \n",
    "        logger.debug(f\"File {file_path} not detected as capture file. Extension: {file_extension}, Filename: {filename}, Header: {header[:8].hex()}\")\n",
    "        return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Error detecting file type for {file_path}: {e}\")\n",
    "        # Enhanced fallback - check both extension and filename patterns\n",
    "        filename = os.path.basename(file_path).lower()\n",
    "        file_extension = os.path.splitext(file_path)[1].lower()\n",
    "        \n",
    "        # Check extension\n",
    "        if file_extension in ['.pcap', '.pcapng', '.cap', '.dmp', '.trc', '.trace']:\n",
    "            logger.debug(f\"Fallback extension check for {file_path}: True\")\n",
    "            return True\n",
    "            \n",
    "        # Check filename patterns\n",
    "        fallback_patterns = ['pcap', 'capture', 'trace', 'dump', 'packet', 'sniff']\n",
    "        for pattern in fallback_patterns:\n",
    "            if pattern in filename:\n",
    "                logger.debug(f\"Fallback filename pattern check for {file_path}: True (pattern: {pattern})\")\n",
    "                return True\n",
    "                \n",
    "        logger.debug(f\"Fallback check for {file_path}: False\")\n",
    "        return False\n",
    "\n",
    "def update_progress(message):\n",
    "    \"\"\"Update progress label and log to logger.\"\"\"\n",
    "    logger.info(f\"Progress: {message}\")\n",
    "    progress_label.value = message\n",
    "    with progress_output:\n",
    "        clear_output(wait=True)\n",
    "        display(progress_label)\n",
    "\n",
    "def update_button_states():\n",
    "    \"\"\"Update button states based on session existence.\"\"\"\n",
    "    logger.debug(\"Updating button states\")\n",
    "    capture = load_capture()\n",
    "    output_dir = get_output_dir(capture) if capture else None\n",
    "    session_files = list_session_files(output_dir) if output_dir else []\n",
    "    ingest_button.disabled = bool(session_files)\n",
    "    reingest_button.disabled = not bool(session_files)\n",
    "    replay_button.disabled = not bool(session_files)\n",
    "    logger.debug(f\"Button states updated: ingest={ingest_button.disabled}, reingest={reingest_button.disabled}, replay={replay_button.disabled}\")\n",
    "\n",
    "# Export update_button_states to builtins\n",
    "builtins.update_button_states = update_button_states\n",
    "\n",
    "def render_page():\n",
    "    \"\"\"Render the operations table with smb2.cmd, smb2.create.action, and smb2.nt_status, leaving NULL as blank.\"\"\"\n",
    "    with output_cell:\n",
    "        logger.info(\"Starting render_page\")\n",
    "        if not operations:\n",
    "            clear_output(wait=True)\n",
    "        \n",
    "        update_progress(\"Rendering operations...\")\n",
    "        \n",
    "        try:\n",
    "            logger.info(f\"Operations list length: {len(operations)}\")\n",
    "            if not operations:\n",
    "                logger.info(\"No operations data to render. Check session selection.\")\n",
    "                display(HTML(\"<p style='color: red;'>No operations to display. Ensure a session is selected.</p>\"))\n",
    "                return\n",
    "            \n",
    "            ops_df = pd.DataFrame(operations)\n",
    "            if ops_df.empty:\n",
    "                logger.info(\"Operations DataFrame is empty. Check data source.\")\n",
    "                display(HTML(\"<p style='color: red;'>No data in operations DataFrame. Check logs for details.</p>\"))\n",
    "                return\n",
    "            \n",
    "            logger.debug(f\"Columns in ops_df: {ops_df.columns.tolist()}\")\n",
    "            if \"smb2.nt_status\" in ops_df.columns:\n",
    "                logger.debug(f\"Sample smb2.nt_status values: {ops_df['smb2.nt_status'].head().tolist()}\")\n",
    "            else:\n",
    "                logger.warning(\"smb2.nt_status not found in operations DataFrame.\")\n",
    "            \n",
    "            mandatory_columns = ['Frame', 'Command', 'Path', 'smb2.nt_status']\n",
    "            available_columns = [col for col in mandatory_columns if col in ops_df.columns]\n",
    "            if not all(col in ops_df.columns for col in mandatory_columns):\n",
    "                logger.debug(f\"Missing columns: {set(mandatory_columns) - set(ops_df.columns)}. Using available columns: {available_columns}\")\n",
    "                if not available_columns:\n",
    "                    raise ValueError(\"No mandatory columns found in DataFrame.\")\n",
    "            \n",
    "            logger.debug(\"First 2 lines of operations:\")\n",
    "            for i, op in enumerate(operations[:2]):\n",
    "                logger.debug(f\"Line {i + 1}: {op}\")\n",
    "            \n",
    "            if 'smb2.nt_status' in ops_df.columns:\n",
    "                ops_df['smb2.nt_status'] = ops_df.get('Status', ops_df['smb2.nt_status']).apply(\n",
    "                    lambda x: FIELD_MAPPINGS.get(\"smb2.nt_status\", {}).get(\"mapping\", {}).get(str(x), \"\") if pd.notna(x) and x != 'N/A' else \"\"\n",
    "                )\n",
    "            \n",
    "            optional_columns = [col for col in ops_df.columns if col in check_fields_select.value and col not in mandatory_columns]\n",
    "            display_columns = available_columns + optional_columns\n",
    "            if \"smb2.nt_status\" in ops_df.columns and \"smb2.nt_status\" not in display_columns:\n",
    "                display_columns.append(\"smb2.nt_status\")\n",
    "            logger.debug(f\"Rendering table with columns: {display_columns}\")\n",
    "            \n",
    "            styles = [\n",
    "                {\"selector\": \"td:nth-child(3)\", \"props\": [(\"max-width\", \"300px\"), (\"word-wrap\", \"break-word\"), (\"white-space\", \"normal\")]}\n",
    "            ]\n",
    "            styled_df = ops_df[display_columns].fillna('').style.set_table_styles(styles).hide(axis=\"index\")\n",
    "            html_table = styled_df.to_html()\n",
    "            \n",
    "            sesid = session_frames[\"smb2.sesid\"].iloc[0] if not session_frames.empty and \"smb2.sesid\" in session_frames.columns else \"N/A\"\n",
    "            if \"smb2.sesid\" in session_frames.columns and \"smb2.sesid\" in FIELD_MAPPINGS:\n",
    "                sesid = FIELD_MAPPINGS[\"smb2.sesid\"][\"normalize\"](session_frames[\"smb2.sesid\"].iloc[0]) or sesid\n",
    "            \n",
    "            cmd_list = []\n",
    "            action_list = []\n",
    "            if \"smb2.cmd\" in session_frames.columns:\n",
    "                cmd_series = session_frames[\"smb2.cmd\"].dropna()\n",
    "                for idx, value in cmd_series.items():\n",
    "                    if pd.notna(value):\n",
    "                        cmd_values = str(value).split(',')\n",
    "                        for cmd in cmd_values:\n",
    "                            cmd_stripped = cmd.strip()\n",
    "                            try:\n",
    "                                cmd_int = int(cmd_stripped)\n",
    "                                cmd_mapping = FIELD_MAPPINGS.get(\"smb2.cmd\", {}).get(\"mapping\", {})\n",
    "                                cmd_name = cmd_mapping.get(str(cmd_int), \"\")\n",
    "                                cmd_list.append(cmd_name)\n",
    "                            except ValueError:\n",
    "                                logger.debug(f\"Invalid command value '{cmd_stripped}' in frame {session_frames.loc[idx, 'frame.number']}\")\n",
    "            \n",
    "            if \"smb2.create.action\" in session_frames.columns:\n",
    "                action_series = session_frames[\"smb2.create.action\"].dropna()\n",
    "                for idx, value in action_series.items():\n",
    "                    if pd.notna(value):\n",
    "                        action_values = str(value).split(',')\n",
    "                        for action in action_values:\n",
    "                            action_stripped = action.strip()\n",
    "                            try:\n",
    "                                action_int = int(action_stripped)\n",
    "                                action_name = CREATE_ACTION_DESC.get(str(action_int), \"\")\n",
    "                                action_list.append(action_name)\n",
    "                            except ValueError:\n",
    "                                logger.debug(f\"Invalid create action value '{action_stripped}' in frame {session_frames.loc[idx, 'frame.number']}\")\n",
    "            \n",
    "            commands = pd.Series(cmd_list).value_counts() if cmd_list else pd.Series()\n",
    "            actions = pd.Series(action_list).value_counts() if action_list else pd.Series()\n",
    "            summary = f\"<b>Session {sesid}</b>: {len(session_frames)} frames, {len(commands)} unique commands: {commands.to_dict() if not commands.empty else {}}, {len(actions)} unique create actions: {actions.to_dict() if not actions.empty else {}}\"\n",
    "            \n",
    "            logger.info(\"Displaying summary and table\")\n",
    "            display(HTML(summary))\n",
    "            display(HTML(html_table), display_id=True)\n",
    "            logger.info(\"Finished rendering\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Error during rendering: {e}\")\n",
    "            display(HTML(f\"<p style='color: red;'>Rendering failed: {e}</p>\"))\n",
    "        finally:\n",
    "            update_progress(\"Rendering complete\")\n",
    "\n",
    "def on_case_number_change(change):\n",
    "    \"\"\"Handle case number changes.\"\"\"\n",
    "    case_number = change[\"new\"].strip()\n",
    "    logger.info(f\"Case number changed to: {case_number}\")\n",
    "    if not case_number:\n",
    "        capture_dropdown.options = [\"Select a capture\"]\n",
    "        capture_dropdown.disabled = True\n",
    "        session_dropdown.options = [\"Select a session\"]\n",
    "        session_dropdown.disabled = True\n",
    "        file_combobox.options = [\"Entire Stream\"]\n",
    "        file_combobox.disabled = True\n",
    "        check_fields_select.options = []\n",
    "        check_fields_select.disabled = True\n",
    "        check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "        save_button.disabled = True\n",
    "        ingest_button.disabled = True\n",
    "        reingest_button.disabled = True\n",
    "        replay_button.disabled = True\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p>Info: No case number entered.</p>\"))\n",
    "        return\n",
    "    \n",
    "    base_dir = f\"/stingray/{case_number}\"\n",
    "    if not os.path.isdir(base_dir):\n",
    "        logger.warning(f\"Case directory not found: {base_dir}. Note: Path may be remote; consider SSH validation.\")\n",
    "        capture_dropdown.options = [\"Select a capture\"]\n",
    "        capture_dropdown.disabled = True\n",
    "        file_combobox.options = [\"Entire Stream\"]\n",
    "        file_combobox.disabled = True\n",
    "        check_fields_select.options = []\n",
    "        check_fields_select.disabled = True\n",
    "        check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>Case directory not found: {base_dir}</p>\"))\n",
    "        return\n",
    "    \n",
    "    pcap_files = []\n",
    "    logger.info(f\"Scanning {base_dir} for capture files using enhanced detection...\")\n",
    "    \n",
    "    file_count = 0\n",
    "    error_count = 0\n",
    "    \n",
    "    for root, _, files in os.walk(base_dir):\n",
    "        for f in files:\n",
    "            file_count += 1\n",
    "            full_path = os.path.join(root, f)\n",
    "            try:\n",
    "                # Use enhanced detection, with fallback to extension-based\n",
    "                if is_capture_file(full_path):\n",
    "                    rel_path = os.path.relpath(full_path, base_dir).replace(\"\\\\\", \"/\")\n",
    "                    pcap_files.append(rel_path)\n",
    "                    logger.debug(f\"Added capture file: {rel_path}\")\n",
    "            except Exception as e:\n",
    "                error_count += 1\n",
    "                logger.warning(f\"Error checking file {full_path}: {e}\")\n",
    "                # Fallback to extension-based detection for this file\n",
    "                if f.lower().endswith(('.pcap', '.pcapng', '.cap', '.trc', '.trace')):\n",
    "                    rel_path = os.path.relpath(full_path, base_dir).replace(\"\\\\\", \"/\")\n",
    "                    pcap_files.append(rel_path)\n",
    "                    logger.debug(f\"Added capture file (fallback): {rel_path}\")\n",
    "    \n",
    "    logger.info(f\"Scanned {file_count} files, found {len(pcap_files)} capture files, {error_count} errors\")\n",
    "    \n",
    "    if not pcap_files:\n",
    "        logger.warning(f\"No capture files found in {base_dir} using enhanced detection (scanned {file_count} files, {error_count} errors)\")\n",
    "        capture_dropdown.options = [\"Select a capture\"]\n",
    "        capture_dropdown.disabled = True\n",
    "        file_combobox.options = [\"Entire Stream\"]\n",
    "        file_combobox.disabled = True\n",
    "        check_fields_select.options = []\n",
    "        check_fields_select.disabled = True\n",
    "        check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>No capture files found in {base_dir} (scanned {file_count} files)</p>\"))\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Found {len(pcap_files)} capture files in {base_dir}: {pcap_files[:5]}... (first 5 shown)\")\n",
    "    capture_dropdown.options = [\"Select a capture\"] + sorted(pcap_files)\n",
    "    capture_dropdown.disabled = False\n",
    "    capture_dropdown.value = \"Select a capture\"\n",
    "    with output_cell:\n",
    "        clear_output(wait=True)\n",
    "        display(HTML(f\"<p>Info: Select a capture file to proceed.</p>\"))\n",
    "\n",
    "def on_capture_change(change):\n",
    "    \"\"\"Handle capture selection.\"\"\"\n",
    "    capture_rel_path = change[\"new\"]\n",
    "    logger.info(f\"Capture changed to: {capture_rel_path}\")\n",
    "    if capture_rel_path == \"Select a capture\":\n",
    "        session_dropdown.options = [\"Select a session\"]\n",
    "        session_dropdown.disabled = True\n",
    "        file_combobox.options = [\"Entire Stream\"]\n",
    "        file_combobox.disabled = True\n",
    "        check_fields_select.options = []\n",
    "        check_fields_select.disabled = True\n",
    "        check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "        save_button.disabled = True\n",
    "        ingest_button.disabled = True\n",
    "        reingest_button.disabled = True\n",
    "        replay_button.disabled = True\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p>Info: No capture selected.</p>\"))\n",
    "        return\n",
    "    \n",
    "    case_number = case_number_input.value.strip()\n",
    "    capture_path = os.path.join(\"/stingray\", case_number, capture_rel_path).replace(\"\\\\\", \"/\")\n",
    "    logger.debug(f\"Constructed capture path: {capture_path}\")\n",
    "    if not os.path.exists(capture_path):\n",
    "        logger.warning(f\"Invalid capture path: {capture_path}\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>Invalid capture path: {capture_path}</p>\"))\n",
    "        return\n",
    "    \n",
    "    pcap_config[\"capture_path\"] = capture_path\n",
    "    builtins.capture = capture_path\n",
    "    try:\n",
    "        with open(config_file, 'wb') as f:\n",
    "            pickle.dump({'pcap_config': pcap_config, 'replay_config': replay_config}, f)\n",
    "        logger.info(f\"Saved pcap_config to {config_file}: capture_path={capture_path}, verbose_level={pcap_config['verbose_level']}\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to save {config_file}: {e}\")\n",
    "    \n",
    "    capture = load_capture()\n",
    "    if capture is None:\n",
    "        logger.warning(f\"Failed to load capture from config.pkl, using constructed path: {capture_path}\")\n",
    "        capture = capture_path  # Fallback to constructed path\n",
    "        pcap_config[\"capture_path\"] = capture\n",
    "        builtins.capture = capture\n",
    "    \n",
    "    output_dir = get_output_dir(capture) if capture else None\n",
    "    session_files = list_session_files(output_dir) if output_dir else []\n",
    "    \n",
    "    if session_files:\n",
    "        session_dropdown.options = [\"Select a session\"] + session_files\n",
    "        session_dropdown.disabled = False\n",
    "        ingest_button.disabled = True\n",
    "        reingest_button.disabled = False\n",
    "        replay_button.disabled = False\n",
    "        logger.info(f\"Populated session_dropdown with {len(session_files)} sessions: {session_files[:5]}...\")\n",
    "    else:\n",
    "        session_dropdown.options = [\"Select a session\"]\n",
    "        session_dropdown.disabled = True\n",
    "        ingest_button.disabled = False\n",
    "        reingest_button.disabled = True\n",
    "        replay_button.disabled = True\n",
    "        logger.info(\"No session files found, ingest_button enabled for processing\")\n",
    "    \n",
    "    file_combobox.options = [\"Entire Stream\"] + (session_files if session_files else [])\n",
    "    file_combobox.disabled = not session_files\n",
    "    check_fields_select.tooltip = f\"Select fields to display as columns in the table for {file_combobox.value}\"\n",
    "    with output_cell:\n",
    "        clear_output(wait=True)\n",
    "        if session_files:\n",
    "            display(HTML(f\"<p>Info: Found {len(session_files)} sessions. Select one or click 'Re-ingest' to rebuild.</p>\"))\n",
    "        else:\n",
    "            display(HTML(\"<p>Info: No sessions found. Click 'Ingest Trace' to process.</p>\"))\n",
    "    logger.debug(f\"Button states: ingest={ingest_button.disabled}, reingest={reingest_button.disabled}, replay={replay_button.disabled}\")\n",
    "\n",
    "def on_ingest_button_clicked(b):\n",
    "    \"\"\"Handle ingest button click.\"\"\"\n",
    "    logger.info(\"Ingest button clicked\")\n",
    "    capture = load_capture()\n",
    "    logger.info(f\"Using capture_path = {capture} for ingestion\")\n",
    "    if not capture:\n",
    "        logger.warning(\"No valid capture path available for ingestion.\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: red;'>No valid capture path available for ingestion.</p>\"))\n",
    "        return\n",
    "    \n",
    "    ingest_button.disabled = True\n",
    "    reingest_button.disabled = True\n",
    "    replay_button.disabled = True\n",
    "    update_progress(f\"Ingesting {os.path.basename(capture)}...\")\n",
    "    with progress_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            logger.info(\"Starting run_ingestion\")\n",
    "            result = run_ingestion(capture_path=capture, reassembly_enabled=True, force_reingest=False, verbose=None)\n",
    "            logger.info(f\"Ingestion of {capture} completed. Result: {result is not None}\")\n",
    "            case_number = case_number_input.value.strip()\n",
    "            if case_number:\n",
    "                logger.info(f\"Calling on_capture_change with {capture_dropdown.value}\")\n",
    "                on_capture_change({\"new\": capture_dropdown.value})\n",
    "            else:\n",
    "                logger.warning(\"Case number not set. Please select a case.\")\n",
    "            if result:\n",
    "                logger.info(f\"Ingestion result: {list(result.get('sessions', {}).keys())}\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Error during ingestion: {e}\")\n",
    "            with output_cell:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(f\"<p style='color: red;'>Ingestion failed: {e}</p>\"))\n",
    "        finally:\n",
    "            ingest_button.disabled = False\n",
    "            reingest_button.disabled = False\n",
    "            replay_button.disabled = False\n",
    "            update_button_states()\n",
    "\n",
    "def on_reingest_button_clicked(b):\n",
    "    \"\"\"Handle re-ingest button click.\"\"\"\n",
    "    logger.info(\"Re-ingest button clicked\")\n",
    "    capture = load_capture()\n",
    "    logger.info(f\"Using capture_path = {capture} for re-ingestion\")\n",
    "    if not capture:\n",
    "        logger.warning(\"No valid capture path available for re-ingestion.\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: red;'>No valid capture path available for re-ingestion.</p>\"))\n",
    "        return\n",
    "    \n",
    "    ingest_button.disabled = True\n",
    "    reingest_button.disabled = True\n",
    "    replay_button.disabled = True\n",
    "    update_progress(f\"Re-ingesting {os.path.basename(capture)}...\")\n",
    "    with progress_output:\n",
    "        clear_output(wait=True)\n",
    "        try:\n",
    "            logger.info(\"Starting run_ingestion\")\n",
    "            result = run_ingestion(capture_path=capture, reassembly_enabled=True, force_reingest=True, verbose=None)\n",
    "            logger.info(f\"Re-ingestion of {capture} completed. Result: {result is not None}\")\n",
    "            case_number = case_number_input.value.strip()\n",
    "            if case_number:\n",
    "                logger.info(f\"Calling on_capture_change with {capture_dropdown.value}\")\n",
    "                on_capture_change({\"new\": capture_dropdown.value})\n",
    "            else:\n",
    "                logger.warning(\"Case number not set. Please select a case.\")\n",
    "            if result:\n",
    "                logger.info(f\"Re-ingestion result: {list(result.get('sessions', {}).keys())}\")\n",
    "        except Exception as e:\n",
    "            logger.critical(f\"Error during re-ingestion: {e}\")\n",
    "            with output_cell:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(f\"<p style='color: red;'>Re-ingestion failed: {e}</p>\"))\n",
    "        finally:\n",
    "            ingest_button.disabled = False\n",
    "            reingest_button.disabled = False\n",
    "            replay_button.disabled = False\n",
    "            update_button_states()\n",
    "\n",
    "def on_replay_button_clicked(b):\n",
    "    \"\"\"Handle replay button click, triggering replay_session.\"\"\"\n",
    "    logger.info(\"Replay button clicked\")\n",
    "    if session_dropdown.value == \"Select a session\":\n",
    "        logger.info(\"No session selected for replay.\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: red;'>Select a session to replay.</p>\"))\n",
    "        return\n",
    "    \n",
    "    update_progress(f\"Replaying session {session_dropdown.value}...\")\n",
    "    try:\n",
    "        replay_session(session_dropdown.value, output_cell)\n",
    "        logger.info(f\"Replay of {session_dropdown.value} completed.\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error during replay: {e}\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>Replay failed: {e}</p>\"))\n",
    "    finally:\n",
    "        update_progress(\"Replay complete\")\n",
    "        update_button_states()\n",
    "\n",
    "def on_session_change(change):\n",
    "    \"\"\"Handle session selection.\"\"\"\n",
    "    logger.info(f\"Session changed to: {change['new']}\")\n",
    "    if change[\"new\"] == \"Select a session\":\n",
    "        file_combobox.options = [\"Entire Stream\"]\n",
    "        file_combobox.disabled = True\n",
    "        check_fields_select.options = []\n",
    "        check_fields_select.disabled = True\n",
    "        check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "        save_button.disabled = True\n",
    "        replay_button.disabled = True\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p>Info: No session selected.</p>\"))\n",
    "        return\n",
    "    \n",
    "    capture = load_capture()\n",
    "    if capture is None:\n",
    "        logger.warning(\"Failed to load capture for session change\")\n",
    "        return\n",
    "    \n",
    "    session_frames, field_options, file_options, selected_fields = load_and_summarize_session(capture, change[\"new\"])\n",
    "    if session_frames is None:\n",
    "        logger.warning(f\"Session frames not loaded. Check load_and_summarize_session.\")\n",
    "        return\n",
    "    \n",
    "    file_combobox.options = [\"Entire Stream\"] + (file_options if file_options else [])\n",
    "    file_combobox.value = \"Entire Stream\"\n",
    "    file_combobox.disabled = False\n",
    "    check_fields_select.options = field_options if field_options else []\n",
    "    check_fields_select.value = selected_fields if selected_fields else []\n",
    "    check_fields_select.disabled = not file_options\n",
    "    check_fields_select.tooltip = f\"Select fields to display as columns in the table for {file_combobox.value}\"\n",
    "    save_button.disabled = False\n",
    "    replay_button.disabled = False\n",
    "    \n",
    "    with output_cell:\n",
    "        clear_output(wait=True)\n",
    "        if file_options:\n",
    "            display(HTML(f\"<p>Info: Session loaded. Viewing entire stream or select a file to filter operations.</p>\"))\n",
    "        else:\n",
    "            display(HTML(\"<p>Info: No files found in session. Viewing entire stream.</p>\"))\n",
    "    logger.info(f\"Session changed to {change['new']}, {len(file_options)} file options available\")\n",
    "    global operations\n",
    "    operations = update_operations(capture, change[\"new\"], None, check_fields_select.value)\n",
    "    logger.info(f\"Set operations to entire stream, count: {len(operations)}\")\n",
    "    render_page()\n",
    "\n",
    "def on_file_change(change):\n",
    "    \"\"\"Handle file filter changes.\"\"\"\n",
    "    logger.info(f\"File filter changed to: {change['new']} at {time.strftime('%H:%M:%S')}\")\n",
    "    check_fields_select.tooltip = f\"Select fields to display as columns in the table for {change['new']}\"\n",
    "    if session_dropdown.value != \"Select a session\":\n",
    "        capture = load_capture()\n",
    "        if capture is None:\n",
    "            logger.warning(\"Failed to load capture for file change\")\n",
    "            return\n",
    "        global operations\n",
    "        operations = update_operations(capture, session_dropdown.value, change[\"new\"], check_fields_select.value)\n",
    "        logger.info(f\"update_operations returned, operations count: {len(operations)}\")\n",
    "        render_page()\n",
    "\n",
    "def on_fields_change(change):\n",
    "    \"\"\"Handle field selection changes.\"\"\"\n",
    "    logger.info(f\"Fields selection changed to: {change['new']} at {time.strftime('%H:%M:%S')}\")\n",
    "    if session_dropdown.value != \"Select a session\":\n",
    "        capture = load_capture()\n",
    "        if capture is None:\n",
    "            logger.warning(\"Failed to load capture for fields change\")\n",
    "            return\n",
    "        global operations\n",
    "        operations = update_operations(capture, session_dropdown.value, file_combobox.value, change[\"new\"])\n",
    "        logger.info(f\"update_operations returned, operations count: {len(operations)}\")\n",
    "        render_page()\n",
    "\n",
    "def on_save_config(b):\n",
    "    \"\"\"Save replay server configuration to config.pkl.\"\"\"\n",
    "    logger.info(\"Save config button clicked\")\n",
    "    if session_dropdown.value == \"Select a session\":\n",
    "        logger.info(\"No session selected to save configuration.\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: red;'>Select a session to save configuration.</p>\"))\n",
    "        return\n",
    "    \n",
    "    replay_config.update({\n",
    "        \"server_ip\": server_ip_input.value,\n",
    "        \"domain\": domain_input.value,\n",
    "        \"username\": username_input.value,\n",
    "        \"password\": password_input.value,\n",
    "        \"tree_name\": tree_name_input.value,\n",
    "        \"max_wait\": max_time_input.value\n",
    "    })\n",
    "    try:\n",
    "        # Load existing config to preserve pcap_config\n",
    "        current_config = {}\n",
    "        if os.path.exists(config_file):\n",
    "            with open(config_file, 'rb') as f:\n",
    "                current_config = pickle.load(f)\n",
    "                logger.debug(f\"Loaded existing config.pkl for save: {current_config}\")\n",
    "        current_config.update({\n",
    "            'pcap_config': pcap_config,\n",
    "            'replay_config': replay_config\n",
    "        })\n",
    "        with open(config_file, 'wb') as f:\n",
    "            pickle.dump(current_config, f)\n",
    "        logger.info(f\"Saved configuration to {config_file}: pcap_config={pcap_config}, replay_config={replay_config}\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(\"<p style='color: green;'>Configuration saved.</p>\"))\n",
    "            render_page()\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Error saving configuration to {config_file}: {e}\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>Error saving configuration: {e}</p>\"))\n",
    "\n",
    "def on_case_number_change(change):\n",
    "    \"\"\"Handle case number changes.\"\"\"\n",
    "    case_number = change[\"new\"].strip()\n",
    "    logger.info(f\"Case number changed to: {case_number}\")\n",
    "    \n",
    "    try:\n",
    "        if not case_number:\n",
    "            logger.info(\"Empty case number - resetting dropdowns\")\n",
    "            capture_dropdown.options = [\"Select a capture\"]\n",
    "            capture_dropdown.disabled = True\n",
    "            session_dropdown.options = [\"Select a session\"]\n",
    "            session_dropdown.disabled = True\n",
    "            file_combobox.options = [\"Entire Stream\"]\n",
    "            file_combobox.disabled = True\n",
    "            check_fields_select.options = []\n",
    "            check_fields_select.disabled = True\n",
    "            check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "            save_button.disabled = True\n",
    "            ingest_button.disabled = True\n",
    "            reingest_button.disabled = True\n",
    "            replay_button.disabled = True\n",
    "            with output_cell:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(\"<p>Info: No case number entered.</p>\"))\n",
    "            return\n",
    "        \n",
    "        base_dir = f\"/stingray/{case_number}\"\n",
    "        logger.info(f\"Checking directory: {base_dir}\")\n",
    "        \n",
    "        if not os.path.isdir(base_dir):\n",
    "            logger.warning(f\"Case directory not found: {base_dir}. Note: Path may be remote; consider SSH validation.\")\n",
    "            capture_dropdown.options = [\"Select a capture\"]\n",
    "            capture_dropdown.disabled = True\n",
    "            file_combobox.options = [\"Entire Stream\"]\n",
    "            file_combobox.disabled = True\n",
    "            check_fields_select.options = []\n",
    "            check_fields_select.disabled = True\n",
    "            check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "            with output_cell:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(f\"<p style='color: red;'>Case directory not found: {base_dir}</p>\"))\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Directory {base_dir} exists, scanning for files...\")\n",
    "        \n",
    "        pcap_files = []\n",
    "        logger.info(f\"Scanning {base_dir} for capture files using enhanced detection...\")\n",
    "        \n",
    "        file_count = 0\n",
    "        error_count = 0\n",
    "        \n",
    "        for root, _, files in os.walk(base_dir):\n",
    "            logger.debug(f\"Scanning directory: {root}, found {len(files)} files\")\n",
    "            for f in files:\n",
    "                file_count += 1\n",
    "                full_path = os.path.join(root, f)\n",
    "                logger.debug(f\"Checking file {file_count}: {f}\")\n",
    "                try:\n",
    "                    # Use enhanced detection, with fallback to extension-based\n",
    "                    if is_capture_file(full_path):\n",
    "                        rel_path = os.path.relpath(full_path, base_dir).replace(\"\\\\\", \"/\")\n",
    "                        pcap_files.append(rel_path)\n",
    "                        logger.info(f\"Added capture file: {rel_path}\")\n",
    "                except Exception as e:\n",
    "                    error_count += 1\n",
    "                    logger.warning(f\"Error checking file {full_path}: {e}\")\n",
    "                    # Fallback to extension-based detection for this file\n",
    "                    if f.lower().endswith(('.pcap', '.pcapng', '.cap', '.trc', '.trace', '.trc0')):\n",
    "                        rel_path = os.path.relpath(full_path, base_dir).replace(\"\\\\\", \"/\")\n",
    "                        pcap_files.append(rel_path)\n",
    "                        logger.info(f\"Added capture file (fallback): {rel_path}\")\n",
    "        \n",
    "        logger.info(f\"Scanned {file_count} files, found {len(pcap_files)} capture files, {error_count} errors\")\n",
    "        \n",
    "        if not pcap_files:\n",
    "            logger.warning(f\"No capture files found in {base_dir} using enhanced detection (scanned {file_count} files, {error_count} errors)\")\n",
    "            capture_dropdown.options = [\"Select a capture\"]\n",
    "            capture_dropdown.disabled = True\n",
    "            file_combobox.options = [\"Entire Stream\"]\n",
    "            file_combobox.disabled = True\n",
    "            check_fields_select.options = []\n",
    "            check_fields_select.disabled = True\n",
    "            check_fields_select.tooltip = \"Select fields to display as columns in the table for Entire Stream\"\n",
    "            with output_cell:\n",
    "                clear_output(wait=True)\n",
    "                display(HTML(f\"<p style='color: red;'>No capture files found in {base_dir} (scanned {file_count} files)</p>\"))\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(pcap_files)} capture files in {base_dir}: {pcap_files[:5]}... (first 5 shown)\")\n",
    "        capture_dropdown.options = [\"Select a capture\"] + sorted(pcap_files)\n",
    "        capture_dropdown.disabled = False\n",
    "        capture_dropdown.value = \"Select a capture\"\n",
    "        \n",
    "        # Auto-load the first capture file if there's only one, or if we have a loaded capture that matches\n",
    "        loaded_capture = load_capture()\n",
    "        if loaded_capture:\n",
    "            # Extract relative path from the loaded capture\n",
    "            try:\n",
    "                loaded_rel_path = os.path.relpath(loaded_capture, base_dir).replace(\"\\\\\", \"/\")\n",
    "                if loaded_rel_path in pcap_files:\n",
    "                    logger.info(f\"Auto-selecting loaded capture: {loaded_rel_path}\")\n",
    "                    capture_dropdown.value = loaded_rel_path\n",
    "                    # Trigger the capture change event to populate sessions\n",
    "                    on_capture_change({\"new\": loaded_rel_path})\n",
    "                    with output_cell:\n",
    "                        clear_output(wait=True)\n",
    "                        display(HTML(f\"<p>Info: Auto-loaded capture: {loaded_rel_path}</p>\"))\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error auto-loading capture {loaded_capture}: {e}\")\n",
    "        \n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p>Info: Select a capture file to proceed.</p>\"))\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Critical error in on_case_number_change: {e}\")\n",
    "        import traceback\n",
    "        logger.critical(f\"Traceback: {traceback.format_exc()}\")\n",
    "        with output_cell:\n",
    "            clear_output(wait=True)\n",
    "            display(HTML(f\"<p style='color: red;'>Error processing case number: {e}</p>\"))\n",
    "\n",
    "def initialize_dashboard():\n",
    "    \"\"\"Initialize dashboard by loading capture if case number and capture path are pre-set.\"\"\"\n",
    "    logger.info(\"Initializing dashboard\")\n",
    "    capture = load_capture()\n",
    "    \n",
    "    # Extract case number from capture path if not already set\n",
    "    case_number = case_number_input.value.strip() if case_number_input else \"\"\n",
    "    if not case_number and capture:\n",
    "        # Try to extract case number from the capture path\n",
    "        parts = capture.split(os.sep)\n",
    "        if len(parts) >= 3 and parts[1] == 'stingray':\n",
    "            case_number = parts[2]\n",
    "            logger.info(f\"Extracted case number from capture path: {case_number}\")\n",
    "            case_number_input.value = case_number\n",
    "    \n",
    "    logger.debug(f\"Initial pcap_config: {pcap_config}\")\n",
    "    if case_number and capture:\n",
    "        logger.info(f\"Pre-filled case number: {case_number}, capture path: {capture}\")\n",
    "        base_dir = f\"/stingray/{case_number}\"\n",
    "        if os.path.isdir(base_dir):\n",
    "            found = False\n",
    "            for root, _, files in os.walk(base_dir):\n",
    "                for f in files:\n",
    "                    full_path = os.path.join(root, f)\n",
    "                    # Use the enhanced capture file detection instead of just extension checking\n",
    "                    try:\n",
    "                        if is_capture_file(full_path) and full_path == capture:\n",
    "                            capture_rel_path = os.path.relpath(capture, base_dir).replace(\"\\\\\", \"/\")\n",
    "                            logger.info(f\"Auto-loading capture: {capture_rel_path}\")\n",
    "                            \n",
    "                            # First populate the dropdown options by calling the case number change handler\n",
    "                            on_case_number_change({\"new\": case_number})\n",
    "                            \n",
    "                            # Then set the dropdown value to the found capture\n",
    "                            capture_dropdown.value = capture_rel_path\n",
    "                            logger.info(f\"Set capture dropdown value to: {capture_rel_path}\")\n",
    "                            \n",
    "                            # Finally trigger the capture change event to populate sessions\n",
    "                            on_capture_change({\"new\": capture_rel_path})\n",
    "                            found = True\n",
    "                            break\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error checking file {full_path} during initialization: {e}\")\n",
    "                        # Fallback check - if the full path matches exactly, assume it's the right file\n",
    "                        if full_path == capture:\n",
    "                            capture_rel_path = os.path.relpath(capture, base_dir).replace(\"\\\\\", \"/\")\n",
    "                            logger.info(f\"Auto-loading capture (fallback): {capture_rel_path}\")\n",
    "                            \n",
    "                            # First populate the dropdown options by calling the case number change handler\n",
    "                            on_case_number_change({\"new\": case_number})\n",
    "                            \n",
    "                            # Then set the dropdown value to the found capture\n",
    "                            capture_dropdown.value = capture_rel_path\n",
    "                            logger.info(f\"Set capture dropdown value to: {capture_rel_path} (fallback)\")\n",
    "                            \n",
    "                            # Finally trigger the capture change event to populate sessions\n",
    "                            on_capture_change({\"new\": capture_rel_path})\n",
    "                            found = True\n",
    "                            break\n",
    "                if found:\n",
    "                    break\n",
    "            if not found:\n",
    "                logger.warning(f\"Capture path {capture} not found in {base_dir} or its subdirectories\")\n",
    "                # Try to trigger case number change handler to populate the capture dropdown\n",
    "                logger.info(f\"Triggering case number change handler for case {case_number}\")\n",
    "                on_case_number_change({\"new\": case_number})\n",
    "        else:\n",
    "            logger.warning(f\"Case directory {base_dir} not found\")\n",
    "    else:\n",
    "        logger.info(\"No valid case number or capture path to auto-load\")\n",
    "\n",
    "# Clear existing bindings and re-apply\n",
    "ingest_button.on_click(None)\n",
    "reingest_button.on_click(None)\n",
    "ingest_button.on_click(on_ingest_button_clicked)\n",
    "reingest_button.on_click(on_reingest_button_clicked)\n",
    "logger.info(\"Ingest and Re-ingest buttons bound\")\n",
    "\n",
    "# Attach handlers\n",
    "case_number_input.observe(on_case_number_change, names=\"value\")\n",
    "capture_dropdown.observe(on_capture_change, names=\"value\")\n",
    "session_dropdown.observe(on_session_change, names=\"value\")\n",
    "file_combobox.observe(on_file_change, names=\"value\")\n",
    "check_fields_select.observe(on_fields_change, names=\"value\")\n",
    "save_button.on_click(on_save_config)\n",
    "replay_button.on_click(on_replay_button_clicked)\n",
    "\n",
    "# Trigger initial load\n",
    "initialize_dashboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bed403-2e6b-4152-b7ca-bb05ebc82b3f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Cell 11: Dashboard Display for Session Visualization\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import builtins\n",
    "import logging\n",
    "\n",
    "# Import logger and required widgets from builtins (set in Cells 1–10)\n",
    "try:\n",
    "    from builtins import (\n",
    "        logger, pcap_config, replay_config, log_output, output_cell, progress_output,\n",
    "        case_number_input, capture_dropdown, session_dropdown,\n",
    "        button_box, file_combobox, check_fields_select, server_ip_input,\n",
    "        domain_input, username_input, password_input, tree_name_input,\n",
    "        max_time_input, save_button, dashboard\n",
    "    )\n",
    "except ImportError as e:\n",
    "    logging.critical(f\"Failed to import from builtins: {e}. Ensure Cells 1–10 are executed.\")\n",
    "    raise\n",
    "\n",
    "def update_dashboard_layout(verbose_level: int = None) -> None:\n",
    "    \"\"\"Update and display the dashboard layout with dynamic log visibility.\n",
    "\n",
    "    Args:\n",
    "        verbose_level: Verbosity level (0: minimal, 1: standard, 2+: verbose logs).\n",
    "                       If None, uses pcap_config['verbose_level'] from Cell 1.\n",
    "    \"\"\"\n",
    "    # Use verbose_level from pcap_config if not provided\n",
    "    effective_verbose_level = verbose_level if verbose_level is not None else pcap_config.get(\"verbose_level\", 0)\n",
    "    logger.info(f\"Updating dashboard layout with verbose_level={effective_verbose_level}\")\n",
    "\n",
    "    # Create replay configuration section with clear labeling\n",
    "    replay_config_box = widgets.VBox([\n",
    "        widgets.HTML(\"<h3>Replay Server Configuration</h3>\", layout={'margin': '10px 0'}),\n",
    "        widgets.HBox([\n",
    "            server_ip_input,\n",
    "            domain_input,\n",
    "            username_input\n",
    "        ], layout={'margin': '5px 0'}),\n",
    "        widgets.HBox([\n",
    "            password_input,\n",
    "            tree_name_input,\n",
    "            max_time_input\n",
    "        ], layout={'margin': '5px 0'}),\n",
    "        save_button\n",
    "    ], layout={'border': '1px solid #ddd', 'padding': '10px', 'margin': '10px 0'})\n",
    "\n",
    "    # Define core dashboard components (remove debug_slider)\n",
    "    core_components = [\n",
    "        widgets.HTML(\"<h3>Session Visualization Dashboard</h3>\"),\n",
    "        progress_output,\n",
    "        case_number_input,\n",
    "        capture_dropdown,\n",
    "        session_dropdown,\n",
    "        button_box,\n",
    "        file_combobox,\n",
    "        check_fields_select,\n",
    "        output_cell,\n",
    "        replay_config_box\n",
    "    ]\n",
    "\n",
    "    # Conditionally include log_output based on verbose_level\n",
    "    if effective_verbose_level > 0:\n",
    "        dashboard_components = core_components + [log_output]\n",
    "    else:\n",
    "        dashboard_components = core_components + [widgets.Label(value=\"Logs hidden (set Debug Level > 0 in Configuration Cell to show)\")]\n",
    "\n",
    "    # Update dashboard children with error handling\n",
    "    try:\n",
    "        dashboard.children = dashboard_components\n",
    "        logger.debug(f\"Dashboard updated with {len(dashboard_components)} components\")\n",
    "    except Exception as e:\n",
    "        logger.critical(f\"Failed to update dashboard layout: {e}\")\n",
    "        raise\n",
    "\n",
    "    # Export to builtins for reuse\n",
    "    builtins.update_dashboard_layout = update_dashboard_layout\n",
    "    logger.info(\"Exported update_dashboard_layout to builtins\")\n",
    "\n",
    "# Initialize and display dashboard\n",
    "try:\n",
    "    verbose_level = pcap_config.get(\"verbose_level\", 0)  # Default to CRITICAL (from Cell 1)\n",
    "    update_dashboard_layout(verbose_level)\n",
    "    display(dashboard)\n",
    "    logger.info(\"Dashboard displayed successfully\")\n",
    "except Exception as e:\n",
    "    logger.critical(f\"Error displaying dashboard: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74069d2c-67a8-4c51-a3c4-475b3fb1ad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging cell\n",
    "capture = load_capture()\n",
    "case_number = case_number_input.value.strip()\n",
    "print(f\"Loaded capture: {capture}\")\n",
    "print(f\"Case number: {case_number}\")\n",
    "base_dir = f\"/stingray/{case_number}\"\n",
    "pcap_files = [f for root, _, files in os.walk(base_dir) for f in files if f.endswith(('.pcap', '.pcapng'))]\n",
    "print(f\"Found PCAP files: {pcap_files}\")\n",
    "if capture and os.path.basename(capture) in pcap_files:\n",
    "    capture_dropdown.options = [\"Select a capture\"] + [os.path.relpath(capture, base_dir).replace(\"\\\\\", \"/\")]\n",
    "    capture_dropdown.value = os.path.relpath(capture, base_dir).replace(\"\\\\\", \"/\")\n",
    "    logger.info(f\"Manually added and selected capture: {capture_dropdown.value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853928c-e1fd-4a63-ab01-1a0efb9210b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# case_number = \"2010373016\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
